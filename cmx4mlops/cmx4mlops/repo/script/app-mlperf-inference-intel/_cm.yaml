# Identification of this CM script
alias: app-mlperf-inference-intel
uid: c05a90433bb04cc1
cache: false
can_force_cache: true

automation_alias: script
automation_uid: 5b4e0237da074764

category: "Modular MLPerf benchmarks"


# User-friendly tags to find this CM script
tags:
  - reproduce
  - mlcommons
  - mlperf
  - inference
  - harness
  - intel-harness
  - intel
  - intel-harness
  - intel

# Default environment
default_env:
  CM_BATCH_COUNT: '1'
  CM_BATCH_SIZE: '1'
  CM_FAST_COMPILATION: 'yes'
  CM_MLPERF_LOADGEN_SCENARIO: Offline
  CM_MLPERF_LOADGEN_MODE: performance
  CM_SKIP_PREPROCESS_DATASET: 'no'
  CM_SKIP_MODEL_DOWNLOAD: 'no'
  CM_MLPERF_SUT_NAME_IMPLEMENTATION_PREFIX: intel
  CM_MLPERF_SKIP_RUN: 'no'
  verbosity: 1
  loadgen_trigger_cold_run: 0

env:
  CM_CALL_MLPERF_RUNNER: 'no'
  CUDA_VISIBLE_DEVICES: ''
  USE_CUDA: '0'

# Map script inputs to environment variables
input_mapping:
  count: CM_MLPERF_LOADGEN_QUERY_COUNT
  max_batchsize: CM_MLPERF_LOADGEN_MAX_BATCHSIZE
  mlperf_conf: CM_MLPERF_CONF
  mode: CM_MLPERF_LOADGEN_MODE
  output_dir: CM_MLPERF_OUTPUT_DIR
  performance_sample_count: CM_MLPERF_PERFORMANCE_SAMPLE_COUNT
  scenario: CM_MLPERF_LOADGEN_SCENARIO
  user_conf: CM_MLPERF_USER_CONF
  skip_preprocess: CM_SKIP_PREPROCESS_DATASET
  skip_preprocessing: CM_SKIP_PREPROCESS_DATASET
  target_qps: CM_MLPERF_LOADGEN_TARGET_QPS
  offline_target_qps: CM_MLPERF_LOADGEN_OFFLINE_TARGET_QPS
  server_target_qps: CM_MLPERF_LOADGEN_SERVER_TARGET_QPS
  target_latency: CM_MLPERF_LOADGEN_TARGET_LATENCY
  singlestream_target_latency: CM_MLPERF_LOADGEN_SINGLESTREAM_TARGET_LATENCY
  multistream_target_latency: CM_MLPERF_LOADGEN_MULTISTREAM_TARGET_LATENCY
  performance_sample_count: CM_MLPERF_LOADGEN_PERFORMANCE_SAMPLE_COUNT
  rerun: CM_RERUN

new_state_keys:
  - mlperf-inference-implementation
  - CM_SUT_*



# Dependencies on other CM scripts

deps:

  # Detect host OS features
  - tags: detect,os

  # Detect host CPU features
  - tags: detect,cpu

  # Install system dependencies on a given host
  - tags: get,sys-utils-cm


  # Get MLPerf logging library
  - tags: get,generic-python-lib,_mlperf_logging
    names:
    - mlperf-logging


  ########################################################################
  # Install ResNet50 model (ONNX) and ImageNet
 
  - enable_if_env:
      CM_MODEL:
      - resnet50
    names:
      - resnet50-model
      - ml-model
    tags: get,ml-model,resnet50,_fp32,_pytorch

  - enable_if_env:
      CM_MODEL:
      - resnet50
    names:
      - imagenet-original
      - dataset-original
    tags: get,dataset,imagenet,original,_full



  ########################################################################
  # Install OpenImages


  - enable_if_env:
      CM_MODEL:
      - retinanet
    names:
      - openimages-original
      - dataset-original
    tags: get,dataset,original,openimages,_validation,_custom-annotations,_full

  - enable_if_env:
      CM_MODEL:
      - retinanet
    names:
      - openimages-calibration
      - dataset-calibration
    tags: get,dataset,original,openimages,_calibration




# Post dependencies to run this app including for power measurement
post_deps:

  - names:
    - runner
    - mlperf-runner
    skip_if_env:
      CM_MLPERF_SKIP_RUN:
        - 'yes'
        - yes
    enable_if_env:
      CM_LOCAL_MLPERF_INFERENCE_INTEL_RUN_MODE:
        - run_harness
    tags: benchmark-mlperf

  - tags: save,mlperf,inference,state
    names:
      - save-mlperf-inference-state

# Variations to customize dependencies
variations:
  # version
  v4.0:
    group: version
    default: true
    env:
      CM_MLPERF_INFERENCE_CODE_VERSION: "v4.0"
    deps:
    - tags: get,mlperf,inference,results,_go
      names:
        inference-results
      version: v4.0
  v4.0,gptj_:
    adr:
      pytorch:
        tags: _for-intel-mlperf-inference-v4.0
  v4.0,bert_:
    adr:
      pytorch:
        tags: _for-intel-mlperf-inference-v4.0
  v3.1:
    group: version
    env:
      CM_MLPERF_INFERENCE_CODE_VERSION: "v3.1"
    deps:
    - tags: get,mlperf,inference,results,_ctuning
      names:
        inference-results
      version: v3.1

  v3.1,gptj_:
    adr:
      pytorch:
        tags: _for-intel-mlperf-inference-v3.1
  v3.1,dlrm-v2_:
    adr:
      pytorch:
        tags: _for-intel-mlperf-inference-v3.1
  v3.1,bert_:
    adr:
      pytorch:
        tags: _for-intel-mlperf-inference-v3.1

  # Target devices
  cpu:
    group: device
    default: true
    env:
      CM_MLPERF_DEVICE: cpu

  # ML engine
  pytorch:
    group: framework
    default: true
    env:
      CM_MLPERF_BACKEND: pytorch
      CM_MLPERF_BACKEND_LIB_NAMESPEC: pytorch

  bs.#:
    env:
      ML_MLPERF_MODEL_BATCH_SIZE: "#"

  # Reference MLPerf models
  resnet50:
    group: model
    default: true
    env:
      CM_MODEL: resnet50
      CM_BENCHMARK: STANDALONE_CLASSIFICATION

  resnet50,int8:
    env:
      CM_IMAGENET_ACCURACY_DTYPE: int8

  bert-99:
    deps:
      - tags: compile,intel,model,_bert-99
        names:
          - bert-99-compiler
    env:
      CM_ML_MODEL_WEIGHTS_DATA_TYPE: int8
      CM_ML_MODEL_INPUTS_DATA_TYPE: int8

  retinanet:
    group: model
    env:
      CM_MODEL: retinanet
      CM_ML_MODEL_STARTING_WEIGHTS_FILENAME: "https://zenodo.org/record/6617981/files/resnext50_32x4d_fpn.pth"
      CM_BENCHMARK: STANDALONE_OBJECT_DETECTION

    deps:
    - tags: get,generic-python-lib,_numpy
      names:
        - pip-package
        - numpy
      version: "1.23.5"

  3d-unet-99:
    group: model
    base:
      - 3d-unet_
    env:
      CM_MODEL: 3d-unet-99

  3d-unet-99.9:
    group: model
    base:
      - 3d-unet_
    env:
      CM_MODEL: 3d-unet-99.9

  3d-unet_:
    env:
      CM_BENCHMARK: MEDICAL_IMAGING
    deps:
      - tags: get,dataset,kits19,preprocessed
      - tags: get,ml-model,medical-imaging,3d-unet,_pytorch,_weights

  bert_:
    env:
      CM_BENCHMARK: STANDALONE_BERT

  bert_,pytorch:
    deps:
      - tags: get,conda,_name.bert-pt
      - tags: install,llvm,src,_tag.llvmorg-15.0.7,_runtimes.libcxx:libcxxabi:openmp,_clang,_release,_for-intel-mlperf-inference-v3.1-bert
        names:
          - llvm-from-src
      - tags: get,generic-sys-util,_libffi7
      - tags: get,generic,conda-package,_package.python
        names:
        - conda-package
        - python
        version: "3.8"
      - names:
        - conda-package
        - ncurses
        tags: get,generic,conda-package,_package.ncurses,_source.conda-forge
 
      - tags: get,generic-sys-util,_numactl
      - tags: get,generic,conda-package,_package.jemalloc,_source.conda-forge
        names:
        - conda-package
        - jemalloc
      - tags: get,pytorch,from.src,_for-intel-mlperf-inference-v3.1-bert
        names:
          - pytorch-from-src
      - tags: install,onednn,from.src,_for-intel-mlperf-inference-v3.1-bert
        names:
          - onednn-from-src
      - tags: install,transformers,from.src,_for-intel-mlperf-inference-v3.1-bert
        names:
          - transformers-from-src

  gptj_:
    env:
      CM_BENCHMARK: STANDALONE_GPTJ

  int4,gptj_,build-harness:
    deps:
      - tags: reproduce,mlperf,inference,intel,harness,_calibration
        inherit_variation_tags: true
        names:
          - calibration
        skip_inherit_variation_groups:
          - run-mode
          - device-info
          - sut
          - loadgen-batchsize
        enable_if_env:
          CM_MLPERF_INFERENCE_CODE_VERSION:
          - v3.1
        force_cache: true
      - tags: get,generic-python-lib,_package.optimum
        names:
          - pip-package
          - optimum

  sdxl:
    group: model
    env:
      CM_BENCHMARK: STANDALONE_SDXL
      CM_MODEL: stable-diffusion-xl

  sdxl,pytorch:
    adr:
      conda-package:
        tags: _name.sdxl-pt
    deps:
      - tags: get,conda,_name.sdxl-pt
      - tags: get,python,_conda.sdxl-pt
        adr:
          conda-python:
            version: "3.9"
      - names:
        - conda-package
        - mkl
        tags: get,generic,conda-package,_package.mkl,_source.conda-forge
      - names:
        - conda-package
        - mkl-include
        tags: get,generic,conda-package,_package.mkl-include,_source.intel
      - names:
        - conda-package
        - llvm-openmp
        tags: get,generic,conda-package,_package.llvm-openmp,_source.conda-forge
      - names:
        - conda-package
        - ncurses
        tags: get,generic,conda-package,_package.ncurses,_source.conda-forge
      - tags: get,generic-sys-util,_numactl
      - tags: get,generic,conda-package,_package.jemalloc,_source.conda-forge
        names:
        - conda-package
        - jemalloc
      - tags: get,generic-python-lib,_package.torch,_path.https://download.pytorch.org/whl/nightly/cpu/torch-2.3.0.dev20231214%2Bcpu-cp39-cp39-linux_x86_64.whl
        names:
        - pip-package
        - pip-torch
      - tags: get,generic-python-lib,_package.torchvision,_path.https://download.pytorch.org/whl/nightly/cpu/torchvision-0.18.0.dev20231214%2Bcpu-cp39-cp39-linux_x86_64.whl
        names:
        - pip-package
        - pip-torchvision
      - tags: get,generic-python-lib,_torch
        names:
        - pip-package
        - torch
      - tags: install,diffusers,from.src,_for-intel-mlperf-inference-v4.0-sdxl
        names:
          - diffusers-from-src
      - tags: install,ipex,from.src,_for-intel-mlperf-inference-v4.0-sdxl
        names:
          - ipex-from-src
      - tags: get,generic,conda-package,_package.ninja
        names:
        - conda-package
        - ninja
      - tags: get,mlcommons,inference,src
        names:
        - inference-src
      - tags: get,mlcommons,inference,loadgen,_custom-python,_keep-build
        names:
        - inference-loadgen

  sdxl,build-harness:
    deps:
      - tags: get,generic-python-lib,_package.pybind11[global]
        names:
          - pip-package
          - pybind11

  sdxl,run-harness:
    deps:
      - tags: get,ml-model,sdxl,_fp32,_pytorch
      - tags: get,dataset,coco2014,original,_validation
      - tags: get,generic-python-lib,_package.opencv-python
        names:
          - pip-package
          - opencv
      - tags: get,generic-python-lib,_package.transformers
        names:
          - pip-package
          - transformers
      - tags: get,generic-python-lib,_package.accelerate
        names:
          - pip-package
          - accelerate
      - tags: get,generic-python-lib,_package.open-clip-torch
        names:
          - pip-package
          - open-clip-torch
      - tags: get,generic-python-lib,_package.pycocotools
        names:
          - pip-package
          - pycocotools
      - tags: get,generic-python-lib,_package.torchmetrics[image]
        names:
          - pip-package
          - torchmetrics
      - tags: get,generic-python-lib,_torchvision
        version: "0.17.1"
        names:
          - pip-package
          - torchvision
      - tags: get,generic-python-lib,_package.py-libnuma
        names:
          - pip-package
          - libnuma





  resnet50,pytorch:
    adr:
      conda-package:
        tags: _name.resnet50-pt
    deps:
      - tags: get,conda,_name.resnet50-pt
      - tags: get,python,_conda.resnet50-pt
        adr:
          conda-python:
            version: "3.9"
      - names:
        - conda-package
        - mkl
        tags: get,generic,conda-package,_package.mkl,_source.conda-forge
      - names:
        - conda-package
        - mkl-include
        tags: get,generic,conda-package,_package.mkl-include,_source.intel
      - names:
        - conda-package
        - llvm-openmp
        tags: get,generic,conda-package,_package.llvm-openmp,_source.conda-forge
      - names:
        - conda-package
        - ncurses
        tags: get,generic,conda-package,_package.ncurses,_source.conda-forge
      - tags: get,generic-sys-util,_numactl
      - tags: get,generic,conda-package,_package.jemalloc,_source.conda-forge
        names:
        - conda-package
        - jemalloc
      - tags: get,generic-python-lib,_package.torchvision,_no-deps
        names:
          - pip-package
          - torchvision
        version: "0.13.0"
      - tags: get,pytorch,from.src,_for-intel-mlperf-inference-resnet50
      - tags: install,opencv,from.src,_branch.4.x
        names:
          - opencv-from-src
      - tags: get,git,repo,_repo.https://github.com/Tencent/rapidjson.git,_sha.e4bde977
        names:
          - rapidjson-src
        env:
          CM_GIT_CHECKOUT_PATH_ENV_NAME: CM_RAPIDJSON_SRC_REPO_PATH
      - tags: install,gflags,from.src
        names:
          - gflags-from-src
      - tags: install,onednn,from.src,_branch.rls-v2.6
        names:
          - onednn-from-src
      - tags: get,generic-python-lib,_package.scikit-learn
        names:
          - pip-package
          - scikit-learn
      - tags: install,ipex,from.src,_for-intel-mlperf-inference-v3.1-resnet50
        names:
          - ipex-from-src
      - tags: get,generic,conda-package,_package.ninja
        names:
        - conda-package
        - ninja
      - tags: get,mlcommons,inference,src
        names:
        - inference-src
      - tags: get,mlcommons,inference,loadgen,_custom-python,_keep-build
        names:
        - inference-loadgen
 

  resnet50,build-harness:
    deps: 
    - tags: reproduce,mlperf,inference,intel,_compile-model
      inherit_variation_tags: true
      force_cache: true
      skip_inherit_variation_groups:
        - run-mode
        - loadgen-scenario

  resnet50,compile-model:
    deps: 
    - tags: reproduce,mlperf,inference,intel,_calibration
      inherit_variation_tags: true
      force_cache: true
      skip_inherit_variation_groups:
        - run-mode
        - loadgen-scenario

  retinanet,pytorch:
    adr:
      conda-package:
        tags: _name.retinanet-pt
      compiler:
        tags: gcc
      conda-python:
        version: "3.9"
    deps:
      - tags: get,conda,_name.retinanet-pt
      - tags: get,python,_conda.retinanet-pt
      - names:
        - conda-package
        - mkl
        tags: get,generic,conda-package,_package.mkl,_source.conda-forge
      - names:
          - conda-package
          - libstdcxx-ng
        tags: get,generic,conda-package,_package.libstdcxx-ng,_source.conda-forge

      - names:
        - conda-package
        - mkl
        tags: get,generic,conda-package,_package.mkl,_source.intel
      - names:
        - conda-package
        - mkl-include
        tags: get,generic,conda-package,_package.mkl-include,_source.intel
      - names:
        - conda-package
        - intel-openmp
        tags: get,generic,conda-package,_package.intel-openmp,_source.intel
      - names:
        - conda-package
        - llvm-openmp
        tags: get,generic,conda-package,_package.llvm-openmp,_source.conda-forge
      - names:
        - conda-package
        - ncurses
        tags: get,generic,conda-package,_package.ncurses,_source.conda-forge
      - tags: get,generic-sys-util,_numactl
      - tags: get,generic,conda-package,_package.cmake,_source.conda-forge
        names:
        - conda-package
        - cmake
      - tags: get,generic,conda-package,_package.jemalloc,_source.conda-forge
        names:
        - conda-package
        - jemalloc
      - tags: get,pytorch,from.src,_for-intel-mlperf-inference-retinanet
        names:
          - pytorch
      - tags: get,torchvision,from.src,_sha.8e078971b8aebdeb1746fea58851e3754f103053
        update_tags_from_env_with_prefix:
          "_python.":
            - CM_PYTHON_BIN_WITH_PATH
        names:
          - torchvision
      - tags: install,opencv,from.src,_branch.4.x
        names:
          - opencv-from-src
      - tags: get,git,repo,_repo.https://github.com/Tencent/rapidjson.git,_sha.e4bde977
        names:
          - rapidjson-src
        env:
          CM_GIT_CHECKOUT_PATH_ENV_NAME: CM_RAPIDJSON_SRC_REPO_PATH
      - tags: install,gflags,from.src
        names:
          - gflags-from-src
      - tags: install,onednn,from.src,_branch.rls-v2.6
        names:
          - onednn-from-src
      - tags: get,generic-python-lib,_package.scikit-learn
        names:
          - pip-package
          - scikit-learn
      - tags: get,generic-python-lib,_package.opencv-python
        names:
          - pip-package
          - opencv-python
      - tags: get,generic-python-lib,_package.pycocotools
        names:
          - pip-package
          - pycocotools
      - tags: install,ipex,from.src,_for-intel-mlperf-inference-v3.1-retinanet
        names:
          - ipex-from-src
      - tags: get,generic,conda-package,_package.ninja
        names:
        - conda-package
        - ninja
      - tags: get,mlcommons,inference,src
        names:
        - inference-src
      - tags: get,mlcommons,inference,loadgen,_custom-python,_keep-build
        names:
        - inference-loadgen
 

  retinanet,build-harness:
    deps: 
    - tags: reproduce,mlperf,inference,intel,_compile-model
      inherit_variation_tags: true
      force_cache: true
      skip_inherit_variation_groups:
        - run-mode
        - loadgen-scenario
      new_env_keys:
        - CM_ML_MODEL_RETINANET_INT8_FILE_WITH_PATH

  retinanet,compile-model:
    deps: 
    - tags: get,ml-model,retinanet,_pytorch,_fp32
    new_env_keys:
      - CM_ML_MODEL_RETINANET_INT8_FILE_WITH_PATH

  3d-unet_,pytorch:
    adr:
      conda-package:
        tags: _name.3d-unet-pt
    deps:
      - tags: get,generic-sys-util,_libffi7
      - tags: get,conda,_name.3d-unet-pt
      - tags: get,python,_conda.3d-unet-pt
        adr:
          conda-python:
            version: "3.8"
      - names:
        - conda-package
        - mkl
        tags: get,generic,conda-package,_package.mkl,_source.conda-forge
      - names:
        - conda-package
        - mkl-include
        tags: get,generic,conda-package,_package.mkl-include,_source.intel
      - names:
        - conda-package
        - mkl-service
        tags: get,generic,conda-package,_package.mkl-service,_source.intel
      - names:
        - conda-package
        - mkl_fft
        tags: get,generic,conda-package,_package.mkl_fft,_source.intel
      - names:
        - conda-package
        - mkl_random
        tags: get,generic,conda-package,_package.mkl_random,_source.intel
      - names:
        - conda-package
        - ncurses
        tags: get,generic,conda-package,_package.ncurses,_source.conda-forge
      - tags: get,generic-sys-util,_numactl
      - tags: get,generic,conda-package,_package.jemalloc,_source.conda-forge
        names:
        - conda-package
        - jemalloc
      - tags: install,ipex,from.src,_for-intel-mlperf-inference-v3.1-3d-unet
        names:
          - ipex-from-src
      - tags: get,generic,conda-package,_package.ninja
        names:
        - conda-package
        - ninja
      - tags: get,mlcommons,inference,src
        names:
        - inference-src
      - tags: get,mlcommons,inference,loadgen,_custom-python,_keep-build
        names:
        - inference-loadgen
 

  3d-unet_,build-harness:
    deps: 
    - tags: reproduce,mlperf,inference,intel,_compile-model
      inherit_variation_tags: true
      force_cache: true
      skip_inherit_variation_groups:
        - run-mode
        - loadgen-scenario

  3d-unet_,compile-model:
    deps: 
    - tags: reproduce,mlperf,inference,intel,_calibration
      inherit_variation_tags: true
      force_cache: true
      skip_inherit_variation_groups:
        - run-mode
        - loadgen-scenario

  gptj_,pytorch:
    adr:
      conda-package:
        tags: _name.gptj-pt
    deps:
      - tags: get,conda,_name.gptj-pt
      - tags: get,python,_conda.gptj-pt
        adr:
          conda-python:
            version: "3.9"
      - names:
        - conda-package
        - mkl
        tags: get,generic,conda-package,_package.mkl,_source.conda-forge
        enable_if_env:
          CM_MLPERF_INFERENCE_CODE_VERSION:
          -  v4.0
      - names:
        - conda-package
        - mkl-include
        tags: get,generic,conda-package,_package.mkl-include,_source.conda-forge
        enable_if_env:
          CM_MLPERF_INFERENCE_CODE_VERSION:
          -  v4.0
      - names:
        - conda-package
        - llvm-openmp
        tags: get,generic,conda-package,_package.llvm-openmp,_source.conda-forge
        enable_if_env:
          CM_MLPERF_INFERENCE_CODE_VERSION:
          -  v4.0
      - names:
        - conda-package
        - pybind11
        tags: get,generic,conda-package,_package.pybind11,_source.conda-forge
        enable_if_env:
          CM_MLPERF_INFERENCE_CODE_VERSION:
          -  v4.0
      - names:
        - conda-package
        - ncurses
        tags: get,generic,conda-package,_package.ncurses,_source.conda-forge
      - tags: install,llvm,src,_for-intel-mlperf-inference-v3.1-gptj
        names:
          - llvm-from-src
        enable_if_env:
          CM_MLPERF_INFERENCE_CODE_VERSION:
          -  v3.1
      - names:
        - conda-package
        - ncurses
        tags: get,generic,conda-package,_package.ncurses,_source.conda-forge
      - tags: get,generic-sys-util,_numactl
      - tags: get,generic,conda-package,_package.jemalloc,_source.conda-forge
        names:
        - conda-package
        - jemalloc
      - tags: install,ipex,from.src,_for-intel-mlperf-inference-v3.1-gptj
        names:
          - ipex-from-src
        enable_if_env:
          CM_MLPERF_INFERENCE_CODE_VERSION:
          -  v3.1
      - tags: get,generic,conda-package,_package.ninja
        names:
        - conda-package
        - ninja
        enable_if_env:
          INTEL_GPTJ_INT4:
            - 'yes'
      - tags: install,tpp-pex,from.src,_for-intel-mlperf-inference-v3.1-gptj
        names:
          - tpp-pex-from-src
        enable_if_env:
          INTEL_GPTJ_INT4:
            - 'yes'
          CM_MLPERF_INFERENCE_CODE_VERSION:
          -  v3.1
      - tags: get,generic-python-lib,_package.transformers
        names:
          - pip-package
          - transformers
        version: "4.28.1"
      - tags: get,mlcommons,inference,src
        names:
        - inference-src
      - tags: get,mlcommons,inference,loadgen,_custom-python
        names:
        - inference-loadgen
      - tags: get,ml-model,large-language-model,gptj
        names:
        - ml-model
        - gptj-model
        - gpt-j-model
      - tags: get,generic-python-lib,_package.datasets
        names:
          - pip-package
          - datasets
      - tags: get,generic-python-lib,_package.accelerate
        names:
          - pip-package
          - accelerate
      - tags: get,generic-python-lib,_custom-python,_package.torch,_url.git+https://github.com/pytorch/pytorch.git@927dc662386af052018212c7d01309a506fc94cd
        enable_if_env:
          CM_MLPERF_INFERENCE_CODE_VERSION:
          -  v3.1
        env:
          "+ CXXFLAGS":
            - "-Wno-nonnull"
            - "-Wno-maybe-uninitialized"
            - "-Wno-uninitialized"
            - "-Wno-free-nonheap-object"
      - tags: get,generic-python-lib,_custom-python,_package.torch
        env:
          CM_GENERIC_PYTHON_PIP_EXTRA_INDEX_URL: https://download.pytorch.org/whl/cpu
        enable_if_env:
          CM_MLPERF_INFERENCE_CODE_VERSION:
            - 'v4.0'
      - tags: install,intel-neural-speed,_for-intel-mlperf-inference-v4.0-gptj,_branch.mlperf-v4-0
        enable_if_env:
          CM_MLPERF_INFERENCE_CODE_VERSION:
            - 'v4.0'


  gptj-99:
    group: model
    base:
    - gptj_
    env:
      CM_MODEL: gptj-99
      CM_NOT_ML_MODEL_STARTING_WEIGHTS_FILENAME: "https://zenodo.org/record/3733910/files/model.onnx"
      CM_ML_MODEL_WEIGHTS_DATA_TYPE: int8
      CM_ML_MODEL_INPUTS_DATA_TYPE: int8

  gptj-99.9:
    group: model
    base:
    - gptj_
    env:
      CM_MODEL: gptj-99.9
      CM_NOT_ML_MODEL_STARTING_WEIGHTS_FILENAME: "https://zenodo.org/record/3733910/files/model.onnx"

  dlrm-v2_,build-harness:
    deps: 
    - tags: reproduce,mlperf,inference,intel,_calibration
      inherit_variation_tags: true
      force_cache: true
      skip_inherit_variation_groups:
        - run-mode
        - loadgen-scenario

  dlrm-v2_,pytorch:
    adr:
      conda-package:
        tags: _name.dlrm-v2-pt
    deps:
      - tags: get,conda,_name.dlrm-v2-pt
      - tags: get,python,_conda.dlrm-v2-pt
        adr:
          conda-python:
            version: "3.9"
      - names:
        - conda-package
        - mkl
        tags: get,generic,conda-package,_package.mkl,_source.conda-forge
      - names:
        - conda-package
        - mkl-include
        tags: get,generic,conda-package,_package.mkl-include,_source.conda-forge
      - names:
        - conda-package
        - llvm-openmp
        tags: get,generic,conda-package,_package.llvm-openmp,_source.conda-forge
        enable_if_env:
          CM_MLPERF_INFERENCE_CODE_VERSION:
          -  v4.0
      - names:
        - conda-package
        - pybind11
        tags: get,generic,conda-package,_package.pybind11,_source.conda-forge
        enable_if_env:
          CM_MLPERF_INFERENCE_CODE_VERSION:
          -  v4.0
      - names:
        - conda-package
        - ncurses
        tags: get,generic,conda-package,_package.ncurses,_source.conda-forge
      - names:
        - conda-package
        - ncurses
        tags: get,generic,conda-package,_package.ncurses,_source.conda-forge
      - tags: get,generic-sys-util,_numactl
      - tags: get,generic,conda-package,_package.jemalloc,_source.conda-forge
        names:
        - conda-package
        - jemalloc
      - tags: install,ipex,from.src,_for-intel-mlperf-inference-v3.1-dlrm-v2
        names:
          - ipex-from-src
      - tags: get,mlcommons,inference,src
        names:
        - inference-src
      - tags: get,mlcommons,inference,loadgen,_custom-python
        names:
        - inference-loadgen
      - tags: get,ml-model,dlrm,_pytorch
        names:
        - ml-model
        - dlrm-v2-model
        - dlrm_v2-model
      - tags: get,generic-python-lib,_package.absl-py
        names:
          - pip-package
          - absl-py
      - tags: get,generic-python-lib,_package.accelerate
        names:
          - pip-package
          - accelerate
      - tags: install,pytorch,from-src,_for-intel-mlperf-inference-v3.1-dlrm-v2
        names: 
         - pytorch
         - torch
  dlrm-v2_:
    env: {}

  dlrm-v2-99:
    group: model
    base:
    - dlrm-v2_
    env:
      CM_MODEL: dlrm-v2-99
      CM_ML_MODEL_WEIGHTS_DATA_TYPE: int8
      CM_ML_MODEL_INPUTS_DATA_TYPE: int8

  dlrm-v2-99.9:
    group: model
    base:
    - dlrm-v2_
    env:
      CM_MODEL: dlrm-v2-99.9

  standalone:
    group: network-mode
    default: true
    env:
      CM_MLPERF_NETWORK_RUN_MODE: standalone

  network-server:
    group: network-mode
    env:
      CM_MLPERF_NETWORK_RUN_MODE: network-server

  network-client:
    group: network-run-mode
    env:
      CM_MLPERF_NETWORK_RUN_MODE: network-client

  bert_,network-server:
    env:
      CM_BENCHMARK: NETWORK_BERT_SERVER

  bert_,network-client:
    env:
      CM_BENCHMARK: NETWORK_BERT_CLIENT

  bert-99:
    group: model
    base:
    - bert_
    env:
      CM_MODEL: bert-99
      CM_SQUAD_ACCURACY_DTYPE: float32
      CM_NOT_ML_MODEL_STARTING_WEIGHTS_FILENAME: "https://zenodo.org/record/3750364/files/bert_large_v1_1_fake_quant.onnx"

  bert-99.9:
    group: model
    base:
    - bert_
    env:
      CM_MODEL: bert-99.9
      CM_NOT_ML_MODEL_STARTING_WEIGHTS_FILENAME: "https://zenodo.org/record/3733910/files/model.onnx"

  batch_size.#:
    group: loadgen-batchsize
    env:
      CM_MLPERF_LOADGEN_BATCH_SIZE: "#"


  build-harness:
    docker:
      real_run: false
    group: run-mode
    env:
      CM_LOCAL_MLPERF_INFERENCE_INTEL_RUN_MODE: build_harness
    new_env_keys:
      - CM_MLPERF_INFERENCE_INTEL_HARNESS_PATH
      - CM_ML_MODEL_*
      - DATA_PATH

  compile-model:
    group: run-mode
    env:
      CM_LOCAL_MLPERF_INFERENCE_INTEL_RUN_MODE: compilation

  calibration:
    group: run-mode
    env:
      CM_LOCAL_MLPERF_INFERENCE_INTEL_RUN_MODE: calibration
    new_env_keys:
      - CM_ML_MODEL_*
      - INT4_CALIBRATION_DIR

  calibration,gptj_:
    deps: []

  build-harness,bert_:
    deps:
      - tags: get,generic-sys-util,_rsync
      - tags: get,dataset,original,squad
        names:
          - squad-original
      - tags: get,ml-model,bert-large,_pytorch,_int8
        names:
          - bert-large
          - ml-model
      - tags: get,generic-python-lib,_package.tokenization 


  run-harness:
    docker:
      real_run: false
    group: run-mode
    default: true
    deps:
      - tags: reproduce,mlperf,inference,intel,harness,_build-harness
        inherit_variation_tags: true
        names:
          - build-harness
        skip_inherit_variation_groups:
          - run-mode
          - device-info
          - sut
          - loadgen-batchsize
          - loadgen-scenario
        force_cache: true
  
      # Download MLPerf inference source
      - tags: get,mlcommons,inference,src
        names:
        - inference-src

      # Creates user conf for given SUT
      - tags: generate,user-conf,mlperf,inference
        names:
        - user-conf-generator
      - tags: get,generic-sys-util,_rsync

    env:
      CM_LOCAL_MLPERF_INFERENCE_INTEL_RUN_MODE: run_harness

    # Env keys which are exposed to higher level scripts
    new_env_keys:
      - CM_MLPERF_*
      - CM_DATASET_*
      - CM_HW_NAME
      - CM_ML_MODEL_*
      - CM_MAX_EXAMPLES
      - CM_IMAGENET_ACCURACY_DTYPE
      - CM_SQUAD_ACCURACY_DTYPE



  maxq:
    group: power-mode
    env:
      CM_MLPERF_NVIDIA_HARNESS_MAXQ: yes

  maxn:
    group: power-mode
    env:
      CM_MLPERF_NVIDIA_HARNESS_MAXN: yes

  singlestream:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: SingleStream

  multistream:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: MultiStream
  offline:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: Offline
  server:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: Server

  int4:
    group: precision

  uint8:
    group: precision
    adr:
      dataset-preprocessed:
        tags: _uint8,_rgb8

  int8:
    alias: uint8

  int4,gptj_:
    env:
      INTEL_GPTJ_INT4: 'yes'

  int8,gptj_:
    env:
      INTEL_GPTJ_INT4: 'no'

  fp32:
    group: precision
    adr:
      dataset-preprocessed:
        tags: _float32,_rgb32
    env:
      CM_IMAGENET_ACCURACY_DTYPE: float32

  sapphire-rapids.112c:
    group: sut
    env:
      WARMUP: " --warmup"

  sapphire-rapids.24c:
    group: sut

  sapphire-rapids.24c,gptj-99,offline,int8:
    env:
      KMP_BLOCKTIME: 10
      WORKERS_PER_PROC: 1
    default_env:
      CM_MLPERF_LOADGEN_BATCH_SIZE: 8

  sapphire-rapids.24c,gptj-99,offline,int4:
    env:
      KMP_BLOCKTIME: 10
      WORKERS_PER_PROC: 1
    default_env:
      CM_MLPERF_LOADGEN_BATCH_SIZE: 8

  sapphire-rapids.112c,gptj-99,offline,int8:
    env:
      KMP_BLOCKTIME: 1
      WORKERS_PER_PROC: 2
    default_env:
      CM_MLPERF_LOADGEN_BATCH_SIZE: 14

  sapphire-rapids.112c,gptj-99,offline,int4:
    env:
      NUM_PROC: 4
      KMP_BLOCKTIME: 1
      WORKERS_PER_PROC: 3
    default_env:
      CM_MLPERF_LOADGEN_BATCH_SIZE: 8

  sapphire-rapids.112c,gptj-99,server,int8:
    env:
      KMP_BLOCKTIME: 1
      WORKERS_PER_PROC: 2
    default_env:
      CM_MLPERF_LOADGEN_BATCH_SIZE: 1

  sapphire-rapids.112c,gptj-99,server,int4:
    env:
      KMP_BLOCKTIME: 1
      WORKERS_PER_PROC: 4
    default_env:
      CM_MLPERF_LOADGEN_BATCH_SIZE: 1

  sapphire-rapids.24c,bert_:
    env:
      WORKERS_PER_PROC: 1
  sapphire-rapids.112c,bert_,offline:
    env:
      WORKERS_PER_PROC: 4
  sapphire-rapids.112c,bert_,server:
    env:
      WORKERS_PER_PROC: 8


docker:
  real_run: False
