# Identification of this CM script
alias: app-mlperf-inference-mlcommons-cpp
uid: bf62405e6c7a44bf

automation_alias: script
automation_uid: 5b4e0237da074764

category: "Modular MLPerf inference benchmark pipeline"

developers: "[Thomas Zhu](https://www.linkedin.com/in/hanwen-zhu-483614189), [Arjun Suresh](https://www.linkedin.com/in/arjunsuresh), [Grigori Fursin](https://cKnowledge.org/gfursin)"

# User-friendly tags to find this CM script
tags:
  - app
  - mlcommons
  - mlperf
  - inference
  - cpp



# Default environment
default_env:
  CM_BATCH_COUNT: '1'
  CM_BATCH_SIZE: '1'
  CM_FAST_COMPILATION: "yes"
  CM_MLPERF_SUT_NAME_IMPLEMENTATION_PREFIX: cpp


# Map script inputs to environment variables
input_mapping:
  count: CM_MLPERF_LOADGEN_QUERY_COUNT
  max_batchsize: CM_MLPERF_LOADGEN_MAX_BATCHSIZE
  mlperf_conf: CM_MLPERF_CONF
  mode: CM_MLPERF_LOADGEN_MODE
  output_dir: CM_MLPERF_OUTPUT_DIR
  performance_sample_count: CM_MLPERF_LOADGEN_PERFORMANCE_SAMPLE_COUNT
  scenario: CM_MLPERF_LOADGEN_SCENARIO
  user_conf: CM_MLPERF_USER_CONF

new_env_keys:
  - CM_MLPERF_*
  - CM_DATASET_*
  - CM_ML_MODEL_*
  - CM_HW_NAME

new_state_keys:
  - mlperf-inference-implementation
  - CM_SUT_*

# Dependencies on other CM scripts

deps:

  # Detect host OS features
  - tags: detect,os

  # Detect host CPU features
  - tags: detect,cpu

  # Install system dependencies on a given host
  - tags: get,sys-utils-cm

  # Detect CUDA if required
  - tags: get,cuda,_cudnn
    enable_if_env:
      CM_MLPERF_DEVICE:
      - gpu

  ########################################################################
  # Install MLPerf inference dependencies
  
  # Install MLPerf loadgen
  - tags: get,loadgen
    names:
    - loadgen
    
  # Download MLPerf inference source
  - tags: get,mlcommons,inference,src
    names:
    - inference-src

  ########################################################################
  # Install ML engines via CM
  - enable_if_env:
      CM_MLPERF_BACKEND:
      - onnxruntime
      CM_MLPERF_DEVICE:
      - cpu
    tags: get,lib,onnxruntime,lang-cpp,_cpu

  - enable_if_env:
      CM_MLPERF_BACKEND:
      - onnxruntime
      CM_MLPERF_DEVICE:
      - gpu
    tags: get,lib,onnxruntime,lang-cpp,_cuda


  ########################################################################
  # Install ResNet50 model (ONNX) and ImageNet

  - enable_if_env:
      CM_MODEL:
      - resnet50
    names:
      - imagenet-preprocessed
    tags: get,dataset,preprocessed,imagenet,_NCHW

  - enable_if_env:
      CM_MODEL:
      - resnet50
    tags: get,ml-model,raw,resnet50,_onnx


  ########################################################################
  # Install RetinaNet model (ONNX) and OpenImages

  - enable_if_env:
      CM_MODEL:
      - retinanet
    names:
      - openimages-preprocessed
    tags: get,dataset,preprocessed,openimages,_validation,_NCHW

  - enable_if_env:
      CM_MODEL:
      - retinanet
    tags: get,ml-model,retinanet,_onnx,_fp32

  # Creates user conf for given SUT
  - tags: generate,user-conf,mlperf,inference
    names:
    - user-conf-generator
  

# Post dependencies to compile and run this app
post_deps:

  - names:
    - compile-program
    tags: compile,cpp-program
    skip_if_env:
      CM_MLPERF_SKIP_RUN:
        - "yes"

  - names:
    - mlperf-runner
    tags: benchmark-mlperf
    skip_if_env:
      CM_MLPERF_SKIP_RUN:
        - "yes"

  - tags: save,mlperf,inference,state
    names:
      - save-mlperf-inference-state

# Variations to customize dependencies
variations:
  # Target devices
  cpu:
    group: device
    default: true
    env:
      CM_MLPERF_DEVICE: cpu
  cuda:
    group: device
    env:
      CM_MLPERF_DEVICE: gpu
      CM_MLPERF_DEVICE_LIB_NAMESPEC: cudart

  # ML engine
  onnxruntime:
    group: framework
    default: true
    env:
      CM_MLPERF_BACKEND: onnxruntime
      CM_MLPERF_BACKEND_LIB_NAMESPEC: onnxruntime

  pytorch:
    group: framework
    env:
      CM_MLPERF_BACKEND: pytorch

  tf:
    group: framework
    env:
      CM_MLPERF_BACKEND: tf

  tflite:
    group: framework
    env:
      CM_MLPERF_BACKEND: tflite

  tvm-onnx:
    group: framework
    env:
      CM_MLPERF_BACKEND: tvm-onnx

  # Reference MLPerf models
  resnet50:
    group: model
    default: true
    env:
      CM_MODEL: resnet50

  retinanet:
    group: model
    default_env:
      CM_MLPERF_LOADGEN_MAX_BATCHSIZE: 1
    env:
      CM_MODEL: retinanet

  resnet50,offline:
    default_env:
      CM_MLPERF_LOADGEN_MAX_BATCHSIZE: 32

  resnet50,server:
    default_env:
      CM_MLPERF_LOADGEN_MAX_BATCHSIZE: 32

  resnet50,multistream:
    default_env:
      CM_MLPERF_LOADGEN_MAX_BATCHSIZE: 8

  batch-size.#:
    group: batch-size
    env:
      CM_MLPERF_LOADGEN_MAX_BATCHSIZE: "#"

  offline:
    group: loadgen-scenario
    default: true
    env:
      CM_MLPERF_LOADGEN_SCENARIO: Offline
  multistream:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: MultiStream
  singlestream:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: SingleStream
      CM_MLPERF_LOADGEN_MAX_BATCHSIZE: 1
  server:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: Server

  multistream,resnet50:
    default_variations:
      batch-size: batch-size.8

  offline,resnet50:
    default_variations:
      batch-size: batch-size.32

  multistream,retinanet:
    default_variations:
      batch-size: batch-size.1
