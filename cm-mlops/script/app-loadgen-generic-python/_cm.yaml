# Identification of this CM script
alias: app-loadgen-generic-python
uid: d3d949cc361747a6

automation_alias: script
automation_uid: 5b4e0237da074764

category: "Modular MLPerf inference benchmark pipeline"

developers: "[Gaz Iqbal](https://www.linkedin.com/in/gaziqbal), [Arjun Suresh](https://www.linkedin.com/in/arjunsuresh), [Grigori Fursin](https://cKnowledge.org/gfursin)"


# User-friendly tags to find this CM script
tags:
  - app
  - loadgen
  - generic
  - loadgen-generic
  - python

tags_help: "python app generic loadgen"


# Default environment
default_env:
  CM_MLPERF_EXECUTION_MODE: parallel
  CM_MLPERF_BACKEND: onnxruntime

# Map script inputs to environment variables
input_mapping:
  modelpath: CM_ML_MODEL_FILE_WITH_PATH
  modelcodepath: CM_ML_MODEL_CODE_WITH_PATH
  modelcfgpath: CM_ML_MODEL_CFG_WITH_PATH
  modelcfg: CM_ML_MODEL_CFG
  modelsamplepath: CM_ML_MODEL_SAMPLE_WITH_PATH
  output_dir: CM_MLPERF_OUTPUT_DIR
  scenario: CM_MLPERF_LOADGEN_SCENARIO
  runner: CM_MLPERF_RUNNER
  concurrency: CM_MLPERF_CONCURRENCY
  ep: CM_MLPERF_EXECUTION_PROVIDER
  intraop: CM_MLPERF_INTRAOP
  interop: CM_MLPERF_INTEROP
  execmode: CM_MLPERF_EXEC_MODE
  samples: CM_MLPERF_LOADGEN_SAMPLES
  loadgen_expected_qps: CM_MLPERF_LOADGEN_EXPECTED_QPS
  loadgen_duration_sec: CM_MLPERF_LOADGEN_DURATION_SEC

# New env keys exported from this script
new_env_keys:
  - CM_MLPERF_*

# Dependencies on other CM scripts

deps:

  # Detect host OS features
  - tags: detect,os

  # Detect host CPU features
  - tags: detect,cpu

  # Get Python
  - tags: get,python3
    names:
      - python
      - python3

  # Extra package
  - tags: get,generic-python-lib,_psutil
  - tags: get,generic-python-lib,_package.numpy
    version_max: "1.99.99"

  # Detect CUDA if required
  - tags: get,cuda
    enable_if_env:
      CM_MLPERF_DEVICE:
      - gpu

  # Install loadgen
  - tags: get,loadgen
    names:
    - loadgen

  ########################################################################
  # Install ML engines via CM
  # ONNX
  - enable_if_env:
      CM_MLPERF_BACKEND:
      - onnxruntime
      CM_MLPERF_DEVICE:
      - cpu
    tags: get,generic-python-lib,_onnxruntime
    names:
    - onnxruntime

  - enable_if_env:
      CM_MLPERF_BACKEND:
      - onnxruntime
      CM_MLPERF_DEVICE:
      - gpu
    tags: get,generic-python-lib,_onnxruntime_gpu
    names:
    - onnxruntime

  - enable_if_env:
      CM_MLPERF_BACKEND:
      - onnxruntime
    tags: get,generic-python-lib,_onnx
    names:
    - onnx

  ########################################################################
  # Install ML engines via CM
  # PyTorch

  # CPU
  
  - enable_if_env:
      CM_MLPERF_BACKEND:
      - pytorch
      CM_MLPERF_DEVICE:
      - cpu
    tags: get,generic-python-lib,_torch
    names:
    - torch

  - enable_if_env:
      CM_MLPERF_BACKEND:
      - pytorch
      CM_MLPERF_DEVICE:
      - cpu
    tags: get,generic-python-lib,_torchvision
    names:
    - torchvision

  # CUDA/GPU
  
  - enable_if_env:
      CM_MLPERF_BACKEND:
      - pytorch
      CM_MLPERF_DEVICE:
      - gpu
    tags: get,generic-python-lib,_torch_cuda
    names:
    - torch

  - enable_if_env:
      CM_MLPERF_BACKEND:
      - pytorch
      CM_MLPERF_DEVICE:
      - gpu
    tags: get,generic-python-lib,_torchvision_cuda
    names:
    - torchvision



  ########################################################################
  # Install MLPerf models
  - enable_if_env:
      CM_MODEL:
      - resnet50
    tags: get,ml-model,resnet50,_onnx

  - enable_if_env:
      CM_MODEL:
      - retinanet
    tags: get,ml-model,retinanet,_onnx,_fp32

  - enable_if_env:
      CM_MODEL:
      - retinanet
    tags: get,ml-model,retinanet,_onnx,_fp32




# Customize this CM script
variations:

  pytorch:
    group: backend
    env:
      CM_MLPERF_BACKEND:
        pytorch

  onnxruntime:
    group: backend
    default: true
    env:
      CM_MLPERF_BACKEND:
        onnxruntime



  cpu:
    group:
      device
    default:
      true
    env:
      CM_MLPERF_DEVICE:
        cpu
      CM_MLPERF_EXECUTION_PROVIDER:
        CPUExecutionProvider

  cuda:
    docker:
      all_gpus: 'yes'
      base_image: nvcr.io/nvidia/pytorch:24.03-py3
    group:
      device
    env:
      CM_MLPERF_DEVICE:
        gpu
      CM_MLPERF_EXECUTION_PROVIDER:
        CUDAExecutionProvider



  retinanet:
    group:
      models
    env:
      CM_MODEL: retinanet

  resnet50:
    group:
      models
    env:
      CM_MODEL: resnet50
  
  custom:
    group:
      models
    env:
      CM_MODEL: custom



  huggingface:
    env:
      CM_CUSTOM_MODEL_SOURCE: huggingface

  custom,huggingface:
    deps:
    - tags: get,ml-model,huggingface
      names:
      - hf-downloader
      update_tags_from_env_with_prefix:
        "_model-stub.":
        - CM_ML_MODEL_STUB

  model-stub.#:
    env:
      CM_ML_MODEL_STUB: "#"


  cmc:
    env:
      CM_CUSTOM_MODEL_CMC: yes


  custom,cmc:
    deps:
    - tags: get,ml-model,cmc
    names:
    - cmc-model


input_description:
  modelpath: 
    desc: Full path to file with model weights
  modelcodepath: 
    desc: (for PyTorch models) Full path to file with model code and cmc.py
  modelcfgpath: 
    desc: (for PyTorch models) Full path to JSON file with model cfg
  modelsamplepath: 
    desc: (for PyTorch models) Full path to file with model sample in pickle format
  ep: 
    desc: ONNX Execution provider
  scenario: 
    desc: MLPerf LoadGen scenario
  samples: 
    desc: Number of samples
    default: 2
  runner: 
    desc: MLPerf runner
  execmode: 
    desc: MLPerf exec mode
  output_dir: 
    desc: MLPerf output directory
  concurrency: 
    desc: MLPerf concurrency
  intraop: 
    desc: MLPerf intra op threads
  interop: 
    desc: MLPerf inter op threads


docker:
  skip_run_cmd: 'no'
  input_paths:
    - modelpath
    - modelsamplepath
    - env.CM_ML_MODEL_FILE_WITH_PATH
    - env.CM_ML_MODEL_CODE_WITH_PATH
    - output_dir
  skip_input_for_fake_run:
    - modelpath
    - modelsamplepath
    - env.CM_ML_MODEL_FILE_WITH_PATH
    - env.CM_ML_MODEL_CODE_WITH_PATH
    - output_dir
    - scenario
    - runner
    - concurrency
    - intraop
    - interop
    - execmode
    - samples
    - modelcfg.num_classes
    - modelcfg.config
