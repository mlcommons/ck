# Identification of this CM script
alias: build-mlperf-inference-server-nvidia
uid: f37403af5e9f4541
cache: true
automation_alias: script
automation_uid: 5b4e0237da074764
default_version: r3.1

category: "MLPerf benchmark support"


# User-friendly tags to find this CM script
tags:
  - build
  - mlcommons
  - mlperf
  - inference
  - inference-server
  - server
  - nvidia-harness
  - nvidia


new_env_keys:
  - CM_MLPERF_INFERENCE_NVIDIA_CODE_PATH

default_env:
  CM_MAKE_BUILD_COMMAND: build
  CM_MAKE_CLEAN: "no"
  CM_CUSTOM_SYSTEM_NVIDIA: "yes"

input_mapping:
  custom_system: CM_CUSTOM_SYSTEM_NVIDIA
  clean: CM_MAKE_CLEAN

# Dependencies on other CM scripts

deps:

  # Detect host OS features
  - tags: detect,os

  # Detect host CPU features
  - tags: detect,cpu

  # Install system dependencies on a given host
  - tags: get,sys-utils-cm

  # Detect python3
  - tags: get,python3
    names:
    - python
    - python3

  # Detect CUDA
  - tags: get,cuda,_cudnn
    names:
    - cuda
    enable_if_env:
      CM_MLPERF_DEVICE:
        - cuda
        - inferentia

  # Detect Tensorrt
  - tags: get,tensorrt,_dev
    names:
      - tensorrt
    enable_if_env:
      CM_MLPERF_DEVICE:
        - cuda
        - inferentia
    skip_if_env:
      CM_TENSORRT_SYSTEM_DETECT:
        - yes

  # Detect gcc
  - tags: get,gcc

  # Detect CMake
  - tags: get,cmake
    version_min: "3.25"

  # Detect Google Logger
  - tags: get,generic,sys-util,_glog-dev

  # Detect GFlags
  - tags: get,generic,sys-util,_gflags-dev

  # Detect libgmock-dev
  - tags: get,generic,sys-util,_libgmock-dev

  # Detect libre2-dev
  - tags: get,generic,sys-util,_libre2-dev

  # Detect libnuma-dev
  - tags: get,generic,sys-util,_libnuma-dev

  # Detect libboost-all-dev
  - tags: get,generic,sys-util,_libboost-all-dev

  # Detect rapidjson-dev
  - tags: get,generic,sys-util,_rapidjson-dev


  # Download Nvidia Submission Code
  - tags: get,nvidia,mlperf,inference,common-code
    names:
    - nvidia-inference-common-code

  - tags: get,generic-python-lib,_package.pybind11

  # Detect pycuda
  - tags: get,generic-python-lib,_pycuda
    skip_if_env:
      CM_RUN_STATE_DOCKER:
        - 'yes'
        - True
        - 'True'

  # Detect opencv-python
  - tags: get,generic-python-lib,_opencv-python

  # Detect nvidia-dali
  - tags: get,generic-python-lib,_nvidia-dali

  # Get Nvidia scratch space where data and models get downloaded
  - tags: get,mlperf,inference,nvidia,scratch,space
    names:
    - nvidia-scratch-space


post_deps:
  # Detect nvidia system
  - tags: add,custom,system,nvidia
    names:
    - custom-system-nvidia
    - nvidia-inference-common-code
    skip_if_env:
      CM_CUSTOM_SYSTEM_NVIDIA:
      - "no"
      - False
      - "False"
    enable_if_env:
      CM_RUN_STATE_DOCKER:
      - False

variations:
  # Target devices
  cpu:
    group: device
    env:
      CM_MLPERF_DEVICE: cpu
  inferentia:
    group: device
    env:
      CM_MLPERF_DEVICE: inferentia
  cuda:
    group: device
    default: true
    env:
      CM_MLPERF_DEVICE: cuda
      CM_MLPERF_DEVICE_LIB_NAMESPEC: cudart

  ctuning:
    group: code
    default: true
    add_deps_recursive:
      nvidia-inference-common-code:
        tags: _ctuning
  nvidia-only:
    group: code
    add_deps_recursive:
      nvidia-inference-common-code:
        tags: _nvidia-only
  custom:
    group: code
    add_deps_recursive:
      nvidia-inference-common-code:
        tags: _custom
  mlcommons:
    group: code
    add_deps_recursive:
      nvidia-inference-common-code:
        tags: _mlcommons


versions:
  r2.1:
    add_deps_recursive:
      nvidia-inference-common-code:
        version: r2.1
      nvidia-scratch-space:
        tags: _version.2_1

  r3.0:
    add_deps_recursive:
      nvidia-inference-common-code:
        version: r3.0
      nvidia-scratch-space:
        tags: _version.3_0

  r3.1:
    add_deps_recursive:
      nvidia-inference-common-code:
        version: r3.1
      nvidia-scratch-space:
        tags: _version.4_0
    deps:
      - tags: install,nccl,libs,_cuda
      - tags: install,pytorch,from.src,_for-nvidia-mlperf-inference-v3.1-gptj
        names:
        - pytorch

docker:
  skip_run_cmd: 'no'
  all_gpus: 'yes'
  shm_size: '32gb'
  extra_run_args: ' --runtime=nvidia --ulimit memlock=-1 --cap-add SYS_ADMIN --cap-add SYS_TIME --security-opt apparmor=unconfined --security-opt seccomp=unconfined'
  docker_os: ubuntu
  docker_real_run: False
  interactive: True
  docker_os_version: '20.04'
  base_image: nvcr.io/nvidia/mlperf/mlperf-inference:mlpinf-v3.1-cuda12.2-cudnn8.9-x86_64-ubuntu20.04-l4-public
  docker_input_mapping:
    imagenet_path: IMAGENET_PATH
    gptj_checkpoint_path: GPTJ_CHECKPOINT_PATH
    criteo_preprocessed_path: CRITEO_PREPROCESSED_PATH
    results_dir: RESULTS_DIR
    submission_dir: SUBMISSION_DIR
    cudnn_tar_file_path: CM_CUDNN_TAR_FILE_PATH
    tensorrt_tar_file_path: CM_TENSORRT_TAR_FILE_PATH
    cuda_run_file_path: CUDA_RUN_FILE_LOCAL_PATH
    dlrm_data_path: DLRM_DATA_PATH
    scratch_path: MLPERF_SCRATCH_PATH
  deps:
    - tags: get,mlperf,inference,nvidia,scratch,space
    - tags: get,mlperf,inference,results,dir
    - tags: get,mlperf,inference,submission,dir
  pre_run_cmds1:
    - cm pull repo ctuning@mlcommons-ck
  run_cmd_prefix: sudo apt remove -y cmake && cm pull repo ctuning@mlcommons-ck
  mounts:
   - "${{ IMAGENET_PATH }}:/data/imagenet-val"
   - "${{ CM_MLPERF_INFERENCE_RESULTS_DIR }}:${{ CM_MLPERF_INFERENCE_RESULTS_DIR }}"
   - "${{ CM_MLPERF_INFERENCE_SUBMISSION_DIR }}:${{ CM_MLPERF_INFERENCE_SUBMISSION_DIR }}"
   - "${{ RESULTS_DIR }}:/home/cmuser/results_dir"
   - "${{ SUBMISSION_DIR }}:/home/cmuser/submission_dir"
   - "${{ CM_CUDNN_TAR_FILE_PATH }}:${{ CM_CUDNN_TAR_FILE_PATH }}"
   - "${{ CM_TENSORRT_TAR_FILE_PATH }}:${{ CM_TENSORRT_TAR_FILE_PATH }}"
   - "${{ CUDA_RUN_FILE_LOCAL_PATH }}:${{ CUDA_RUN_FILE_LOCAL_PATH }}"
   - "${{ MLPERF_SCRATCH_PATH }}:${{ MLPERF_SCRATCH_PATH }}"
   - "${{ DLRM_DATA_PATH }}:/home/mlperf_inf_dlrmv2"
