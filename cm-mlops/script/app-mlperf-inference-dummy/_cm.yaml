# Identification of this CM script
alias: app-mlperf-inference-dummy
uid: 5b71627383a94576
cache: false

automation_alias: script
automation_uid: 5b4e0237da074764

category: "Modular MLPerf benchmarks"


# User-friendly tags to find this CM script
tags:
  - reproduce
  - mlcommons
  - mlperf
  - inference
  - harness
  - dummy-harness
  - dummy

# Default environment
default_env:
  CM_MLPERF_LOADGEN_SCENARIO: Offline
  CM_MLPERF_LOADGEN_MODE: performance
  CM_SKIP_PREPROCESS_DATASET: 'no'
  CM_SKIP_MODEL_DOWNLOAD: 'no'
  CM_MLPERF_SUT_NAME_IMPLEMENTATION_PREFIX: dummy_harness
  CM_MLPERF_SKIP_RUN: 'no'

env:
  CM_CALL_MLPERF_RUNNER: 'no'

# Map script inputs to environment variables
input_mapping:
  count: CM_MLPERF_LOADGEN_QUERY_COUNT
  max_batchsize: CM_MLPERF_LOADGEN_MAX_BATCHSIZE
  mlperf_conf: CM_MLPERF_CONF
  mode: CM_MLPERF_LOADGEN_MODE
  output_dir: CM_MLPERF_OUTPUT_DIR
  performance_sample_count: CM_MLPERF_PERFORMANCE_SAMPLE_COUNT
  scenario: CM_MLPERF_LOADGEN_SCENARIO
  user_conf: CM_MLPERF_USER_CONF
  skip_preprocess: CM_SKIP_PREPROCESS_DATASET
  skip_preprocessing: CM_SKIP_PREPROCESS_DATASET
  target_qps: CM_MLPERF_LOADGEN_TARGET_QPS
  offline_target_qps: CM_MLPERF_LOADGEN_OFFLINE_TARGET_QPS
  server_target_qps: CM_MLPERF_LOADGEN_SERVER_TARGET_QPS
  target_latency: CM_MLPERF_LOADGEN_TARGET_LATENCY
  singlestream_target_latency: CM_MLPERF_LOADGEN_SINGLESTREAM_TARGET_LATENCY
  multistream_target_latency: CM_MLPERF_LOADGEN_MULTISTREAM_TARGET_LATENCY
  performance_sample_count: CM_MLPERF_LOADGEN_PERFORMANCE_SAMPLE_COUNT
  rerun: CM_RERUN
  results_repo: CM_MLPERF_INFERENCE_RESULTS_REPO

new_state_keys:
  - mlperf-inference-implementation
  - CM_SUT_*

# Env keys which are exposed to higher level scripts
new_env_keys:
  - CM_MLPERF_*
  - CM_DATASET_*
  - CM_HW_NAME
  - CM_ML_MODEL_*
  - CM_MAX_EXAMPLES
  - CM_IMAGENET_ACCURACY_DTYPE
  - CM_SQUAD_ACCURACY_DTYPE


# Dependencies on other CM scripts

deps:

  # Detect host OS features
  - tags: detect,os

  # Detect host CPU features
  - tags: detect,cpu

  # Install system dependencies on a given host
  - tags: get,sys-utils-cm


  ########################################################################
  # Install MLPerf inference dependencies

  # Download MLPerf inference source
  - tags: get,mlcommons,inference,src
    names:
    - inference-src

  # Download MLPerf inference loadgen
  - tags: get,mlcommons,inference,loadgen
    names:
    - inference-loadgen

  # Creates user conf for given SUT
  - tags: generate,user-conf,mlperf,inference
    names:
    - user-conf-generator

  # Get MLPerf logging library
  - tags: get,generic-python-lib,_mlperf_logging
    names:
    - mlperf-logging

  - tags: get,git,repo
    names:
      inference-results
      inference-code
    updats_tags_from_env_with_prefix:
      _repo.: CM_MLPERF_INFERENCE_RESULTS_REPO
    env:
      CM_GIT_CHECKOUT_PATH_ENV_NAME: CM_MLPERF_INFERENCE_IMPLEMENTATION_REPO
    extra_cache_tags: inference-implementation,mlperf

# Post dependencies to run this app including for power measurement
post_deps:

  - names:
    - runner
    - mlperf-runner
    skip_if_env:
      CM_MLPERF_SKIP_RUN:
        - 'yes'
        - yes
    tags: benchmark-mlperf

  - tags: save,mlperf,inference,state
    names:
      - save-mlperf-inference-state

# Variations to customize dependencies
variations:
  # Target devices
  cpu:
    group: device
    default: true
    env:
      CM_MLPERF_DEVICE: cpu
  cuda:
    group: device
    env:
      CM_MLPERF_DEVICE: gpu
      CM_MLPERF_DEVICE_LIB_NAMESPEC: cudart

  pytorch:
    group: backend
    default: true
    env:
      CM_MLPERF_BACKEND: pytorch

  pytorch,cuda:
    deps:
      - tags: get,generic-python-lib,_torch_cuda

  pytorch,cpu:
    deps:
      - tags: get,generic-python-lib,_torch

  bs.#:
    group: batch-size

  
  # Reference MLPerf models
  resnet50:
    group: model
    default: true
    env:
      CM_MODEL: resnet50

  retinanet:
    group: model
    base:
      - bs.1
    env:
      CM_MODEL: retinanet

  bert_:
    {}

  bert-99:
    group: model
    base:
    - bert_
    env:
      CM_MODEL: bert-99
      CM_SQUAD_ACCURACY_DTYPE: float32

  bert-99.9:
    group: model
    base:
    - bert_
    env:
      CM_MODEL: bert-99.9

  bert_:
    {}

  bert-99:
    group: model
    base:
    - bert_
    env:
      CM_MODEL: bert-99
      CM_SQUAD_ACCURACY_DTYPE: float32

  bert-99.9:
    group: model
    base:
    - bert_
    env:
      CM_MODEL: bert-99.9

  gptj_:
    deps:
      - tags: get,ml-model,gptj
        names:
         - gptj-model
      - tags: get,dataset,cnndm,_validation

  gptj-99:
    group: model
    base:
    - gptj_
    env:
      CM_MODEL: gptj-99
      CM_SQUAD_ACCURACY_DTYPE: float32

  gptj-99.9:
    group: model
    base:
    - gptj_
    env:
      CM_MODEL: gptj-99.9

  llama2-70b_:
    {}

  llama2-70b-99:
    group: model
    base:
    - llama2-70b_
    env:
      CM_MODEL: llama2-70b-99

  llama2-70b-99.9:
    group: model
    base:
    - llama2-70b_
    env:
      CM_MODEL: llama2-70b-99.9

  singlestream:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: SingleStream

  singlestream,resnet50:
    default_variations:
      batch-size: bs.1

  singlestream,retinanet:
    default_variations:
      batch-size: bs.1

  multistream:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: MultiStream

  offline:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: Offline

  server:
    group: loadgen-scenario
    env:
      CM_MLPERF_LOADGEN_SCENARIO: Server

  uint8:
    group: precision
  fp16:
    group: precision
  fp32:
    group: precision

docker:
  docker_real_run: False
