{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HOME","text":""},{"location":"#collective-knowledge-project-ck","title":"Collective Knowledge project (CK)","text":"<p>Collective Knowledge (CK)  is a community-driven project dedicated to supporting open science, enhancing reproducible research,  and fostering collaborative learning on how to run AI, ML, and other emerging workloads in the most efficient and cost-effective way across diverse models, data sets, software and hardware: [ white paper ].</p> <p>It includes the following sub-projects.</p>"},{"location":"#collective-mind-project-mlcommons-cm","title":"Collective Mind project (MLCommons CM)","text":"<p>The Collective Mind automation framework (CM) was developed to support open science and facilitate collaborative, reproducible, and reusable research, development,  and experimentation based on FAIR principles.</p> <p>It helps users non-intrusively convert their software projects  into file-based repositories of portable and reusable artifacts  (code, data, models, scripts) with extensible metadata and reusable automations, a unified command-line interface,  and a simple Python API.</p> <p>Such artifacts can be easily chained together into portable  and technology-agnostic automation workflows, enabling users to  rerun, reproduce, and reuse complex experimental setups across diverse and rapidly evolving models, datasets, software, and hardware. </p> <p>For example, CM helps to modularize, automate and customize MLPerf benchmarks.</p>"},{"location":"#legacy-cm-api-and-cli-2021-2024","title":"Legacy CM API and CLI (2021-2024)","text":"<p>See the project page for more details.</p> <p>Legacy and simplified CM and MLPerf automations were donated to MLCommons by Grigori Fursin, the cTuning foundation and OctoML. They are now supported by the MLCommons Infra WG (MLCFlow, MLC scripts, mlcr ...).</p>"},{"location":"#new-cm-api-and-cli-cmx-2025","title":"New CM API and CLI (CMX, 2025+)","text":"<p>Collective Mind eXtension or Common Metadata eXchange (CMX)  is the next evolution of the Collective Mind automation framework (MLCommons CM)  designed to enhance simplicity, flexibility, and extensibility of automations  based on user feedback. It is backwards compatible with CM, released along with CM  in the cmind package and can serve as drop-in replacement  for CM and legacy MLPerf automations while providing a simpler and more robust interface.</p> <p>See the project page  and CMX4MLOps automations for more details.</p>"},{"location":"#mlops-and-mlperf-automations","title":"MLOps and MLPerf automations","text":"<p>We have developed a collection of portable, extensible and technology-agnostic automation recipes with a common CLI and Python API (CM scripts) to unify and automate  all the manual steps required to compose, run, benchmark and optimize complex ML/AI applications  on diverse platforms with any software and hardware. </p> <p>The two key automations are script and cache: see online catalog at CK playground, online MLCommons catalog.</p> <p>CM scripts extend the concept of <code>cmake</code> with simple Python automations, native scripts and JSON/YAML meta descriptions. They require Python 3.8+ with minimal dependencies and are  continuously extended by the community and MLCommons members to run natively on Ubuntu, MacOS, Windows, RHEL, Debian, Amazon Linux and any other operating system, in a cloud or inside automatically generated containers while keeping backward compatibility.</p> <p>See the online MLPerf documentation  at MLCommons to run MLPerf inference benchmarks across diverse systems using CMX. Just install <code>pip install cmx4mlperf</code> and substitute the following commands and flags: * <code>cm</code> -&gt; <code>cmx</code> * <code>mlc</code> -&gt; <code>cmlc</code> * <code>mlcr</code> -&gt; <code>cmlcr</code> * <code>-v</code> -&gt; <code>--v</code></p>"},{"location":"#collective-knowledge-playground","title":"Collective Knowledge Playground","text":"<p>Collective Knowledge Playground -  a unified and open-source platform designed to index all CM/CMX automations  similar to PYPI and assist users in preparing CM/CMX commands to:</p> <ul> <li>aggregate, process, visualize, and compare MLPerf benchmarking results for AI and ML systems</li> <li>run MLPerf benchmarks</li> <li>organize open and reproducible optimization challenges and tournaments. </li> </ul>"},{"location":"#artifact-evaluation-and-reproducibility-initiatives","title":"Artifact Evaluation and Reproducibility Initiatives","text":"<p>Artifact Evaluation automation - a community-driven initiative  leveraging CK, CM and CMX to automate artifact evaluation  and support reproducibility efforts at ML and systems conferences.</p>"},{"location":"#legacy-projects","title":"Legacy projects","text":"<ul> <li>CM-MLOps (2021)</li> <li>CM4MLOps (2022-2024)</li> <li>CK automation framework v1 and v2</li> </ul>"},{"location":"#license","title":"License","text":"<p>Apache 2.0</p>"},{"location":"#copyright","title":"Copyright","text":"<p>Copyright (c) 2021-2025 MLCommons</p> <p>Grigori Fursin, the cTuning foundation and OctoML donated this project to MLCommons to benefit everyone.</p> <p>Copyright (c) 2014-2021 cTuning foundation</p>"},{"location":"#author","title":"Author","text":"<ul> <li>Grigori Fursin</li> </ul>"},{"location":"#maintainers","title":"Maintainers","text":"<ul> <li>Legacy CM, CM4MLOps and MLPerf automations: MLCommons infra WG</li> <li>CMX (the next generation of CM since 2025): Grigori Fursin</li> </ul>"},{"location":"#concepts","title":"Concepts","text":"<p>To learn more about the motivation behind this project, please explore the following articles and presentations:</p> <ul> <li>HPCA'25 article \"MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from Microwatts to Megawatts for Sustainable AI\": [ Arxiv ], [ tutorial to reproduce results using CM/CMX ]</li> <li>NeuralMagic's vLLM MLPerf inference 4.1 submission automated by CM:  [README] </li> <li>SDXL MLPerf inference 4.1 submission automated by CM:  [README] </li> <li>\"Enabling more efficient and cost-effective AI/ML systems with Collective Mind, virtualized MLOps, MLPerf, Collective Knowledge Playground and reproducible optimization tournaments\": [ ArXiv ]</li> <li>ACM REP'23 keynote about the MLCommons CM automation framework: [ slides ] </li> <li>ACM TechTalk'21 about Collective Knowledge project: [ YouTube ] [ slides ]</li> <li>Journal of Royal Society'20: [ paper ]</li> </ul>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This open-source project was created by Grigori Fursin and sponsored by cTuning.org, OctoAI and HiPEAC. Grigori donated this project to MLCommons to modularize and automate MLPerf benchmarks, benefit the community, and foster its development as a collaborative, community-driven effort.</p> <p>We thank MLCommons, FlexAI  and cTuning for supporting this project, as well as our dedicated volunteers and collaborators for their feedback and contributions!</p> <p>If you found the CM, CMX and MLPerf automations helpful, kindly reference this article: [ ArXiv ], [ BibTex ].</p> <p>You are welcome to contact the author to discuss long-term plans and potential collaboration.</p>"},{"location":"README.CM/","title":"CM documentation","text":"<p>We plan to rewrite and simplify the CM documentation and tutorials based on user feedback - please stay tuned for more details.</p> <p>Collective Mind (CM) is a lightweight, non-intrusive and technology-agnostic workflow automation framework  being developed by the MLCommons Task Force on Automation and Reproducibility based on the feedback from the the community, MLCommons members and individual contributors.</p> <p>The goal is to provide a common, simple and human-readable interface to help users encode their knowledge about how to build, run and customize diverse AI/ML apps, benchmarks and research projects across  continuously changing models, datasets, software and hardware from different vendors in a unified and automated way.</p> <p>You can find on-going development tasks here.</p> <ul> <li>Getting Started Guide and FAQ</li> <li>Introduction</li> <li>CM installation and customization</li> <li>Unified CLI and Python API</li> <li>CM framework core API</li> <li>CM \"script\" automation</li> <li>CM \"cache\" automation</li> <li>CM \"experiment\" automation</li> <li>List of all unified CM automations from MLCommons</li> <li>List of all portable and reusable CM scripts from MLCommons<ul> <li>The most commonly used CM scripts (basic blocks needed for most portable and tech-agnostic automation workflows)</li> <li>detect OS</li> <li>detect CPU</li> <li>install system deps for CM<ul> <li>install min system deps for Windows</li> </ul> </li> <li>download file</li> <li>extract file</li> <li>download and extract file</li> <li>detect or install python</li> <li>install/manage multiple python venv</li> <li>detect conda manager</li> <li>detect/download COCO dataset</li> </ul> </li> <li>Debugging</li> <li>Real-world use cases</li> <li>Tutorials</li> <li>Specifications</li> <li>Source code</li> <li>FAQ</li> <li>CM and CK history</li> </ul>"},{"location":"debugging/","title":"Debugging","text":"<p>[ Back to index ]</p>"},{"location":"debugging/#cm-debugging","title":"CM debugging","text":"<p>Since CM language uses native OS scripts with python wrappers, it is relatively straightforward to debug it using your existing tools.</p> <p>When you run CM language from the command line or via Python API, the most common execution flow is:</p> <ol> <li>cm/cmr binary or cmind.access function</li> <li>\"access\" function in core.py from cmind package</li> <li>CM automation either from the internal CM repo     or mlcommons@ck</li> <li>CM scripts</li> <li>preprocess function from customize.py from a given CM script </li> <li>native OS script</li> <li>postprocess function from customize.py from a given CM script</li> </ol> <p>When running MLPerf and other benchmarks, you can often use the <code>--debug</code> flag to  start a shell with all environment variables prepared to run/debug the final OS command manually. You can also use GDB via environment variable <code>--env.CM_RUN_PREFIX=\"gdb --args \"</code></p>"},{"location":"debugging/#visual-studio-code","title":"Visual Studio Code","text":"<p>You can debug CM scripts using Visual Studio Code. See the test-debug CM script  for a demo:</p> <ul> <li>Debug CM internals and customize.py using python entry point</li> <li>Debug native python code wrapped by CM scripts using CM break points: code,    README</li> </ul>"},{"location":"faq/","title":"Faq","text":"<p>[ Back to index ]</p>"},{"location":"faq/#cmck-faq","title":"CM/CK FAQ","text":"Click here to see the table of contents.    * [CM FAQ](#cm-faq)     * [How to use CM scripts without affecting native Python installation?](#how-to-use-cm-scripts-without-affecting-native-python-installation?)     * [What is the difference between Repeatability, Reproducibility and Replicability?](#what-is-the-difference-between-repeatability-reproducibility-and-replicability?)   * [Discussions](#discussions)"},{"location":"faq/#how-to-use-cm-scripts-without-affecting-native-python-installation","title":"How to use CM scripts without affecting native Python installation?","text":"<p>We've created CM automation to help users set up multiple virtual Python environments without altering current Python installation. Please follow this guide to set up Python virtual environment on your system.</p>"},{"location":"faq/#what-is-the-difference-between-repeatability-reproducibility-and-replicability","title":"What is the difference between Repeatability, Reproducibility and Replicability?","text":"<p>We use the following definitions adopted by ACM and NISO:</p> <ul> <li>Repeatability (Same team, same experimental setup)</li> </ul> <p>The measurement can be obtained with stated precision by the same team using the same measurement procedure,    the same measuring system, under the same operating conditions, in the same location on multiple trials.    For computational experiments, this means that a researcher can reliably repeat her own computation.</p> <ul> <li>Reproducibility (Different team, different experimental setup)</li> </ul> <p>The measurement can be obtained with stated precision by a different team using the same measurement procedure,    the same measuring system, under the same operating conditions, in the same or a different location on multiple trials.    For computational experiments, this means that an independent group can obtain the same result using the author's own artifacts.</p> <ul> <li>Replicability (Different team, same experimental setup)</li> </ul> <p>The measurement can be obtained with stated precision by a different team, a different measuring system,    in a different location on multiple trials. For computational experiments, this means that an independent group    can obtain the same result using artifacts which they develop completely independently.</p>"},{"location":"faq/#discussions","title":"Discussions","text":"<p>Feel free to ask your questions at our public Discord server. Join our MLCommons taskforce on automation and reproducibility to participate in collaborative developments and optimization challenges.</p>"},{"location":"getting-started/","title":"Getting started","text":"<p>[ Back to documentation ]</p>"},{"location":"getting-started/#collective-mind-getting-started-guide-and-faq","title":"Collective Mind Getting Started Guide and FAQ","text":""},{"location":"getting-started/#why-cm","title":"Why CM?","text":"<p>Collective Mind (CM) is a community project to develop  a collection of portable, extensible, technology-agnostic and ready-to-use automation recipes for MLOps and DevOps with a human-friendly interface (aka CM scripts) that can help to automate all the manual steps required to prepare, build, run, benchmark and optimize complex ML/AI applications  on any platform with any software and hardware.  They require Python 3.7+ with minimal dependencies and can run natively on Ubuntu, MacOS, Windows, RHEL, Debian, Amazon Linux and any other operating system, in a cloud or inside automatically generated containers.</p> <p>CM scripts were originally developed based on the following requirements from the MLCommons engineers and researchers  to help them automatically build, benchmark and optimize complex MLPerf benchmarks across diverse and continuously changing models, data sets, software and hardware from Nvidia, Intel, AMD, Google, Qualcomm, Amazon and other vendors: * must work out of the box with the default options and without the need to edit some paths, environment variables and configuration files; * must be non-intrusive, easy to debug and must reuse existing    user scripts and automation tools (such as cmake, make, ML workflows,    python poetry and containers) rather than substituting them;  * must have a very simple and human-friendly command line with a Python API and minimal dependencies; * must require minimal or zero learning curve by using plain Python, native scripts, environment variables    and simple JSON/YAML descriptions instead of inventing new workflow languages; * must have the same interface to run all automations natively, in a cloud or inside containers.</p> <p>Let's use a relatively simple image classification example to explain how CM achieves that and how it helps to automate much more complex projects including MLPerf benchmarks and reproducibility initatives at ML and Systems conferences.</p> Expand to see the feedback and requirements from MLCommons researchers and engineers   While image classification sounds like a trivial example nowadays, it may still require many manual steps to download some validation data sets and models, install frameworks and low-level dependencies and update various environment variables and paths depending on your platform and target hardware  (for example CPU vs CUDA).  You may also need to make sure that all dependencies are compatible (for example that ONNX run-time  or PyTorch framework is compatible with your CUDA version, etc). Of course, you can also develop a container and fix all the versions but what if you or someone else  want to try a different CUDA version or newer ONNX/TF/PyTorch framework or different operating system or different model or different data set or different framework or different hardware?  While helping MLCommons automate [MLPerf inference benchmarks](https://github.com/mlcommons/inference)  and run them across diverse models, data sets, software and hardware,  we've realized that there is no portable and technology-agnostic automation tool  that can handle such cases.  The feedback from [MLCommons engineers and researchers](taskforce.md) motivated us to develop a simple automation framework that can help them  assemble, run, benchmark and optimize complex AI/ML applications  across diverse and continuously changing models, data sets, software and hardware from Nvidia, Intel, AMD, Google, Qualcomm, Amazon and other vendors."},{"location":"getting-started/#cm-automation-recipe-for-image-classification","title":"CM automation recipe for image classification","text":"<p>We designed CM as a small Python library  with a human-friendly command line, simple Python API and minimal dependencies  needed to implement automation recipes (Python 3.7+, PIP, pyyaml, git, wget) and chain them into portable workflows. CM scripts can run natively (development mode)  or inside containers that CM generates on the fly (stable mode).</p> <p>Most of the time, these dependencies are already installed on your platform. In such case, you should be able to prepare and run image classification with ONNX, ImageNet validation data set and ResNet-50 on Linux, MacOS, Windows and any other operating system using a few CM commands:</p> <p><sup> <pre><code>pip install cmind\ncm pull repo mlcommons@cm4mlops --checkout=dev\ncm run script \"python app image-classification onnx _cpu\"\n</code></pre> <p></p> <p>Note that you may need to re-login when you install cmind for the first time  to let your platform pick up path to the <code>cm</code> command line front-end.</p> <p>You can also run and customize above automation recipe in alternative ways as follows:</p> <p><sup> <pre><code>cm run script \"python app image-classification onnx _cpu\" --help\n\ncm run script \"download file _wget\" --url=https://cKnowledge.org/ai/data/computer_mouse.jpg --verify=no --env.CM_DOWNLOAD_CHECKSUM=45ae5c940233892c2f860efdf0b66e7e\ncm run script \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg\n\ncmr \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg\ncmr --tags=python,app,image-classification,onnx,_cpu --input=computer_mouse.jpg\ncmr 3d5e908e472b417e --input=computer_mouse.jpg\n\ncm docker script \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg\n\ncm gui script \"python app image-classification onnx _cpu\"\n</code></pre> <p></p> <p>If you encounter some issues, please check CM installation guide -  if it doesn't help you, please report your issues here  and/or contact us via our public Discord server -  CM is a community project being developed  and improved across diverse software and hardware  based on your feedback! </p>"},{"location":"getting-started/#how-cm-scripts-works","title":"How CM scripts works?","text":"<p>Next, we briefly explain how CM commands work - it will help you understand what happens when you see similar commands in MLPerf results, README files,  technical reports, research papers, Jupyter notebooks,  Google colab, containers, scripts and artifact appendices.</p> <p>Whenever you run <code>cm run script \"python app image-classification onnx _cpu\"</code>  or <code>cmr \"python app image-classification onnx _cpu\"</code>,  the CM script automation  will simply search for <code>_cm.yaml</code> and <code>_cm.json</code> files (CM meta-description dictionary) in all <code>script</code>  directories in all software projects registered in CM via <code>cm pull repo</code>.</p> <p>In our case, we've pulled github.com/mlcommons/ck project that has most MLCommons' CM automation recipes embedded  in a <code>cm-mlops/script</code> directory. </p> <p>Note that you can pull any public or private Git repository, download any software project  or register any local directory in the CM to search for embedded automation recipes.</p> <p>CM will then try to match all your tags without <code>_</code> prefix (<code>_</code> in tags mark  the so-called CM script variations that customize a give script behavior  and will be described later)  with a <code>tags</code> list in the CM meta-description dictionary. In our case, it will match the corresponding <code>_cm.yaml</code>  in <code>$HOME/CM/repos/mlcommons@cm4mlops/script/app-image-classification-onnx-py/_cm.yaml</code> -  a wrapper for a given CM automation recipe.</p> <p>Note that if you use unique ID instead of tags to identify automation (such as <code>3d5e908e472b417e</code>),   CM will try to match <code>uid</code> string in the CM meta descriptions instead of tags.</p>"},{"location":"getting-started/#how-cm-runs-automation-recipes","title":"How CM runs automation recipes?","text":"<p>Whenever CM finds a directory with a requested automation recipe,  it performs the following steps: * run <code>preprocess</code> function in <code>customize.py</code> if exists * run <code>run.sh</code> (Linux) or <code>run.bat</code> (Windows) if exists * run <code>postprocess</code> function in <code>customize.py</code> if exists</p> <p>Such organization makes it possible to use either Python or native OS scripts or both to implement CM automation recipes while minimizing the learning curve for CM understanding, development and debugging as requested by CM users.</p> <p>Furthermore, CM scripts can keep the source code of image classification (as shown here) that we can easily move around between projects without hardwiring paths and names.</p>"},{"location":"getting-started/#how-cm-unifies-inputs-outputs-and-environment-variables","title":"How CM unifies inputs, outputs and environment variables?","text":"<p>CM allows you to pass environment variables to <code>customize.py</code> and native scripts using <code>--env.ENV=VALUE</code>. </p> <p>When you use some flags such as <code>--input</code> in our image classification example, it will be also converted into an environment variable using <code>input_mapping</code> dictionary  in the CM meta description of this script.</p> <p>All environment variables are aggregated in <code>env</code> dictionary inside CM and then passed to <code>preprocess</code> function in <code>customize.py</code> where you can modify it programmatically. </p> <p>They are then passed to the <code>run</code> script. Since new environment variables are not preserved after <code>run</code> script, one can pass new environment variables back to CM using <code>tmp-run-env.out</code> with ENV=KEY strings as shown here or using <code>tmp-run-state.json</code> as shown here.</p>"},{"location":"getting-started/#how-cm-chains-automation-recipes-into-portable-workflows","title":"How CM chains automation recipes into portable workflows?","text":"<p>CM scripts provide a technology-agnostic wrapper with simple tags, CLI and Python API to prepare and run  user code snippets and native scripts/tools while unifying their inputs and outputs, paths and environment variables.</p> <p>Such architecture makes it possible to easily chain existing user scripts and tools into portable, technology-agnostic and powerful workflows instead of substituting or rewriting them.</p> <p>It is possible to chain CM scripts using simple  <code>deps</code> list  in a meta description of a given script:</p> <p><sup> <pre><code>deps:\n- tags: detect,os\n- tags: get,sys-utils-cm\n- names:\n  - python\n  - python3\n  tags: get,python3\n\n- tags: get,cuda\n  names:\n  - cuda\n  enable_if_env:\n    USE_CUDA:\n    - yes\n- tags: get,cudnn\n  names:\n  - cudnn\n  enable_if_env:\n    USE_CUDA:\n    - yes\n\n- tags: get,dataset,imagenet,image-classification,original\n- tags: get,dataset-aux,imagenet-aux,image-classification\n- tags: get,ml-model,resnet50,_onnx,image-classification\n  names:\n  - ml-model\n\n- tags: get,generic-python-lib,_package.Pillow\n- tags: get,generic-python-lib,_package.numpy\n- tags: get,generic-python-lib,_package.opencv-python\n\n\n- tags: get,generic-python-lib,_onnxruntime\n  names:\n  - onnxruntime\n  skip_if_env:\n    USE_CUDA:\n    - yes\n- tags: get,generic-python-lib,_onnxruntime_gpu\n  names:\n  - onnxruntime\n  enable_if_env:\n    USE_CUDA:\n    - yes\n</code></pre> <p></p> <p>Each entry in this list is a dictionary that specifies which CM script to run using <code>tags</code>. Internally, CM will be updating <code>env</code> dictionary (flat environment) and <code>state</code> dictionary  (to let scripts exchange complex data structures besides environment variables).</p> <p>If you run CM via command line, you can see internal <code>env</code> and <code>state</code> dictionaries by adding <code>-j</code> flag:</p> <pre><code>cmr \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg -j\n</code></pre> <p>Note that we use similar approach for updating environment variables similar   to calling native scripts - by default, they do not alter environment  variables at the host. However, CM allows you to do that   by explicitly specifying which environment variables and state keys  will be updated at the host using <code>new_env_keys</code> and <code>new_state_keys</code>  in the meta of a given script as shown here.  This helped us make behavior of complex CM workflows more deterministic  and reproducible.</p> <p>Each sub-dependency can be turned on or off using environment variables using <code>enable_if_env</code> dictionary or <code>disable_if_env</code> dictionary.</p> <p>You can also specify <code>version_min</code>, <code>version_max</code> and <code>version</code> in these dependencies. You can also give them some specific names such as <code>python</code> and pass versions and environment variables only to a specific script in a pipeline as follows: <pre><code>cmr \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg --adr.python.version_min=3.9\n</code></pre></p> <p>This functionality is usually implemented inside ad-hoc bash or shell scripts  with many hardwired paths and names - CM simply makes such scripts and tools  portable and reusable while enabling technology-agnostic automation workflows  with a unified interface that can adapt to any operating system and are easy  to understand.</p> <p>We can now assemble complex automation workflows by reusing all portable scripts from the community.</p> <p>In our example, we reused CM scripts to detect OS features,  install system dependencies on any supported OS  (Ubuntu, MacOS, RHEL, Arch, Debian, SLES, Windows, etc), detect or install Python and PIP packages, download and preprocess data sets and models, etc.</p>"},{"location":"getting-started/#how-to-add-new-cm-scripts","title":"How to add new CM scripts?","text":"<p>One of the main requirement for CM was to provide a very light-weight connectors  between existing automation scripts and tools rather than substituting them.</p> <p>You can add your own scripts and tools to CM using the following command that will create a ready-to-use dummy CM script:</p> <pre><code>cm add script my-script --tags=my,script\n</code></pre> <p>You can already run this dummy script and plug it into other CM workflows: <pre><code>cmr \"my script\"\n</code></pre></p> <p>You can also run it from python as follows: <pre><code>import cmind\noutput=cmind.access({'action':'run', \n                     'automation':'script', \n                     'tags':'my,script'})\nif output['return']==0: print (output)\n</code></pre></p>"},{"location":"getting-started/#how-to-customize-cm-scripts-using-variations","title":"How to customize CM scripts using variations?","text":"<p>Sometimes we need to set multiple environment variables or run a set of extra CM scripts for a specific purpose (different hardware target or model or dataset).</p> <p>We introduced special tags with <code>_</code>, called variations or variation tags,  that allow you to update a set of environment variables and add extra scripts to the chain of dependencies.</p> <p>Such variations are defined using <code>variations</code> dictionary  in the meta description of a given CM script.</p> <p>For example, our script has 2 variations <code>_cuda</code> and <code>_cpu</code>.</p> <p>If you want to use CUDA implementation of the image classification example,  you can add this variation to the tags that will set <code>USE_CUDA</code> environment to <code>yes</code> and will turn on a specific CM script in <code>deps</code> to install ONNX for CUDA:</p> <pre><code>cmr \"python app image-classification onnx _cuda\" --input=computer_mouse.jpg\n</code></pre>"},{"location":"getting-started/#how-to-cache-and-reuse-cm-scripts-output","title":"How to cache and reuse CM scripts' output?","text":"<p>By default, CM scripts run in the current directory and record all new files there.</p> <p>For example, the following universal download script will download  computer mouse image to the current directory:</p> <p><sup> <pre><code>cm run script \"download file _wget\" --url=https://cKnowledge.org/ai/data/computer_mouse.jpg --verify=no --env.CM_DOWNLOAD_CHECKSUM=45ae5c940233892c2f860efdf0b66e7e\n</code></pre> <p></p> <p>In some cases, we want to cache and reuse the output of automation recipes (such as downloading models, preprocessing data sets or building some applications) rather than just downloading it to the current directory.</p> <p>Following the feedback from our users, we implemented a <code>cache</code> automation in CM similar to <code>script</code>. Whenever CM encounters <code>\"cache\":true</code> in a meta description of a given script, it will create a <code>cache</code> directory in <code>$HOME/CM/repos/local</code> with some unique ID and the same tags as <code>script</code>, and will execute that script there to record all the data in cache. </p> <p>Whenever the same CM script is executed and CM finds an associated cache entry,  it will skip execution and will reuse files from that entry.</p> <p>Furthermore, it is possible to reuse large cached files in other projects that call the same CM scripts!</p> <p>You can see cache entries and find a specific one as follows:</p> <pre><code>cmr \"get ml-model resnet50 _onnx\" -j\n\ncm show cache\ncm show cache \"get ml-model resnet50 _onnx\" \ncm find cache \"download file ml-model resnet50 _onnx\" \ncm info cache \"download file ml-model resnet50 _onnx\" \n</code></pre> <p>You can clean some cache entries as follows: <pre><code>cm rm cache --tags=ml-model,resnet50\n</code></pre></p> <p>You can also clean all CM <code>cache</code> entries and start from scratch as follows: <pre><code>cm rm cache -f\n</code></pre></p> <p>In fact, you can remove <code>$HOME/CM</code> to reset CM framework completely and remove all downloaded repositories and cached entries.</p>"},{"location":"getting-started/#how-to-use-cm-with-python-virtual-environments","title":"How to use CM with Python virtual environments?","text":"<p>Using CM <code>cache</code> makes it possible to run CM automations for multiple virtual environments installed inside CM <code>cache</code> entries. It is possible to run CM automations with different Python virtual environments transparently to users while avoiding messing up native user environment.</p> <p>We created the following CM automation recipe to create virtual environments:</p> <pre><code>cmr \"install python-venv\" --name=mlperf\ncm show cache \"python-venv name-mlperf\"\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre> <p>If you now run our image classification automation recipe,  it will reuse model and dataset from the cache, but will use the newly created virtual environment <code>mlperf</code> for running the script.</p>"},{"location":"getting-started/#how-to-debug-cm-scripts","title":"How to debug CM scripts?","text":"<p>One of the requirements from CM users was to avoid new and/or complex ways to debug CM automations. Using native scripts and Python code makes it possible to apply standard techniques and tools to debug CM automations.</p> <p>We were also asked to add <code>--debug</code> flag to open a shell after the last native script is executed -  this allows users to rerun the last command line with all environment variables and paths assembled by CM while having a full and native access to change environment and run the final command  (such as pinning threads, changing batch sizes, modifying files, etc).</p> <p>You can try it as follows on Linux, MacOS, Windows or other operating system as follows:</p> <pre><code>cmr \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg --debug\n</code></pre> <p>You can also use GDB via environment variable <code>--env.CM_RUN_PREFIX=\"gdb --args \"</code> to run the final command via GDB.</p>"},{"location":"getting-started/#how-to-extendimprove-cm-scripts","title":"How to extend/improve CM scripts?","text":"<p>CM is a community project where CM scripts  are continuously improved to run on different hardware with different software  while keeping backward compatibility through the unified CM interface, tags and variations.</p> <p>Whenever you encounter an issue or want to have support for your own project and environment,  please update these scripts and send a PR to the CM GitHub.</p> <p>You can also reach us via public Discord server  if you questions or suggestions.</p>"},{"location":"getting-started/#how-to-use-cm-with-containers","title":"How to use CM with containers?","text":"<p>One of the key requirements for CM was to run automation natively or inside containers in the same way.</p> <p>We want CM scripts to adapt to the current/latest environment natively or run in the container automatically generated on the fly when requested by user for more stability and determinism.</p> <p>In such case, we can get rid of separate development of native scripts/workflows and Dockerfile  and use the same CM commands instead.</p> <p>To run a given script in an automatically-generated container, you can simply substitute <code>cm run script</code>  with <code>cm docker script</code> or <code>cmr</code> with <code>cmrd</code>:</p> <pre><code>cm docker script \"python app image-classification onnx _cpu\"\n</code></pre> <p>CM will automatically generate a Dockerfile with Ubuntu 22.04 in the <code>dockerfiles</code>  directory of a given script, will build container with the same CM command and will run it inside container.</p> <ul> <li>If you want to stay in the container, you can add flag <code>--docker_it</code>.</li> <li>You can change OS inside container using <code>--docker_base_image</code>, <code>--docker_os</code> and <code>--docker_os_version</code>.</li> </ul> <p>The tricky part is when we want to use host files and directories with a given CM script inside container.  To make it easier for users, we have implemented automatic detection and mounting of files and directories  in CM script.</p> <p>Developers of a CM script just need to specify which flags and environment variables are local files or directories using <code>input_paths</code> in <code>docker</code> dictionary of the meta-description of this script:</p> <pre><code>docker:\n  skip_run_cmd: 'no'\n  all_gpus: 'yes'\n  input_paths:\n    - input\n    - env.CM_IMAGE\n    - output\n  skip_input_for_fake_run:\n    - input\n    - env.CM_IMAGE\n    - output\n    - j\n  pre_run_cmds:\n    - echo \\\"CM pre run commands\\\"\n</code></pre> <p>When you run the same script via container with the local computer_mouse.jpg file as an input, CM will automatically mount current directory and will update the input to the CM script inside container with the internal path:</p> <p><sup> <pre><code>cm docker script \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg\n\n...\n\ndocker build  -f D:\\Work1\\CM\\ck\\cm-mlops\\script\\app-image-classification-onnx-py\\dockerfiles\\ubuntu_22.04.Dockerfile \\\n              -t cknowledge/cm-script-app-image-classification-onnx-py:ubuntu-22.04-latest .\n\n...\n\nContainer launch command:\ndocker run  --entrypoint \"\"  --gpus=all -v D:\\Work1\\CM\\ck\\docs\\computer_mouse.jpg:/cm-mount/Work1/CM/ck/docs/computer_mouse.jpg \n                            cknowledge/cm-script-app-image-classification-onnx-py:ubuntu-22.04-latest \n                            bash -c \"echo \\\"CM pre run commands\\\" &amp;&amp; \n                            cm run script --tags=python,app,image-classification,onnx,_cpu \n                            --input=/cm-mount/Work1/CM/ck/docs/computer_mouse.jpg \"\n\nCM pre run commands\n</code></pre> <p></p> <p>It is now possible to download large data sets and models to the host from CM containers or pass host scratch pads and data to CM containers transparently to a user!</p>"},{"location":"getting-started/#how-to-use-cm-gui-to-run-automation-recipes","title":"How to use CM GUI to run automation recipes?","text":"<p>Another request from CM/MLCommons users was to have a simple GUI that can generate CM commands with user-friendly selector.</p> <p>We've implemented a CM script called <code>gui</code>  that provides a universal Streamlit GUI for any CM script. </p> <p>You just need to describe the inputs for a given script via meta-description as shown for our image classification example:</p> <pre><code>input_description:\n  input: \n    desc: \"Path to JPEG image to classify\"\n  output: \n    desc: \"Output directory (optional)\"\n  j:\n    desc: \"Print JSON output\"\n    boolean: true\n</code></pre> <p>You can run this GUI for your CM script as follows: <pre><code>cm gui script \"python app image-classification onnx _cpu\"\n</code></pre></p> <p>This GUI will allow you to customize your script and run it on your host.</p>"},{"location":"getting-started/#how-to-run-mlperf-benchmarks-via-cm","title":"How to run MLPerf benchmarks via CM?","text":"<p>CM was originally designed to make it easier to run MLPerf inference benchmarks.</p> <p>While MLPerf inference has a common benchmarking engine called loadgen, setting up a given platform, installing all tools, downloading and preprocessing all models and data sets,  updating paths and environment variables, figuring out default parameters for various scenarios, preparing a loadgen command line, keeping track of continuous updates in MLPerf rules, running multiple experiments and submitting results is a major challenge for old and new submitters (see MLPerf inference v4.0 submitter orientation for automation.</p> <p>We created several CM scripts to prepare and run different implementations of MLPerf inference (reference, Nvidia, Intel, Qualcomm, Deep Sparse, etc) with a master CM script to run them all out-of-the-box natively or inside automatically-generated containers  run-mlperf-inference-app. CM helped us to implement it as a simple pipeline with a common and human-friendly interface while reusing all existing automation recipes.</p> <p>This script was successfully validated to modularize MLPerf inference benchmarks  and help the community automate more than 95% of all performance and power submissions in the v3.1 round across more than 120 system configurations (models, frameworks, hardware)  while reducing development and maintenance costs.</p> <p>Please check this documentation for more details.</p>"},{"location":"getting-started/#how-to-use-cm-to-reproduce-research-papers","title":"How to use CM to reproduce research papers?","text":"<p>Following the successful validation of CM concept to modularize and run MLPerf inference benchmarks across diverse software and hardware, the community test it to make it easier to reproduce results from research papers during artifact evaluation and other reproducibility initiatives at systems conferences.</p> <p>The idea is to provide a common interface to prepare and run experiments from research papers. See the latest CM scripts to rerun some experiments from the ACM/IEEE MICRO'23 conference and from the Student Cluster Competition at Supercomputing'23.</p>"},{"location":"getting-started/#how-to-use-cm-as-a-common-interface-to-other-projects","title":"How to use CM as a common interface to other projects?","text":"<p>While CM was successfully validated to unify, modularize and automate MLPerf benchmarks, it turned out to be applicable to any software project. </p> <p>The community started using CM as a common and human-friendly interface to run other software projects  and manage experiments across diverse models, data sets, software and hardware while making them more modular,  portable and reusable.</p> <p>Please check other CM tutorials, CM documentation and our ACM REP'23 keynote for more details.</p>"},{"location":"getting-started/#where-to-read-about-the-cm-vision-and-history","title":"Where to read about the CM vision and history?","text":"<ul> <li>ACM REP'23 keynote about MLCommons CM: slides YouTube</li> <li>ACM TechTalk'21 about automating research projects: YouTube slides</li> <li>Project history</li> </ul>"},{"location":"getting-started/#how-to-get-in-touch-with-the-cm-community","title":"How to get in touch with the CM community?","text":"<p>This is a community project being developed by the MLCommons Task Force on Automation and Reproducibility based on your feedback and contributions - please join our public Discord server if you  would like to help with developments or have questions, suggestions and feature requests.</p>"},{"location":"history/","title":"History","text":"<p>[ Back to index ]</p>"},{"location":"history/#collective-knowledge-v1-and-v2-ck","title":"Collective Knowledge v1 and v2 (CK)","text":"<p>The open-source Collective Knowledge Technology v1 and v2 (CK)  was originally developed by Grigori Fursin  with great contributions from the community. </p> <p>CK was based on Grigori's practical experience helping the community reproduce many research projects  and validate them in the real world across diverse and continuously changing models, data sets, software and hardware since 2013.</p> <p>CK development was also sponsored by the cTuning foundation,  HiPEAC and OctoML.</p> <p>The archive of this discontinued framework is available in the /ck directory. </p>"},{"location":"history/#mlcommons-collective-mind-cm","title":"MLCommons Collective Mind (CM)","text":"<p>Grigori donated the CK technology to MLCommons in 2022 to benefit everyone, prototyped a new version of CK called Collective Mind (CM)  with portable and reusable automation recipes for universal benchmarking and optimization of applications and systems (CM4MLOps scripts and workflows), and helped establish the MLCommons Task Force on Automation and Reproducibility co-led with Arjun Suresh.</p> <p>You can learn more about the CM concept and motivation from the keynote at ACM REP'23 and this white paper.</p>"},{"location":"history/#cmx-the-next-generation-of-cm-and-cm4mlops","title":"CMX (the next generation of CM and CM4MLOps)","text":"<p>Grigori started prototyping CMX during summer 2024 to simplify CM interfaces based on user feedback.  It is available as a part of the standard CM package at PYPI.  Please reach out to learn more about our plans.</p>"},{"location":"installation-cuda/","title":"Installation cuda","text":"<p>[ Back to index ]</p>"},{"location":"installation-cuda/#cuda-installation-using-cm","title":"CUDA installation using CM","text":"<p>Here we describe how to install CUDA drivers and typical dependencies (cuDNN, TensorRT) in a native environment via CM automation language  to reproduce CUDA-based research projects and MLPerf benchmarks.</p> <p>We expect you to have CM already installed as described here.</p> <p>You should also install or update  the MLCommons repository with reusable automation recipes (CM scripts) that are being developed and shared by the community under Apache 2.0 license  to enable portable, modular, and technology-agnostic benchmarks and applications  that can automatically run with any software, hardware, models and data sets:</p> <pre><code>cm pull repo mlcommons@cm4mlops --checkout=dev\n</code></pre>"},{"location":"installation-cuda/#ubuntu-debian-red-hat","title":"Ubuntu, Debian, Red Hat","text":""},{"location":"installation-cuda/#install-cuda-drivers","title":"Install CUDA drivers","text":"<p>If you use a \"clean\" system without CUDA drivers (particularly when you install a new AWS, GCP or Azure instance with minimal OS) you can install CUDA drivers via CM as follows:</p> <pre><code>cmr \"install cuda prebuilt _driver\"\n</code></pre> <p>By default, it will download and install CUDA driver <code>11.8.0</code>. You can change this version  to the supported one as follows: <pre><code>cmr \"install cuda prebuilt _driver\" --version={any CUDA version}\n</code></pre></p> <p>You may need to restart your system to load drivers. You can then test them via CM as follows: <pre><code>cmr \"get cuda-devices\"\n</code></pre></p>"},{"location":"installation-cuda/#detect-cuda-drivers","title":"Detect CUDA drivers","text":"<p>If your system already has CUDA installed, you can detect and plug it into CM as follows: <pre><code>cmr \"get cuda\"\n</code></pre></p>"},{"location":"installation-cuda/#detectinstall-cudnn","title":"Detect/install cuDNN","text":"<p>If cuDNN is already installed on your system, you can detect and plug it into CM as follows:</p> <pre><code>cmr \"get cudnn\"\n</code></pre> <p>Otherwise, download cuDNN tar file from Nvidia website and install it via CM as follows:</p> <pre><code>cmr \"get cudnn\" --tar_file=&lt;PATH_TO_CUDNN_TAR_FILE&gt;\n</code></pre>"},{"location":"installation-cuda/#detectinstall-tensorrt","title":"Detect/install TensorRT","text":"<p>If TensorRT is already installed on your system, you can detect and plug it into CM as follows:</p> <pre><code>cmr \"get tensorrt\"\n</code></pre> <p>However, we suggest you to install a development version (with Python integration, etc) as follows:</p> <ol> <li> <p>Download TensorRT tar file from Nvidia website.</p> </li> <li> <p>Install using CM as follows:</p> </li> </ol> <pre><code>cmr \"get tensorrt _dev\" --tar_file=&lt;PATH_TO_TENSORRT_TAR_FILE&gt;\n</code></pre>"},{"location":"installation-cuda/#showclean-cm-cache-with-all-installations","title":"Show/clean CM cache with all installations","text":"<p>You can list and find all cached installation in CM as follows: <pre><code>cm show cache\n</code></pre></p> <p>Note, that you can clean CM cache and start from scratch as follows: <pre><code>cm rm cache -f\n</code></pre></p> <p>Even more radical, you can delete the whole <code>$HOME/CM</code> directory and start from scratch as follows: <pre><code>(sudo) rm -rf $HOME/CM\ncm pull repo mlcommons@cm4mlops --checkout=dev\n</code></pre></p>"},{"location":"installation-cuda/#windows","title":"Windows","text":""},{"location":"installation-cuda/#drivers","title":"Drivers","text":"<p>The community did not yet share the script to automatically download and install CUDA drivers on Windows. However, you can do it manually and then use CM to detect it and plug into other CM automations:</p> <p>First, download Microsoft Visual C++ Redistributable.</p> <p>Then, install all system dependencies as described here.</p> <p>If Visual Studio and CUDA updated your PATH variable, you should just run the following: <pre><code>cmr \"get cuda\"\n</code></pre></p> <p>However, if the PATH variable was not updated, you need to provide path to the cl.exe and nvcc.exe to help CM detect them:</p> <pre><code>cmr \"get cl\" --path=\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\bin\\Hostx64\\x64\"\ncmr \"get cuda _compiler\" --path=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.7\\bin\"\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>[ Back to index ]</p> Click here to see the table of contents.  * [CM installation](#cm-installation)   * [Ubuntu, Debian](#ubuntu-debian)   * [Red Hat](#red-hat)   * [MacOS](#macos)   * [Windows](#windows) * [CM CLI testing](#cm-cli-testing) * [CUDA installation](#cuda-installation) * [CM customization](#cm-customization) * [CM automation scripts](#cm-automation-scripts) * [Running CM scripts via Docker](#running-cm-scripts-via-docker)   <p>Last revision of this document: September 29, 2024</p>"},{"location":"installation/#cm-installation","title":"CM installation","text":"<p>MLCommons Collective Mind framework requires minimal dependencies to run on any platform: <code>python 3+, pip, git, git-lfs, wget</code>. However, most CM automation recipes shared by the community and MLCommons require Python 3.7+ .</p> <p>By default, CM will pull Git repositories and cache installations and downloaded files in your <code>$HOME/CM</code> directory (Linux/MacOS).     You can change it to any another directory using the <code>CM_REPOS</code> environment variable, for example <code>export CM_REPOS=/scratch/CM</code>.</p> <p>We suggest you not to install <code>cm4mlops</code> package via PIP since you can't control installation of the CM framework and repositories     and it doesn't handle CM errors properly - use a newer version of <code>cm init</code> after installing <code>cmind</code> package as described below.</p> <p>Feel free to use the online installation GUI.</p> <p>Here are typical installation procedures across different operating systems:</p> <ul> <li>Ubuntu, Debian</li> <li>Red Hat</li> <li>MacOS</li> <li>Windows</li> </ul> <p>You can find some Docker containers for CM here.</p> <p>You can customize CM installation using environment variables described here.</p> <p>You can reuse misc CM utils listed here.</p>"},{"location":"installation/#ubuntu-debian","title":"Ubuntu, Debian","text":"<p>We have successfully tested CM with the following system dependencies on Ubuntu 18.x, 20.x, 22.x , 23.x:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade\n\nsudo apt install python3 python3-pip python3-venv git git-lfs wget curl\nsudo apt install libgl1-mesa-dev\n</code></pre> <p>Note that you must set up virtual env on Ubuntu 23+ before using any Python project: <pre><code>python3 -m venv cm\nsource cm/bin/activate\n</code></pre></p> <p>You can now install CM via PIP:</p> <pre><code>python3 -m pip install cmind\n</code></pre> <p>Note that you may need to restart your shell to update PATH to the \"cm\" binary.  Alternatively you can run </p> <pre><code>source $HOME/.profile\n</code></pre> <p>You can check that CM is available and print internal status as follows:</p> <pre><code>gfursin@mlcommons-ck-cm-dev:~$ cm test core\n\nCM version: 2.3.0\n\nPython executable used by CK: C:\\!Progs\\Python310\\python.exe\n\nPath to CM package:           C:\\!Progs\\Python310\\lib\\site-packages\\cmind\nPath to CM core module:       C:\\!Progs\\Python310\\lib\\site-packages\\cmind\\core.py\nPath to CM internal repo:     C:\\!Progs\\Python310\\lib\\site-packages\\cmind\\repo\n\nPath to CM repositories:      D:\\Work1\\CM\n\nGitHub for CM developments:        https://github.com/mlcommons/ck/tree/master/cm\nGitHub for CM automation scripts:  https://github.com/mlcommons/cm4mlops\nReporting issues and ideas:        https://github.com/mlcommons/ck/issues\n</code></pre> <p>You are ready to use CM automation meta-framework. </p>"},{"location":"installation/#red-hat","title":"Red Hat","text":"<p>We have successfully tested CM on Red Hat 9 and CentOS 8</p> <pre><code>sudo dnf update\n\nsudo dnf install python3 python-pip git git-lfs wget curl\n\npython3 -m pip install cmind --user\n</code></pre>"},{"location":"installation/#macos","title":"MacOS","text":"<p>Note that CM currently does not work with Python installed from the Apple Store.  Please install Python via brew as described below.</p> <p>If <code>brew</code> package manager is not installed, please install it as follows (see details here): <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre></p> <p>Don't forget to add brew to PATH environment as described in the end.</p> <p>Then install python, pip, git and wget:</p> <pre><code>brew install python3 git git-lfs wget curl\n\npython3 -m pip install cmind\n</code></pre> <p>Sometimes python does not add <code>cm</code> and <code>cmr</code> binaries to the <code>PATH</code> environment variable.  You may need to find these files and add their path to <code>PATH</code> variable.  We plan to simplify this installation in the future.</p>"},{"location":"installation/#windows","title":"Windows","text":"<ul> <li>Configure Windows 10+ to support long paths from command line as admin:    <pre><code>reg add \"HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\" /v LongPathsEnabled /t REG_DWORD /d 1 /f\n</code></pre> </li> <li>Download and install Git from git-for-windows.github.io.</li> <li>Configure Git to accept long file names: <code>git config --system core.longpaths true</code></li> <li>Download and install Python 3+ from www.python.org/downloads/windows.</li> <li>Don't forget to select option to add Python binaries to PATH environment!</li> <li> <p>Configure Windows to accept long fie names during Python installation!</p> </li> <li> <p>Install CM via PIP:</p> </li> </ul> <pre><code>python -m pip install cmind\n</code></pre> <p>Note that we have reports   that CM does not work when Python was first installed from the Microsoft Store.  If CM fails to run, you can find a fix here.</p> <p>We plan to provide a self-sustained package in the future to simplify CM installation on Windows.</p>"},{"location":"installation/#cm-cli-testing","title":"CM CLI testing","text":"<p>If the installation is successful, you can run the CM CLI as follows:</p> <pre><code>gfursin@cmind:~$ cm\n\ncm {action} {automation} {artifact(s)} {flags} @input.yaml @input.json\n</code></pre> <p>Note that you may need to relogin to your shell to update the PATH to the CM CLI!</p> <p>You can also quickly test the installation and check the version as follows: <pre><code>gfursin@mlcommons-ck-cm-dev:~$ cm test core\n\nCM version: 1.5.0\n\nPython executable used by CK: /usr/bin/python3\n\nPath to CM package:         /home/user/.local/lib/python3.9/site-packages/cmind\nPath to CM core module:     /home/user/.local/lib/python3.9/site-packages/cmind/core.py\nPath to CM internal repo:   /home/user/.local/lib/python3.9/site-packages/cmind/repo\n\nPath to CM repositories:    /home/user/CM\n\nGitHub for CM developments:        https://github.com/mlcommons/ck/tree/master/cm\nGitHub for CM automation scripts:  https://github.com/mlcommons/ck/tree/master/cm-mlops\nReporting issues and ideas:        https://github.com/mlcommons/ck/issues\nJoining the open MLPerf workgroup: https://cKnowledge.org/mlcommons-taskforce\n</code></pre></p>"},{"location":"installation/#cm-init","title":"CM init","text":"<p>Use the following command to test CM system dependencies (git, wget, curl, etc):</p> <pre><code>cm init\n</code></pre> <p>Note that it will also install stable <code>cm4mlops</code> repository with the automation recipes for MLOps and MLPerf.</p> <p>You can skip installation of this repository and use the standard CM command to pull this repo as follows: <pre><code>cm init --min\ncm pull repo mlcommons@cm4mlops\n</code></pre></p>"},{"location":"installation/#cuda-installation","title":"CUDA installation","text":"<p>If you plan to use CUDA for your experiments, please follow this guide  to detect or install it and other related dependencies (cuDNN, TensorRT) using CM.</p>"},{"location":"installation/#cm-customization","title":"CM customization","text":"<p>You can use the following environment variables to customize CM installation:</p> <ul> <li><code>'CM_REPOS'</code> - change path to the CM repositories and repos.json file.</li> </ul> <p>By default, CM will keep CM repositories in:   * <code>$HOME/CM</code> directory on Linux and MacOS   * <code>%USERPROFILE%\\CM</code> directory on Windows</p> <ul> <li> <p><code>'CM_CONFIG'</code> - provide full path to a JSON or YAML file with the CM configuration.   The content of this file will be merged with the \"cfg\" dictionary   from the config.py.</p> </li> <li> <p><code>'CM_DEBUG'</code> - if set to 'yes', turn on internal CM debugging and raise errors    in CM automations instead of returning a dictionary with an error {'return':ERROR CODE, 'error':'ERROR note'}</p> </li> <li> <p><code>'CM_HOME'</code> - change path to the CM python package with the default 'repo' directory.   Useful to improve the default automations inside the CM package.</p> </li> <li> <p><code>'CM_INDEX'</code> (CM v1.3.0+) - set to {off|no|false} to turn off CM indexing of all artifacts   (when on, it speeds up artifact searching and execution of CM scripts by 10..50x)</p> </li> </ul>"},{"location":"installation/#cm-automation-scripts","title":"CM automation scripts","text":"<p>Please go back to index to continue learning about CM interface and scripts.</p> <p>However, if you are already familiar with the CM/CK concepts, you can  use these CM automation scripts  for portable MLOps and DevOps from MLCommons directly by installing the following repository: <pre><code>cm pull repo mlcommons@cm4mlops\n</code></pre></p> <p>You can switch to a development branch of this or any other CM repository as follows:</p> <pre><code>cm checkout repo mlcommons@cm4mlops --branch=dev\n</code></pre> <p>You can switch back to master branch as follows:</p> <pre><code>cm checkout repo mlcommons@cm4mlops --branch=master\n</code></pre> <p>If you plan to participate in our reproducibility and optimization challenges, we suggest you to create a fork of github.com/mlcommons/ck and use it.  In such case, you will be able to create PRs with your updates to the main repository. If you already installed above repo, you will need delete it and install your fork as follows:</p> <pre><code>cm rm repo mlcommons@cm4mlops --all\ncm pull repo --url={URL of the fork of github.com/mlcommons/ck}\n</code></pre> <p>If you want to use stable CM snapshots of reusable automation recipes (CM scripts),  you can download a stable repository from Zenodo (~5MB):</p> <pre><code>cm rm repo mlcommons@cm4mlops --all\ncm pull repo --url=https://zenodo.org/records/10787459/files/cm-mlops-repo-20240306.zip\n</code></pre> <p>You can pull repository and checkout a specific branch as follows: <pre><code>cm rm repo mlcommons@cm4mlops --checkout=dev\ncm pull repo --url=https://zenodo.org/records/10787459/files/cm-mlops-repo-20240306.zip\n</code></pre></p> <p>If you use CM scripts with Python outside containers, we suggest you to set up CM Python virtual environment as described here.</p> <p>Feel free to check these CM tutorials to learn how to use CM to facilitate reproducible research, run MLPerf out-of-the-box and accelerate technology transfer across rapidly evolving  software, hardware, models and data.</p>"},{"location":"installation/#running-cm-scripts-via-docker","title":"Running CM scripts via Docker","text":"<p>CM language allows users to run various automation workflows and applications in the same way either inside automatically generated container snapshots or the latest software/hardware stacks (that may fail and then collaboratively improved by the community).</p> <p>If you have Docker installed, you can run any CM script using Docker and stay in the container to continue running CM commands as follows: <pre><code>cm docker script --tags=detect,os -j\n</code></pre></p> <p>You can see more examples of using CM with Docker containers in this folder.</p> <p>You can browse and reuse shared CM containers from cKnowledge and cTuning via Docker hub.</p>"},{"location":"installation/#problems","title":"Problems","text":"<p>If you experience problems with CM installation, please report here  or reach the community via Discord server to help improve CM and the overall user experience!</p>"},{"location":"interface/","title":"Interface","text":"<p>[ Back to index ]</p>"},{"location":"interface/#cm-interface","title":"CM interface","text":"<p>Please check CM introduction to understand CM motivation and concepts.</p> <p>After installing CM scripting language, you can use it  to manage research projects and run shared automation actions (recipes) on any platform natively or inside containers  via a unified CM CLI or Python API:</p>"},{"location":"interface/#cli","title":"CLI","text":"<p>Here is format of a unified CM command line  to run any reusable automation action  from any software project on Linux, Windows and MacOS:</p> <pre><code>cm {action} {automation alias | UID | alias,UID} \n  ({artifact name(s) | sub-action | argument}) \n  (--flag1=value1) (--flag2.key2=value2) (--flag3,=value3,value4) (--flag4)\n  (@input.json | @input.yaml)\n  (-- extra CMD)\n</code></pre> <p>First, CM will parse CM CLI into a unified CM input dictionary:</p> <pre><code>{\n  \"action\":\"automation action\",\n  \"automation\":\"automation alias | UID | alias,UID\",\n\n  \"artifact\":{above artifact name or sub-action},\n\n  \"flag1\":\"value1\",\n  \"flag2\":{\"key2\":\"value2\"},\n  \"flag3\":[\"value3\",\"value4\"],\n  \"flag4\":True,\n  ...\n  \"unparsed_cmd\": [\n    list of strings in extra CMD\n   ]\n}\n</code></pre> <p>When a user specify one or more input files with @ prefix, they will be loaded  and merged with the CM input in the same order as in command line.</p> <p>CM will then call a unified CM Python \"access\" function  with this input dictionary to perform some automation action. </p> <p>It is equivalent to using CM Python API except that CM will be in interactive mode.  You can add a flag <code>--out=json</code> to print the output dictionary at the end of an automation action invoked via CLI.</p> <p>You can test the CM interface using the following automation action that simply prints the unified CM input dictionary: <pre><code>cm print-input automation artifact1 artifact2 --flag1=value1 --flag2 -- something\n</code></pre></p>"},{"location":"interface/#python-api","title":"Python API","text":"<p>All CM automations can be accessed in a unified way either via CLI as shown above or via Python API:</p> <pre><code>import cmind\n\ninput={\n  \"action\":\"automation action\",\n  \"automation\":\"automation alias | UID | alias,UID\",\n  ...\n}\n\noutput = cmind.access(input)\n\nif output['return']&gt;0:\n    cmind.error(output)\n\nprint (output)\n</code></pre> <p>The output CM dictionary always has an integer key <code>return</code>.</p> <p>If a given automation action succeeded, the <code>output['return']</code> is equal to zero  and the output dictionary contains the output of this action.</p> <p>Otherwise, <code>output['return'] &gt; 0</code> and <code>output['error']</code>  contains some text explaining CM automation error.</p>"},{"location":"interface/#example","title":"Example","text":"<p>For example, we can list all CM automations, their meta descriptions and paths as follows:</p> <pre><code>import cmind\n\noutput = cmind.access({'action':'search',\n                       'automation':'automation,bbeb15d8f0a944a4'})\n\nif output['return']&gt;0: \n    cmind.error(output)\n\nartifacts = output['list']\n\nfor artifact in artifacts:\n    print ('')\n    print (artifact.path)\n    print (artifact.meta)\n</code></pre>"},{"location":"interface/#reusable-automation-actions","title":"Reusable automation actions","text":"<p>CM automation actions are implemented as standard Python functions with unified CM input and output dictionaries.</p> <p>They can call other CM automation actions simply by using the same <code>(dict) = cmind.access(dict)</code> function shown above.</p> <p>Such actions are shared in software projects in <code>module.py</code> files inside <code>automation/artifact alias</code> directories  as can be seen in the internal CM repository or MLCommons CM-MLOps project.</p> <p>Note that CM converts software projects and directories into a database of artifacts abstracted by common automations: Such artifacts are shared in <code>automation alias/artifact alias</code> directories to provide  a simple, common and extensible directory format for shared projects based on FAIR principles. CM automations are also stored and accessed as CM artifacts where <code>artifact alias</code> is simply the automation name.</p> <p>Each CM artifact directory contains <code>_cm.json</code> or <code>_cm.yaml</code> file to help CM find and run a given automation action or an artifact either by unique ID or alias or a combination of both separated by comma (in the last case, only UID is used to find automation action while alias is used as a user-friendly name that can be changed over time without breaking automation workflows):</p> <pre><code>{\n  \"uid\": \"55c3e27e8a140e48\",\n  \"alias\": \"repo\",\n\n  \"automation_alias\": \"automation\",\n  \"automation_uid\": \"bbeb15d8f0a944a4\",\n\n  \"desc\": \"Managing CM repositories and software projects\",\n\n  \"tags\": [\n    \"automation\",\n    \"repo\"\n  ]\n}\n</code></pre> <p>By default, when users install CM via PIP, they have an access to 2 repositories: * \"internal CM repository\"   with a minimal set of reusable automation actions    to manage CM repositories and software projects.  * \"local CM repository\" in <code>$HOME/CM/repos/local</code> that serves as a scratch pad for automation actions</p> <p>When CM is used to pull Git repositories or download zip/tar files with research projects or regiser some local directory as a CM repository, the path will be automatically registered in <code>$HOME/CM/repos.json</code> and used by CM to search for CM automation actions and other artifacts.</p> <p>For example, a user can pull a MLCommons CM-MLOps repository from GitHub using the following CM commands: <pre><code>cm pull repo --url=https://github.com/mlcommons/ck\n</code></pre></p> <p>or shorter as follows:</p> <pre><code>cm pull repo mlcommons@ck\n</code></pre> <p>This repository contains a <code>cmr.yaml</code> file  telling CM to search in the <code>cm-mlops</code> directory for automation actions and artifacts (defined by <code>prefix</code> key to non-intrusively embedd CM inside existing software projects).</p> <p>It is possible to list registered CM repositories using the following automation action: <pre><code>cm search repo\n</code></pre> or</p> <pre><code>cm search repo mlcommons@ck\n</code></pre> <p>This automation action is implemented in this Python function  in the <code>repo</code> automation artifact from the internal CM repository.</p> <p>After pulling <code>mlcommons@ck</code> repository, users will have an access to more reusable automations: <pre><code>cm search automation\n</code></pre></p> <p>or using the equivalent automation action name <code>find</code></p> <pre><code>cm find automation\n</code></pre> <p>It is possible to find automations only in a given repository as follows: <pre><code>cm find automation mlcommons@ck:\n</code></pre></p> <p>Note that \":\" shows that mlcommons@ck is a CM repository and not an artifact.  It is possible to tell CM to search for a given automation or an artifact from the specific repository as follows: <pre><code>cm find {CM repository alias | UID | alias,UID}:automation \n        {CM repository alias | UID | alias,UID}:{artifact1 alias | UID | alias, UID}\n        {CM repository alias | UID | alias,UID}:{artifact2 alias | UID | alias, UID}\n        ...\n</code></pre></p> <p>For example, let's find an automation \"repo\" from the \"internal\" CM repository using wildcards: <pre><code>cm find automation internal:rep*\n</code></pre></p> <p>Note that all automations in CM are abstracted by the following <code>Automation class</code> and has a number of default actions:</p> <ul> <li><code>add</code> - add a new artifact to a CM repository</li> <li><code>delete</code> == <code>rm</code> - delete existing artifact from a CM repository</li> <li><code>load</code> - load meta information (<code>_cm.json</code> or <code>_cm.yaml</code>) for a given artifact</li> <li><code>update</code> - update meta information (<code>_cm.json</code> or <code>_cm.yaml</code>) for a given artifact</li> <li><code>move</code> == <code>rename</code> == <code>mv</code> - rename a given artifact or move it to another CM repository</li> <li><code>copy</code> == <code>cp</code> - copy a given artifact to an artifact with a different name or to another CM repository</li> <li><code>search</code> == <code>list</code> == <code>find</code> - search for automations or artifacts in CM repositories</li> </ul> <p>For example, let's add and find a CM automation \"world\" in the \"local\" CM repository,</p> <pre><code>cm add automation world\ncm find automation world\n</code></pre> <p>By default, CM will add <code>test</code> action to this automation to let you use it as a template for other actions: <pre><code>cm test world\n</code></pre></p> <p>You can now add an artifact called <code>open</code> for this automation in the \"local\" CM repository  with some tags <code>hello,cool</code> as follows: <pre><code>cm add world open --tags=hello,cool\ncm find world\ncm find world open\ncm find world --tags=cool\n</code></pre></p> <p>You can find the command line flags or Python API for any given automation action from the command line as follows: ``bash cm {action} {automation} --help <pre><code>For example, you can obtain help for all above internal (common) automation actions:\n```bash\ncm add world --help\ncm delete world --help\ncm load world --help\ncm update world --help\ncm move world --help\ncm copy world --help\ncm search world --help\n</code></pre></p> <p>You can also customize all above functions in the new automation  by simply adding those functions in a new <code>module.py</code> and then calling the original action with the input key <code>common</code>:True.</p> <p>For example, the CM automation <code>script</code> overloads <code>add</code> function not only to add a new CM script but also copy <code>run.sh</code>, <code>run.bat</code>, <code>customize.py</code> and <code>README-extra.md</code> there. You can study  and reuse this code here.</p> <p>This minimal information covers most of the basic CM functionality. </p> <p>However, it turned out to be enough to enable collaboration between academia and industry to modularize benchmarking, optimiation and design space exploration of AI/ML systems and make it more portable, reproducible and comparable across very diverse and rapidly evolving software and hardware using just 2 extra CM automations: CM scipt and cache.</p> <p>You can see the use of CM in these real-world examples: - README to reproduce published IPOL'22 paper - README to reproduce MLPerf RetinaNet inference benchmark at Student Cluster Competition'22 - Auto-generated READMEs to reproduce official MLPerf BERT inference benchmark v3.0 submission with a model from the Hugging Face Zoo - Auto-generated Docker containers to run and reproduce MLPerf inference benchmark</p>"},{"location":"interface/#further-reading","title":"Further reading","text":"<ul> <li>CM specification</li> <li>Article with the concept of a common automation language based on previous version of CM language before MLCommons</li> </ul>"},{"location":"introduction-ck/","title":"Introduction ck","text":"<p>[ Back to index ]</p>"},{"location":"introduction-ck/#introduction-to-the-mlcommons-ck-playground","title":"Introduction to the MLCommons CK playground","text":"<p>Collective Knowledge playground (MLCommons CK) is an open-source and technology-agnostic automation platform being developed by the MLCommons Task Force on Automation and Reproducibility, cTuning.org and cKnowledge.org.</p> <p>CK is intended to help everyone automatically co-design optimal software and hardware for AI and ML  and deploy them in the real-world in the fastest and most efficient way  while slashing all research, development, optimization and operational costs.</p> <p>CK playground is powered by the portable, technology-agnostic, human-readable and open-source Collective Mind automation language adopted and extended by MLCommons (50+ companies and universities) to collaboratively benchmark and optimize AI and ML systems across diverse software, hardware, models and data from different vendors.</p> <p>The community continuously extends and improves CM via portable and reusable CM scripts solve the \"AI/ML dependency hell\", interconnect incompatible software, hardware, models and data, and encode best practices and optimization techniques  into powerful automation workflows in a transparent and non-intrusive way. </p> <p>While still in the prototyping stage, our open-source technology already helps MLCommons organizations, students and researchers  automate and optimize MLPerf benchmark submissions while contributing to more than half of all performance and power results for MLPerf inference benchmark since the beginning.</p> <p>We thank the community  for helping us to validate a prototype of the MLCommons CK playground by running and reproducing  MLPerf inference v3.0 benchmarks: CK has helped to automatically interconnect very diverse technology from Neural Magic, Qualcomm, Krai, cKnowledge, OctoML, Deelvin, DELL, HPE, Lenovo, Hugging Face, Nvidia and Apple  and run it across diverse CPUs, GPUs and DSPs with PyTorch,  ONNX, QAIC, TF/TFLite, TVM and TensorRT using popular cloud providers (GCP, AWS, Azure) and individual servers and edge devices  via our MLPerf inference v3.0 challenge.</p> <p>Read more about our long-term vision and the next plans in our  ACM REP'23 keynote \"Toward a common language to facilitate reproducible research and technology transfer: challenges and solutions\".</p> <p>See a few real-world examples of using the CK playground powered by the CM language:</p> <ul> <li>Organizing reproducibility, replicability and optimization challenges</li> <li>Reproducing MLPerf inference benchmark and automating submissions</li> <li>Visualizing and comparing MLPerf inference benchmark results</li> <li>Sharing reproducibility reports</li> <li>Adding derived metrics such as power efficiency and/or cost efficiency</li> </ul> <p>Feel free to join our discord server  and participate in the reproducibility and optimization challenges</p>"},{"location":"introduction-cm/","title":"Introduction cm","text":"<p>[ Back to index ]</p>"},{"location":"introduction-cm/#introduction-to-the-mlcommons-cm-language","title":"Introduction to the MLCommons CM language","text":"<p>Check our recent keynote at ACM REP'23: \"toward a common language to facilitate reproducible research and technology transfer: challenges and solutions\".</p> <p>During the past 10 years, the community has considerably improved  the reproducibility of experimental results from research projects and published papers by introducing the artifact evaluation process  with a unified artifact appendix and reproducibility checklists,  Jupyter notebooks, containers, and Git repositories. </p> <p>On the other hand, our experience reproducing more than 150 papers revealed that it still takes weeks and months of painful and repetitive interactions between researchers and evaluators to reproduce experimental results. </p> <p>This effort includes decrypting numerous README files, examining ad-hoc artifacts  and containers, and figuring out how to reproduce computational results. Furthermore, snapshot containers pose a challenge to optimize algorithms' performance,  accuracy, power consumption and operational costs across diverse  and rapidly evolving software, hardware, and data used in the real world.</p> <p></p> <p>This practical experience and the feedback from the community motivated  us to establish the MLCommons Task Force on Automation and Reproducibility and develop a light-weight, technology agnostic, and English-like  workflow automation language called Collective Mind (MLCommons CM).</p> <p>This language provides a common, non-intrusive and human-readable interface to any software project  transforming it into a collection of reusable automation recipes (CM scripts). Following FAIR principles, CM automation actions and scripts  are simple wrappers around existing user scripts and artifacts to make them * findable via human-readable tags, aliases and unique IDs; * accessible via a unified CM CLI and Python API with JSON/YAML meta descriptions; * interoperable and portable across any software, hardware, models and data; * reusable across all projects.</p> <p>CM is written in simple Python and uses JSON and/or YAML meta descriptions with a unified CLI to minimize the learning curve and help researchers and practitioners describe, share, and reproduce experimental results  in a unified, portable, and automated way across any rapidly evolving software, hardware, and data while solving the \"dependency hell\" and automatically generating unified README files and modular containers.</p> <p></p> <p>Our ultimate goal is to use CM language to facilitate reproducible research for AI, ML and systems projects,  minimize manual and repetitive benchmarking and optimization efforts,  and reduce time and costs when transferring technology to production across continuously changing software, hardware, models, and data.</p>"},{"location":"introduction-cm/#some-projects-supported-by-cm","title":"Some projects supported by CM","text":"<ul> <li>A unified way to run MLPerf inference benchmarks with different models, software and hardware. See current coverage.</li> <li>A unitied way to run MLPerf training benchmarks (prototyping phase)</li> <li>A unified way to run MLPerf tiny benchmarks (prototyping phase)</li> <li>A unified CM to run automotive benchmarks (prototyping phase)</li> <li>An open-source platform to aggregate, visualize and compare MLPerf results</li> <li>Leaderboard for community contributions</li> <li>Artifact Evaluation and reproducibility initiatives at ACM/IEEE/NeurIPS conferences:</li> <li>A unified way to run experiments and reproduce results from ACM/IEEE MICRO'23 and ASPLOS papers</li> <li>Student Cluster Competition at SuperComputing'23</li> <li>CM automation to reproduce IPOL paper</li> <li>Auto-generated READMEs to reproduce official MLPerf BERT inference benchmark v3.0 submission with a model from the Hugging Face Zoo</li> <li>Auto-generated Docker containers to run and reproduce MLPerf inference benchmark</li> </ul>"},{"location":"introduction-cm/#presentations","title":"Presentations","text":"<ul> <li>CK vision (ACM Tech Talk at YouTube) </li> <li>CK concepts (Philosophical Transactions of the Royal Society) </li> <li>CM workflow automation introduction (slides from ACM REP'23 keynote)</li> <li>MLPerf inference submitter orientation (slides) </li> </ul>"},{"location":"list_of_automations/","title":"List of automations","text":"<p>[ Back to index ]</p>"},{"location":"list_of_automations/#list-of-cm-automations","title":"List of CM automations","text":"<ul> <li>repo (Managing CM repositories and software projects)</li> <li>script (Making native scripts more portable, interoperable and deterministic)</li> <li>cache (Caching cross-platform CM scripts)</li> <li>utils (Accessing various CM utils)</li> <li>core (Accessing some core CM functions)</li> <li>cfg</li> <li>challenge</li> <li>contributor</li> <li>docker (Managing modular docker containers (under development))</li> <li>experiment (Managing and reproducing experiments (under development))</li> <li>project</li> <li>report</li> <li>ck (Accessing legacy CK automations)</li> <li>automation (Managing CM automations)</li> </ul>"},{"location":"list_of_automations/#repo","title":"repo","text":"<p>Managing CM repositories and software projects.</p> <ul> <li>GitHub repository with CM automations: cm pull internal</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm pull repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm checkout repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm show repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm search repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm where repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm update repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm delete repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm ximport repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm init repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm add repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm pack repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm unpack repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm import_ck_to_cm repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm convert_ck_to_cm repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm detect repo   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm reindex repo   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#script","title":"script","text":"<p>Making native scripts more portable, interoperable and deterministic.</p> <ul> <li>GitHub repository with CM automations: cm pull mlcommons@ck</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm run script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm version script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm search script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm test script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm native_run script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm add script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm run_native_script script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm find_file_in_paths script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm detect_version_using_script script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm find_artifact script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm find_file_deep script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm find_file_back script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm parse_version script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm update_deps script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm get_default_path_list script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm doc script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm gui script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm dockerfile script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm docker script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm prepare script   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm clean_some_tmp_files script   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#cache","title":"cache","text":"<p>Caching cross-platform CM scripts.</p> <ul> <li>GitHub repository with CM automations: cm pull mlcommons@ck</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm test cache   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm show cache   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm search cache   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm copy_to_remote cache   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#utils","title":"utils","text":"<p>Accessing various CM utils.</p> <ul> <li>GitHub repository with CM automations: cm pull mlcommons@ck</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm test utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm get_host_os_info utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm download_file utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm unzip_file utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm compare_versions utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm json2yaml utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm yaml2json utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm sort_json utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm dos2unix utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm replace_string_in_file utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm create_toc_from_md utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm copy_to_clipboard utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm list_files_recursively utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm generate_secret utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm detect_tags_in_artifact utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm prune_input utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm uid utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm system utils   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm load_cfg utils   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#core","title":"core","text":"<p>Accessing some core CM functions.</p> <ul> <li>GitHub repository with CM automations: cm pull internal</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm uid core   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#cfg","title":"cfg","text":"<ul> <li>GitHub repository with CM automations: cm pull mlcommons@ck</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm test cfg   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#challenge","title":"challenge","text":"<ul> <li>GitHub repository with CM automations: cm pull mlcommons@ck</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm test challenge   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#contributor","title":"contributor","text":"<ul> <li>GitHub repository with CM automations: cm pull mlcommons@ck</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm test contributor   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm add contributor   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#docker","title":"docker","text":"<p>Managing modular docker containers (under development).</p> <ul> <li>GitHub repository with CM automations: cm pull mlcommons@ck</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm test docker   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#experiment","title":"experiment","text":"<p>Managing and reproducing experiments (under development).</p> <ul> <li>GitHub repository with CM automations: cm pull mlcommons@ck</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm test experiment   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm run experiment   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm rerun experiment   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm replay experiment   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#project","title":"project","text":"<ul> <li>GitHub repository with CM automations: cm pull mlcommons@ck</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm test project   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#report","title":"report","text":"<ul> <li>GitHub repository with CM automations: cm pull mlcommons@ck</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm test report   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#ck","title":"ck","text":"<p>Accessing legacy CK automations.</p> <ul> <li>GitHub repository with CM automations: cm pull internal</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm any ck   \u00a0\u00a0\u00a0( See CM API )</li> </ul>"},{"location":"list_of_automations/#automation","title":"automation","text":"<p>Managing CM automations.</p> <ul> <li>GitHub repository with CM automations: cm pull internal</li> <li>CM automation code and meta: GitHub</li> <li>CM automation actions:</li> <li>cm print_input automation   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm add automation   \u00a0\u00a0\u00a0( See CM API )</li> <li>cm doc automation   \u00a0\u00a0\u00a0( See CM API )</li> </ul> <p> </p>"},{"location":"list_of_scripts/","title":"List of scripts","text":"<p>[ Back to index ]</p> <p>This is an automatically generated list of portable and reusable automation recipes (CM scripts) with a human-friendly interface (CM)  to run a growing number of ad-hoc MLPerf, MLOps, and DevOps scripts from MLCommons projects and research papers  in a unified way on any operating system with any software and hardware natively or inside containers.</p> <p>Click on any automation recipe below to learn how to run and reuse it  via CM command line, Python API or GUI.</p> <p>CM scripts can easily chained together into automation workflows using <code>deps</code> and <code>tags</code> keys while automatically updating all environment variables and paths  for a given task and platform using simple JSON or YAML.</p> <p>Note that CM is a community project being developed and extended by MLCommons members and individual contributors -  you can find source code of CM scripts maintained by MLCommons here.  Please join Discord server to participate in collaborative developments or provide your feedback.</p>"},{"location":"list_of_scripts/#license","title":"License","text":"<p>Apache 2.0</p>"},{"location":"list_of_scripts/#copyright","title":"Copyright","text":"<p>2022-2024 MLCommons</p>"},{"location":"list_of_scripts/#list-of-cm-scripts-by-categories","title":"List of CM scripts by categories","text":"<ul> <li>AI/ML datasets</li> <li>AI/ML frameworks</li> <li>AI/ML models</li> <li>AI/ML optimization</li> <li>CM interface prototyping</li> <li>CUDA automation</li> <li>Cloud automation</li> <li>Collective benchmarking</li> <li>Compiler automation</li> <li>Dashboard automation</li> <li>Detection or installation of tools and artifacts</li> <li>DevOps automation</li> <li>Docker automation</li> <li>GUI</li> <li>Legacy CK support</li> <li>MLPerf benchmark support</li> <li>Modular AI/ML application pipeline</li> <li>Modular MLPerf benchmarks</li> <li>Modular MLPerf inference benchmark pipeline</li> <li>Modular MLPerf training benchmark pipeline</li> <li>Modular application pipeline</li> <li>Platform information</li> <li>Python automation</li> <li>Remote automation</li> <li>Reproduce MLPerf benchmarks</li> <li>Reproducibility and artifact evaluation</li> <li>Tests</li> <li>TinyML automation</li> </ul>"},{"location":"list_of_scripts/#aiml-datasets","title":"AI/ML datasets","text":"<ul> <li>get-croissant</li> <li>get-dataset-cifar10</li> <li>get-dataset-cnndm</li> <li>get-dataset-coco</li> <li>get-dataset-coco2014</li> <li>get-dataset-criteo</li> <li>get-dataset-imagenet-aux</li> <li>get-dataset-imagenet-calibration</li> <li>get-dataset-imagenet-helper</li> <li>get-dataset-imagenet-train</li> <li>get-dataset-imagenet-val</li> <li>get-dataset-kits19</li> <li>get-dataset-librispeech</li> <li>get-dataset-openimages</li> <li>get-dataset-openimages-annotations</li> <li>get-dataset-openimages-calibration</li> <li>get-dataset-openorca</li> <li>get-dataset-squad</li> <li>get-dataset-squad-vocab</li> <li>get-preprocessed-dataset-criteo</li> <li>get-preprocessed-dataset-imagenet</li> <li>get-preprocessed-dataset-kits19</li> <li>get-preprocessed-dataset-librispeech</li> <li>get-preprocessed-dataset-openimages</li> <li>get-preprocessed-dataset-openorca</li> <li>get-preprocessed-dataset-squad</li> <li>get-preprocesser-script-generic</li> </ul>"},{"location":"list_of_scripts/#aiml-frameworks","title":"AI/ML frameworks","text":"<ul> <li>get-google-saxml</li> <li>get-onnxruntime-prebuilt</li> <li>get-qaic-apps-sdk</li> <li>get-qaic-platform-sdk</li> <li>get-qaic-software-kit</li> <li>get-rocm</li> <li>get-tvm</li> <li>install-qaic-compute-sdk-from-src</li> <li>install-rocm</li> <li>install-tensorflow-for-c</li> <li>install-tensorflow-from-src</li> <li>install-tflite-from-src</li> </ul>"},{"location":"list_of_scripts/#aiml-models","title":"AI/ML models","text":"<ul> <li>convert-ml-model-huggingface-to-onnx</li> <li>get-bert-squad-vocab</li> <li>get-dlrm</li> <li>get-ml-model-3d-unet-kits19</li> <li>get-ml-model-bert-base-squad</li> <li>get-ml-model-bert-large-squad</li> <li>get-ml-model-dlrm-terabyte</li> <li>get-ml-model-efficientnet-lite</li> <li>get-ml-model-gptj</li> <li>get-ml-model-huggingface-zoo</li> <li>get-ml-model-llama2</li> <li>get-ml-model-mobilenet</li> <li>get-ml-model-neuralmagic-zoo</li> <li>get-ml-model-resnet50</li> <li>get-ml-model-retinanet</li> <li>get-ml-model-retinanet-nvidia</li> <li>get-ml-model-rnnt</li> <li>get-ml-model-stable-diffusion</li> <li>get-ml-model-tiny-resnet</li> <li>get-ml-model-using-imagenet-from-model-zoo</li> <li>get-tvm-model</li> </ul>"},{"location":"list_of_scripts/#aiml-optimization","title":"AI/ML optimization","text":"<ul> <li>calibrate-model-for.qaic</li> <li>compile-model-for.qaic</li> <li>prune-bert-models</li> </ul>"},{"location":"list_of_scripts/#cm-interface-prototyping","title":"CM interface prototyping","text":"<ul> <li>test-mlperf-inference-retinanet</li> </ul>"},{"location":"list_of_scripts/#cuda-automation","title":"CUDA automation","text":"<ul> <li>get-cuda</li> <li>get-cuda-devices</li> <li>get-cudnn</li> <li>get-tensorrt</li> <li>install-cuda-package-manager</li> <li>install-cuda-prebuilt</li> </ul>"},{"location":"list_of_scripts/#cloud-automation","title":"Cloud automation","text":"<ul> <li>destroy-terraform</li> <li>get-aws-cli</li> <li>get-terraform</li> <li>install-aws-cli</li> <li>install-terraform-from-src</li> <li>run-terraform</li> </ul>"},{"location":"list_of_scripts/#collective-benchmarking","title":"Collective benchmarking","text":"<ul> <li>launch-benchmark</li> </ul>"},{"location":"list_of_scripts/#compiler-automation","title":"Compiler automation","text":"<ul> <li>get-aocl</li> <li>get-cl (Detect or install Microsoft C compiler)</li> <li>get-compiler-flags</li> <li>get-compiler-rust</li> <li>get-gcc (Detect or install GCC compiler)</li> <li>get-go</li> <li>get-llvm (Detect or install LLVM compiler)</li> <li>install-gcc-src</li> <li>install-ipex-from-src (Build IPEX from sources)</li> <li>install-llvm-prebuilt (Install prebuilt LLVM compiler)</li> <li>install-llvm-src (Build LLVM compiler from sources (can take &gt;30 min))</li> <li>install-onednn-from-src (Build oneDNN from sources)</li> <li>install-onnxruntime-from-src (Build onnxruntime from sources)</li> <li>install-pytorch-from-src (Build pytorch from sources)</li> <li>install-pytorch-kineto-from-src (Build pytorch kineto from sources)</li> <li>install-torchvision-from-src (Build pytorchvision from sources)</li> <li>install-tpp-pytorch-extension (Build TPP-PEX from sources)</li> <li>install-transformers-from-src (Build transformers from sources)</li> </ul>"},{"location":"list_of_scripts/#dashboard-automation","title":"Dashboard automation","text":"<ul> <li>publish-results-to-dashboard</li> </ul>"},{"location":"list_of_scripts/#detection-or-installation-of-tools-and-artifacts","title":"Detection or installation of tools and artifacts","text":"<ul> <li>get-android-sdk</li> <li>get-aria2</li> <li>get-bazel</li> <li>get-blis</li> <li>get-brew</li> <li>get-cmake</li> <li>get-cmsis_5</li> <li>get-docker</li> <li>get-generic-sys-util</li> <li>get-google-test</li> <li>get-java</li> <li>get-javac</li> <li>get-lib-armnn</li> <li>get-lib-dnnl</li> <li>get-lib-protobuf</li> <li>get-lib-qaic-api</li> <li>get-nvidia-docker</li> <li>get-openssl</li> <li>get-rclone</li> <li>get-sys-utils-cm</li> <li>get-sys-utils-min</li> <li>get-xilinx-sdk</li> <li>get-zendnn</li> <li>install-bazel</li> <li>install-cmake-prebuilt</li> <li>install-gflags</li> <li>install-github-cli</li> <li>install-numactl-from-src (Build numactl from sources)</li> <li>install-openssl</li> </ul>"},{"location":"list_of_scripts/#devops-automation","title":"DevOps automation","text":"<ul> <li>benchmark-program</li> <li>compile-program</li> <li>convert-csv-to-md</li> <li>copy-to-clipboard</li> <li>create-conda-env</li> <li>create-patch</li> <li>detect-sudo</li> <li>download-and-extract</li> <li>download-file</li> <li>download-torrent</li> <li>extract-file</li> <li>fail</li> <li>get-conda</li> <li>get-git-repo</li> <li>get-github-cli</li> <li>pull-git-repo</li> <li>push-csv-to-spreadsheet</li> <li>set-device-settings-qaic</li> <li>set-echo-off-win</li> <li>set-performance-mode</li> <li>set-sqlite-dir</li> <li>tar-my-folder</li> </ul>"},{"location":"list_of_scripts/#docker-automation","title":"Docker automation","text":"<ul> <li>build-docker-image</li> <li>build-dockerfile</li> <li>prune-docker</li> <li>run-docker-container</li> </ul>"},{"location":"list_of_scripts/#gui","title":"GUI","text":"<ul> <li>gui</li> </ul>"},{"location":"list_of_scripts/#legacy-ck-support","title":"Legacy CK support","text":"<ul> <li>get-ck</li> <li>get-ck-repo-mlops</li> </ul>"},{"location":"list_of_scripts/#mlperf-benchmark-support","title":"MLPerf benchmark support","text":"<ul> <li>add-custom-nvidia-system</li> <li>benchmark-any-mlperf-inference-implementation</li> <li>build-mlperf-inference-server-nvidia</li> <li>generate-mlperf-inference-submission</li> <li>generate-mlperf-inference-user-conf</li> <li>generate-mlperf-tiny-report</li> <li>generate-mlperf-tiny-submission</li> <li>generate-nvidia-engine</li> <li>get-mlperf-inference-intel-scratch-space</li> <li>get-mlperf-inference-loadgen</li> <li>get-mlperf-inference-nvidia-common-code</li> <li>get-mlperf-inference-nvidia-scratch-space</li> <li>get-mlperf-inference-results</li> <li>get-mlperf-inference-results-dir</li> <li>get-mlperf-inference-src</li> <li>get-mlperf-inference-submission-dir</li> <li>get-mlperf-inference-sut-configs</li> <li>get-mlperf-inference-sut-description</li> <li>get-mlperf-logging</li> <li>get-mlperf-power-dev</li> <li>get-mlperf-tiny-eembc-energy-runner-src</li> <li>get-mlperf-tiny-src</li> <li>get-mlperf-training-nvidia-code</li> <li>get-mlperf-training-src</li> <li>get-nvidia-mitten</li> <li>get-spec-ptd</li> <li>import-mlperf-inference-to-experiment</li> <li>import-mlperf-tiny-to-experiment</li> <li>import-mlperf-training-to-experiment</li> <li>install-mlperf-logging-from-src</li> <li>prepare-training-data-bert</li> <li>prepare-training-data-resnet</li> <li>preprocess-mlperf-inference-submission</li> <li>process-mlperf-accuracy</li> <li>push-mlperf-inference-results-to-github</li> <li>run-mlperf-inference-mobilenet-models</li> <li>run-mlperf-inference-submission-checker</li> <li>run-mlperf-power-client</li> <li>run-mlperf-power-server</li> <li>run-mlperf-training-submission-checker</li> <li>truncate-mlperf-inference-accuracy-log</li> </ul>"},{"location":"list_of_scripts/#modular-aiml-application-pipeline","title":"Modular AI/ML application pipeline","text":"<ul> <li>app-image-classification-onnx-py</li> <li>app-image-classification-tf-onnx-cpp</li> <li>app-image-classification-torch-py</li> <li>app-image-classification-tvm-onnx-py</li> <li>app-stable-diffusion-onnx-py</li> </ul>"},{"location":"list_of_scripts/#modular-mlperf-benchmarks","title":"Modular MLPerf benchmarks","text":"<ul> <li>app-mlperf-inference-dummy</li> <li>app-mlperf-inference-intel</li> <li>app-mlperf-inference-qualcomm</li> </ul>"},{"location":"list_of_scripts/#modular-mlperf-inference-benchmark-pipeline","title":"Modular MLPerf inference benchmark pipeline","text":"<ul> <li>app-loadgen-generic-python</li> <li>app-mlperf-inference</li> <li>app-mlperf-inference-ctuning-cpp-tflite</li> <li>app-mlperf-inference-mlcommons-cpp</li> <li>app-mlperf-inference-mlcommons-python</li> <li>benchmark-program-mlperf</li> <li>run-mlperf-inference-app</li> </ul>"},{"location":"list_of_scripts/#modular-mlperf-training-benchmark-pipeline","title":"Modular MLPerf training benchmark pipeline","text":"<ul> <li>app-mlperf-training-nvidia</li> <li>app-mlperf-training-reference</li> </ul>"},{"location":"list_of_scripts/#modular-application-pipeline","title":"Modular application pipeline","text":"<ul> <li>app-image-corner-detection</li> </ul>"},{"location":"list_of_scripts/#platform-information","title":"Platform information","text":"<ul> <li>detect-cpu</li> <li>detect-os</li> </ul>"},{"location":"list_of_scripts/#python-automation","title":"Python automation","text":"<ul> <li>activate-python-venv (Activate virtual Python environment)</li> <li>get-generic-python-lib</li> <li>get-python3</li> <li>install-generic-conda-package</li> <li>install-python-src</li> <li>install-python-venv</li> </ul>"},{"location":"list_of_scripts/#remote-automation","title":"Remote automation","text":"<ul> <li>remote-run-commands</li> </ul>"},{"location":"list_of_scripts/#reproduce-mlperf-benchmarks","title":"Reproduce MLPerf benchmarks","text":"<ul> <li>app-mlperf-inference-nvidia</li> <li>reproduce-mlperf-octoml-tinyml-results</li> <li>reproduce-mlperf-training-nvidia</li> <li>wrapper-reproduce-octoml-tinyml-submission</li> </ul>"},{"location":"list_of_scripts/#reproducibility-and-artifact-evaluation","title":"Reproducibility and artifact evaluation","text":"<ul> <li>get-ipol-src</li> <li>process-ae-users</li> <li>reproduce-ipol-paper-2022-439</li> </ul>"},{"location":"list_of_scripts/#tests","title":"Tests","text":"<ul> <li>print-croissant-desc</li> <li>print-hello-world</li> <li>print-hello-world-java</li> <li>print-hello-world-javac</li> <li>print-hello-world-py</li> <li>print-python-version</li> <li>run-python</li> <li>test-download-and-extract-artifacts</li> <li>test-set-sys-user-cm</li> <li>upgrade-python-pip</li> </ul>"},{"location":"list_of_scripts/#tinyml-automation","title":"TinyML automation","text":"<ul> <li>create-fpgaconvnet-app-tinyml</li> <li>create-fpgaconvnet-config-tinyml</li> <li>flash-tinyml-binary</li> <li>get-microtvm</li> <li>get-zephyr</li> <li>get-zephyr-sdk</li> </ul>"},{"location":"list_of_scripts/#list-of-all-sorted-cm-scripts","title":"List of all sorted CM scripts","text":"<ul> <li>activate-python-venv (Activate virtual Python environment.)</li> <li>add-custom-nvidia-system</li> <li>app-image-classification-onnx-py</li> <li>app-image-classification-tf-onnx-cpp</li> <li>app-image-classification-torch-py</li> <li>app-image-classification-tvm-onnx-py</li> <li>app-image-corner-detection</li> <li>app-loadgen-generic-python</li> <li>app-mlperf-inference</li> <li>app-mlperf-inference-ctuning-cpp-tflite</li> <li>app-mlperf-inference-dummy</li> <li>app-mlperf-inference-intel</li> <li>app-mlperf-inference-mlcommons-cpp</li> <li>app-mlperf-inference-mlcommons-python</li> <li>app-mlperf-inference-nvidia</li> <li>app-mlperf-inference-qualcomm</li> <li>app-mlperf-training-nvidia</li> <li>app-mlperf-training-reference</li> <li>app-stable-diffusion-onnx-py</li> <li>benchmark-any-mlperf-inference-implementation</li> <li>benchmark-program</li> <li>benchmark-program-mlperf</li> <li>build-docker-image</li> <li>build-dockerfile</li> <li>build-mlperf-inference-server-nvidia</li> <li>calibrate-model-for.qaic</li> <li>compile-model-for.qaic</li> <li>compile-program</li> <li>convert-csv-to-md</li> <li>convert-ml-model-huggingface-to-onnx</li> <li>copy-to-clipboard</li> <li>create-conda-env</li> <li>create-fpgaconvnet-app-tinyml</li> <li>create-fpgaconvnet-config-tinyml</li> <li>create-patch</li> <li>destroy-terraform</li> <li>detect-cpu</li> <li>detect-os</li> <li>detect-sudo</li> <li>download-and-extract</li> <li>download-file</li> <li>download-torrent</li> <li>dump-pip-freeze</li> <li>extract-file</li> <li>fail</li> <li>flash-tinyml-binary</li> <li>generate-mlperf-inference-submission</li> <li>generate-mlperf-inference-user-conf</li> <li>generate-mlperf-tiny-report</li> <li>generate-mlperf-tiny-submission</li> <li>generate-nvidia-engine</li> <li>get-android-sdk</li> <li>get-aocl</li> <li>get-aria2</li> <li>get-aws-cli</li> <li>get-bazel</li> <li>get-bert-squad-vocab</li> <li>get-blis</li> <li>get-brew</li> <li>get-ck</li> <li>get-ck-repo-mlops</li> <li>get-cl (Detect or install Microsoft C compiler.)</li> <li>get-cmake</li> <li>get-cmsis_5</li> <li>get-compiler-flags</li> <li>get-compiler-rust</li> <li>get-conda</li> <li>get-croissant</li> <li>get-cuda</li> <li>get-cuda-devices</li> <li>get-cudnn</li> <li>get-dataset-cifar10</li> <li>get-dataset-cnndm</li> <li>get-dataset-coco</li> <li>get-dataset-coco2014</li> <li>get-dataset-criteo</li> <li>get-dataset-imagenet-aux</li> <li>get-dataset-imagenet-calibration</li> <li>get-dataset-imagenet-helper</li> <li>get-dataset-imagenet-train</li> <li>get-dataset-imagenet-val</li> <li>get-dataset-kits19</li> <li>get-dataset-librispeech</li> <li>get-dataset-openimages</li> <li>get-dataset-openimages-annotations</li> <li>get-dataset-openimages-calibration</li> <li>get-dataset-openorca</li> <li>get-dataset-squad</li> <li>get-dataset-squad-vocab</li> <li>get-dlrm</li> <li>get-dlrm-data-mlperf-inference</li> <li>get-docker</li> <li>get-gcc (Detect or install GCC compiler.)</li> <li>get-generic-python-lib</li> <li>get-generic-sys-util</li> <li>get-git-repo</li> <li>get-github-cli</li> <li>get-go</li> <li>get-google-saxml</li> <li>get-google-test</li> <li>get-ipol-src</li> <li>get-java</li> <li>get-javac</li> <li>get-lib-armnn</li> <li>get-lib-dnnl</li> <li>get-lib-protobuf</li> <li>get-lib-qaic-api</li> <li>get-llvm (Detect or install LLVM compiler.)</li> <li>get-microtvm</li> <li>get-ml-model-3d-unet-kits19</li> <li>get-ml-model-bert-base-squad</li> <li>get-ml-model-bert-large-squad</li> <li>get-ml-model-dlrm-terabyte</li> <li>get-ml-model-efficientnet-lite</li> <li>get-ml-model-gptj</li> <li>get-ml-model-huggingface-zoo</li> <li>get-ml-model-llama2</li> <li>get-ml-model-mobilenet</li> <li>get-ml-model-neuralmagic-zoo</li> <li>get-ml-model-resnet50</li> <li>get-ml-model-retinanet</li> <li>get-ml-model-retinanet-nvidia</li> <li>get-ml-model-rnnt</li> <li>get-ml-model-stable-diffusion</li> <li>get-ml-model-tiny-resnet</li> <li>get-ml-model-using-imagenet-from-model-zoo</li> <li>get-mlperf-inference-intel-scratch-space</li> <li>get-mlperf-inference-loadgen</li> <li>get-mlperf-inference-nvidia-common-code</li> <li>get-mlperf-inference-nvidia-scratch-space</li> <li>get-mlperf-inference-results</li> <li>get-mlperf-inference-results-dir</li> <li>get-mlperf-inference-src</li> <li>get-mlperf-inference-submission-dir</li> <li>get-mlperf-inference-sut-configs</li> <li>get-mlperf-inference-sut-description</li> <li>get-mlperf-inference-utils</li> <li>get-mlperf-logging</li> <li>get-mlperf-power-dev</li> <li>get-mlperf-tiny-eembc-energy-runner-src</li> <li>get-mlperf-tiny-src</li> <li>get-mlperf-training-nvidia-code</li> <li>get-mlperf-training-src</li> <li>get-nvidia-docker</li> <li>get-nvidia-mitten</li> <li>get-onnxruntime-prebuilt</li> <li>get-openssl</li> <li>get-preprocessed-dataset-criteo</li> <li>get-preprocessed-dataset-imagenet</li> <li>get-preprocessed-dataset-kits19</li> <li>get-preprocessed-dataset-librispeech</li> <li>get-preprocessed-dataset-openimages</li> <li>get-preprocessed-dataset-openorca</li> <li>get-preprocessed-dataset-squad</li> <li>get-preprocesser-script-generic</li> <li>get-python3</li> <li>get-qaic-apps-sdk</li> <li>get-qaic-platform-sdk</li> <li>get-qaic-software-kit</li> <li>get-rclone</li> <li>get-rocm</li> <li>get-spec-ptd</li> <li>get-sys-utils-cm</li> <li>get-sys-utils-min</li> <li>get-tensorrt</li> <li>get-terraform</li> <li>get-tvm</li> <li>get-tvm-model</li> <li>get-xilinx-sdk</li> <li>get-zendnn</li> <li>get-zephyr</li> <li>get-zephyr-sdk</li> <li>gui</li> <li>import-mlperf-inference-to-experiment</li> <li>import-mlperf-tiny-to-experiment</li> <li>import-mlperf-training-to-experiment</li> <li>install-aws-cli</li> <li>install-bazel</li> <li>install-cmake-prebuilt</li> <li>install-cuda-package-manager</li> <li>install-cuda-prebuilt</li> <li>install-gcc-src</li> <li>install-generic-conda-package</li> <li>install-gflags</li> <li>install-github-cli</li> <li>install-ipex-from-src (Build IPEX from sources.)</li> <li>install-llvm-prebuilt (Install prebuilt LLVM compiler.)</li> <li>install-llvm-src (Build LLVM compiler from sources (can take &gt;30 min).)</li> <li>install-mlperf-logging-from-src</li> <li>install-nccl-libs</li> <li>install-numactl-from-src (Build numactl from sources.)</li> <li>install-onednn-from-src (Build oneDNN from sources.)</li> <li>install-onnxruntime-from-src (Build onnxruntime from sources.)</li> <li>install-openssl</li> <li>install-pip-package-for-cmind-python</li> <li>install-python-src</li> <li>install-python-venv</li> <li>install-pytorch-from-src (Build pytorch from sources.)</li> <li>install-pytorch-kineto-from-src (Build pytorch kineto from sources.)</li> <li>install-qaic-compute-sdk-from-src</li> <li>install-rocm</li> <li>install-tensorflow-for-c</li> <li>install-tensorflow-from-src</li> <li>install-terraform-from-src</li> <li>install-tflite-from-src</li> <li>install-torchvision-from-src (Build pytorchvision from sources.)</li> <li>install-tpp-pytorch-extension (Build TPP-PEX from sources.)</li> <li>install-transformers-from-src (Build transformers from sources.)</li> <li>launch-benchmark</li> <li>prepare-training-data-bert</li> <li>prepare-training-data-resnet</li> <li>preprocess-mlperf-inference-submission</li> <li>print-croissant-desc</li> <li>print-hello-world</li> <li>print-hello-world-java</li> <li>print-hello-world-javac</li> <li>print-hello-world-py</li> <li>print-python-version</li> <li>process-ae-users</li> <li>process-mlperf-accuracy</li> <li>prune-bert-models</li> <li>prune-docker</li> <li>publish-results-to-dashboard</li> <li>pull-git-repo</li> <li>push-csv-to-spreadsheet</li> <li>push-mlperf-inference-results-to-github</li> <li>remote-run-commands</li> <li>reproduce-ipol-paper-2022-439</li> <li>reproduce-mlperf-octoml-tinyml-results</li> <li>reproduce-mlperf-training-nvidia</li> <li>run-docker-container</li> <li>run-mlperf-inference-app</li> <li>run-mlperf-inference-mobilenet-models</li> <li>run-mlperf-inference-submission-checker</li> <li>run-mlperf-power-client</li> <li>run-mlperf-power-server</li> <li>run-mlperf-training-submission-checker</li> <li>run-python</li> <li>run-terraform</li> <li>save-mlperf-inference-implementation-state</li> <li>set-device-settings-qaic</li> <li>set-echo-off-win</li> <li>set-performance-mode</li> <li>set-sqlite-dir</li> <li>set-venv</li> <li>tar-my-folder</li> <li>test-download-and-extract-artifacts</li> <li>test-mlperf-inference-retinanet</li> <li>test-set-sys-user-cm</li> <li>truncate-mlperf-inference-accuracy-log</li> <li>upgrade-python-pip</li> <li>wrapper-reproduce-octoml-tinyml-submission</li> </ul>"},{"location":"mlperf-cm-automation-demo/","title":"Mlperf cm automation demo","text":"<p>[ Back to index ]</p> <p>Moved here.</p>"},{"location":"mlperf-education-workgroup/","title":"Mlperf education workgroup","text":"<p>Moved here.</p>"},{"location":"news-mlperf-v3.1/","title":"News mlperf v3.1","text":"<p>[ Back to index ]</p>"},{"location":"news-mlperf-v3.1/#new-cm-automation-capabilities-for-mlperf-inference-benchmarks","title":"New CM automation capabilities for MLPerf inference benchmarks","text":"<p>This section highlights the new capabilities of the Collective Mind automation language to modularize MLPerf benchmarks and enable mass-scale submissions by the community added by the community,  cTuning.org and cKnowledge.org via public MLPerf challenges.</p> <p>The latest CM/CK MLPerf automation supports practically any combination of  * All MLPerf models including GPT-J * Main MLPerf implementations (all reference, Nvidia, Intel, TFLite, MIL) * Main frameworks and run-times (DeepSparse, PyTorch, TensorFlow, TFLite, TVM, TensorRT, ONNX, NCNN, Triton) * Diverse hardware including Coral TPU, Nvidia GPUs (A100,T4,L4,RTX 4090, Jetson Orin), Intel/AMD servers, Graviton, NeoVerse, Apple metal * All major cloud providers including AWS, GCP and Azure with Ubuntu, RHEL, SLES, Amazon Linux and Windows 11 * Most laptops and servers including Apple, Dell, Toshiba, Asus, HPE, Lenovo</p>"},{"location":"news-mlperf-v3.1/#highlights-of-the-mlperf-inference-v31-results-from-the-community-and-ctuning","title":"Highlights of the MLPerf inference v3.1 results from the community and cTuning","text":"<p>The goal of CM and CK technology is to democratize MLPerf and make it accessible to everyone to showcase, select and co-design the most efficient AI systems (performance, power, accuracy, costs).</p> <p>Our open-source technology has helped the community  submit and reproduce &gt;90% of all MLPerf inference v3.1 results, &gt;90% of all the power results and &gt;55% of all the submitted system configurations via the cTuning foundation. </p> <p>Never before has MLPerf inference benchmark crossed 10,000 results and this time cTuning foundation and the community submitted 12,000+ results  showing that anyone can benchmark AI systems and even set record performance using our CM based MLPerf inference automation.</p> <p>We thank all our contributors, Neural Magic, TTA, One Stop Systems, Nutanix, Collabora, Deelvin, cKnowledge, AMD and Nvidia for interesting discussions and feedback that helped to improve the open-source MLCommons CM automation workflows for MLPerf benchmarks and make them available to everyone!</p>"},{"location":"news-mlperf-v3.1/#new-cm-capabilities-to-automate-experiments-optimizations-and-design-space-exploration","title":"New CM capabilities to automate experiments, optimizations and design space exploration","text":"<p>The 1st CM experiment automation  for BERT performance/power/accuracy exploration from NeuralMagic Zoo, Hugging Face Hub and NeurIPS papers (sparsity, quantization and batch size) across multiple AMD, Intel and ARM-based systems.</p>"},{"location":"news-mlperf-v3.1/#upcoming-events-powered-by-cm","title":"Upcoming events powered by CM","text":"<ul> <li>CK playground for MLPerf at AI hardware summit'23</li> <li>CM automation language to reproduce papers from the ACM MICRO'23 conference</li> <li>Tutorial about CM automation language and CK playground for MLPerf at IISWC'23</li> <li>CM automation language and CK playground to run MLPerf at the Student Cluster Competition at SuperComputing'23</li> </ul> <p>More events to come soon!</p>"},{"location":"news/","title":"News","text":"<p>[ Back to index ]</p>"},{"location":"news/#news-from-the-mlcommons-task-force-on-automation-and-reproducibility","title":"News from the MLCommons Task Force on Automation and Reproducibility","text":""},{"location":"news/#202406","title":"202406","text":"<ul> <li>We published a white paper about the Collective Knowledge Playground, Collective Mind, MLPerf and CM4MLOps: https://arxiv.org/abs/2406.16791</li> </ul>"},{"location":"news/#202403","title":"202403","text":"<ul> <li>cKnowledge has completed a collaborative engineering project with MLCommons    to enhance CM workflow automation to run MLPerf inference benchmarks   across different models, software and hardware from different vendors in a unified way: GUI.</li> <li>cTuning has validated the new MLCommons CM workflow    to automate ~90% of all MLPerf inference v4.0 performance and power submissions   while finding some top performance and cost-effective software/hardware configurations for AI systems:    report.</li> <li>We presented a new project to \"Automatically Compose High-Performance and Cost-Efficient AI Systems with MLCommons' Collective Mind and MLPerf\"   at the MLPerf-Bench workshop @HPCA'24.</li> </ul>"},{"location":"news/#202311","title":"202311","text":"<ul> <li> <p>ACM/IEEE MICRO'23 used CM    to automate artifact evaluation    and make it easier for research to understand, prepare, run and reproduce research projects   from published papers.</p> </li> <li> <p>The ACM YouTube channel has released the ACM REP'23 keynote about the MLCommons CM automation language and CK playground:   toward a common language to facilitate reproducible research and technology transfer.</p> </li> <li> <p>Grigori Fursin and Arjun Suresh   served as MLCommons liasons at the Student Cluster Competition at SuperComputing'23   helping the community run, optimize and enhance MLPerf inference benchmarks using the MLCommons CM workflow automation language   and CK playground.</p> </li> </ul>"},{"location":"news/#202310","title":"202310","text":"<ul> <li> <p>Grigori Fursin gave an invited talk at AVCC'23 about our MLCommons CM automation language and how it can help    to develop modular, portable and technology-agnostic benchmarks.</p> </li> <li> <p>Grigori Fursin    gave an IISWC'23 tutorial about our CM workflow automation language    and how it can make it easier for researchers to reproduce their projects and validate in the real world   across rapidly evolving software and hardware.</p> </li> </ul>"},{"location":"news/#202309","title":"202309","text":"<ul> <li>The Collective Knowledge Technology v3 with the open-source MLCommons CM automation language,   CK playground    and C++ Modular Inference Library   helped the community automate &gt; 90% of all MLPerf inference v3.1 results    and cross 10000 submissions in one round for the first time (submitted via cTuning foundation)!   Here is the list of the new CM/CK capabilities available to everyone    to prepare and automate their future MLPerf submissions - please check this HPC Wire article   about cTuning's community submission and don't hesitate to reach us via Discord server for more info!*</li> </ul>"},{"location":"news/#202309_1","title":"202309","text":"<p>Our CK playground was featured at the AI hardware summit'23</p>"},{"location":"news/#202307","title":"202307","text":"<p>The overview of the MedPerf project was published in Nature:  Federated benchmarking of medical artificial intelligence with MedPerf!</p>"},{"location":"news/#202306","title":"202306","text":"<p>We were honored to give a keynote about our MLCommons automation and reproducibility language to faciliate reproducible experiments and bridge the growing gap between research and production at the 1st ACM conference for Reproducibility and Replicability.</p>"},{"location":"news/#202305","title":"202305","text":"<p>Following the successful validation of our CK/CM technology by the community to automate MLPerf inference v3.0 submissions, the MLCommons Task Force on automation and reproducibilty  have prepared a presentation  about our development plans for the MLCommons CK playground  and MLCommons CM scripting language for Q3 2023.</p> <p>Our current mission is to prepare new optimization challenges  to help companies, students, researchers and practitioners reproduce and optimize MLPerf inference v3.0 results and/or submit new/better results to MLPerf inference v3.1 across diverse models, software and hardware  as a community effort.</p>"},{"location":"news/#202304","title":"202304","text":"<p>We have successfully validated the MLCommons CK and CM technology  to automate ~80% of MLPerf inference v3.0 submissions (98% of all power results).</p> <p>MLCommons CK and CM has helped to automatically interconnect very diverse technology  from Neural Magic, Qualcomm, Krai, cKnowledge, OctoML, Deelvin, DELL, HPE, Lenovo, Hugging Face, Nvidia and Apple  and run it across diverse CPUs, GPUs and DSPs with PyTorch,  ONNX, QAIC, TF/TFLite, TVM and TensorRT using popular cloud providers (GCP, AWS, Azure) and individual servers and edge devices  via our recent open optimization challenge.</p> <ul> <li>Forbes article highlighting our MLCommons CK technology</li> <li>ZDNet article</li> <li>LinkedIn article from Grigori Fursin (MLCommons Task Force co-chair)</li> <li>Linkedin article from Arjun Suresh (MLCommons Task Force co-chair)</li> </ul>"},{"location":"news/#202304_1","title":"202304","text":"<p>We pre-released a free, open-source and technology-agnostic Collective Knowledge Playground (MLCommon CK) to automate benchmarking, optimization and reproducibility of MLperf inference benchmark via collaborative challenges!</p>"},{"location":"news/#202302","title":"202302","text":"<p>New GUI to visualize all MLPerf results is available here.</p>"},{"location":"news/#202301","title":"202301","text":"<p>New GUI to run MLPerf inference is available here.</p>"},{"location":"news/#202212","title":"202212","text":"<p>We have added GitHub actions to the MLPerf inference repo to automatically test MLPerf inference benchmark with different models, data sets and frameworks using our customizable MLCommons CM-MLPerf workflows:</p> <ul> <li>Test LoadGen</li> <li>Test RetinaNet</li> <li>Test ResNet50</li> </ul>"},{"location":"news/#202211","title":"202211","text":"<p>Grigori Fursin and Arjun Suresh  successfully validated the prototype of  their new workflow automation langugage (MLCommons CM)  at the Student Cluster Competition at SuperComputing'22. It was used to make it easier to prepare and run the MLPerf inference benchmark just under 1 hour! Please test it using this CM tutorial.</p>"},{"location":"news/#202210","title":"202210","text":"<p>We have prototyped modular CM-MLPerf containers  using our portable MLCommons CM scripting language.</p>"},{"location":"news/#202209","title":"202209","text":"<p>We have prepared a presentation  about the mission of the MLCommons Task Force on automation and reproducibility.</p>"},{"location":"news/#202308","title":"202308","text":"<p>We have prototyped universal MLPerf inference workflows using the MLCommons CM scripting language.</p>"},{"location":"news/#202307_1","title":"202307","text":"<p>Grigori Fursin  and Arjun Suresh have established an  MLCommons Task Force on automation and reproducibility to continue developing MLCommons CK/CM as a community effort.</p>"},{"location":"news/#202306_1","title":"202306","text":"<p>We have pre-released stable and portable automation CM scripts  to unify MLOps and DevOps across diverse software, hardware, models and data.</p>"},{"location":"news/#202305_1","title":"202305","text":"<p>We have prepared an example of portable and modular image classification using the MLCommons CM scriping language.</p>"},{"location":"news/#202203","title":"202203","text":"<p>Following positive feedback from the community about our Collective Knowledge concept  to facilitate reproducible research and technology transfer across rapidly evolving models, software, hardware and data, we have started developing its simplified version as a common scripting language to connect academia and industry: Collective Mind framework (MLCommons CM aka CK2).</p>"},{"location":"taskforce/","title":"Taskforce","text":"<p>[ Back to index ]</p>"},{"location":"taskforce/#mlcommons-task-force-on-automation-and-reproducibility","title":"MLCommons Task Force on Automation and Reproducibility","text":"<p>News (May 2024): our task force has successfully accomplished the first goal   to provide a stable CM interface for MLPerf benchmarks   and discussing the next steps with MLCommons - please stay tuned for more details!</p>"},{"location":"taskforce/#mission","title":"Mission","text":"<ul> <li>Extend MLCommons CM workflow automation framework and   reusable automation recipes (CM scripts)   to automate MLCommons projects and make it easier to assemble, run, reproduce, customize and optimize ML(Perf) benchmarks   in a unified and automated way across diverse models, data sets, software and hardware from different vendors.</li> <li>Extend CM workflows    to automate and reproduce MLPerf inference submissions from different vendors starting from v3.1.</li> <li>Encode MLPerf rules and best practices in the CM automation recipes and workflows for MLPerf   to help MLPerf submitters avoid going through many README files and track all the latest MLPerf changes and updates.</li> </ul>"},{"location":"taskforce/#sponsors","title":"Sponsors","text":"<p>We thank cKnowledge.org, cTuning.org, and MLCommons for sponsoring this project!</p>"},{"location":"taskforce/#citing-cm","title":"Citing CM","text":"<p>If you found CM useful, please cite this article:  [ ArXiv ], [ BibTex ].</p>"},{"location":"taskforce/#current-projects","title":"Current projects","text":"<ul> <li> <p>Continue improving CM to support different MLCommons projects for universal benchmarking and optimization across different platforms.</p> </li> <li> <p>Extend CM workflows to reproduce MLPerf inference v4.0 submissions (Intel, Nvidia, Qualcomm, Google, Red Hat, etc) via a unified interface.</p> </li> <li> <p>Prepare tutorial for MLPerf inference v4.1 submissions via CM.</p> </li> <li> <p>Discuss how to introduce the CM automation badge    to MLPerf inference v4.1 submission similar to ACM/IEEE/NeurIPS reproducibility badges to make it easier for   all submitters to re-run and reproduce each others\u2019 results before the publication date.</p> </li> <li> <p>Develop a more universal Python and C++ wrapper for the MLPerf loadgen   with the CM automation to support different models, data sets, software   and hardware: Python prototype;    C++ prototype.</p> </li> <li> <p>Collaborate with system vendors and cloud providers to help them benchmark   their platforms using the best available MLPerf inference implementation.</p> </li> <li> <p>Collaborate with other MLCommons working groups to autoamte, modularize and unify   their benchmarks using CM automation recipes.</p> </li> <li> <p>Use CM to modularize and automate the upcoming automotive benchmark.</p> </li> <li> <p>Use MLCommons Croissant    to unify MLPerf datasets.</p> </li> </ul>"},{"location":"taskforce/#current-tasks","title":"Current tasks","text":"<ul> <li>Improving CM workflow automation framework: GitHub ticket</li> <li>Updating/refactoring CM docs (framework and MLPef workflows): GitHub ticket</li> <li>Improving CM scripts to support MLPerf: GitHub ticket</li> <li>Adding profiling and performance analysis during benchmarking: GitHub ticket</li> <li>Improving universal build and run scripts to support cross-platform compilation: GitHub ticket</li> <li>Automate ABTF benchmarking via CM: GitHub ticket</li> <li>Help automate MLPerf inference benchmark at the Student Cluster Competition'24: GitHub ticket</li> </ul>"},{"location":"taskforce/#completed-deliverables","title":"Completed deliverables","text":"<ul> <li> <p>Developed reusable and technology-agnostic automation recipes and workflows    with a common and human-friendly interface (MLCommons Collective Mind aka CM) to modularize   MLPerf inference benchmarks and run them in a unified and automated way   across diverse models, data sets, software and hardware from different   vendors.</p> </li> <li> <p>Added GitHub actions    to test MLPerf inference benchmarks using CM.</p> </li> <li> <p>Encoded MLPerf inference rules and best practices in the CM automation   recipes and workflows for MLPerf    and reduced the burden for submitters to go through numerous README files    and track all the latest changes and reproduce results.</p> </li> <li> <p>Automated MLPerf inference submissions    and made it easier to re-run and reproduce results    (see submitters orientation    and CM-MLPerf documentation).</p> </li> <li> <p>Started developing an open-source platform to automatically compose   high-performance and cost-effective AI applications and systems using   MLPerf and CM (see our presentation at MLPerf-Bench at HPCA\u201924).</p> </li> <li> <p>Supported AI, ML and Systems conferences to automate artifact evaluation   and reproducibility initiatives (see CM at ACM/IEEE MICRO\u201923    and SCC\u201923/SuperComputing\u201923).</p> </li> </ul>"},{"location":"taskforce/#resources","title":"Resources","text":"<ul> <li>CM GitHub project</li> <li>CM concept (keynote at ACM REP'23)</li> <li>CM Getting Started Guide</li> <li>CM-MLPerf commands</li> <li>CM-MLPerf GUI</li> <li>ACM artifact review and badging methodology </li> <li>Artifact Evaluation at ML and systems conferences</li> <li>Terminology (ACM/NISO): Repeatability, Reproducibility and Replicability</li> <li>CM motivation (ACM TechTalk about reproducing 150+ research papers and validating them in the real world)</li> </ul>"},{"location":"taskforce/#acknowledgments","title":"Acknowledgments","text":"<p>This task force was established by Grigori Fursin  after he donated his CK and CM automation technology to MLCommons in 2022 to benefit everyone. Since then, this open-source technology is being developed as a community effort based on user feedback. We would like to thank all our volunteers, collaborators and contributors  for their support, fruitful discussions, and useful feedback! </p>"},{"location":"tests/","title":"TESTS","text":""},{"location":"archive/taskforce-2022/","title":"Taskforce 2022","text":"<p>[ Back to index ]</p>"},{"location":"archive/taskforce-2022/#mlcommons-taskforce-on-education-and-reproducibility","title":"MLCommons Taskforce on Education and Reproducibility","text":""},{"location":"archive/taskforce-2022/#mission","title":"Mission","text":"<ul> <li>help you automate and validate your MLPerf inference benchmark submissions to the v3.0 round for any hardware target (deadline: March 3, 2023) -    join the related Discord channel;</li> <li>enable faster innovation while adapting to the world of rapidly evolving software, hardware,    and data by encoding everyone\u2019s knowledge in a form of    portable, iteroperable and customizable automation recipes    reusable across the community;</li> <li>modularize AI and ML Systems by decomposing them into above automation recipes   using the MLCommons CK2 automation meta-framework (aka CM);</li> <li>automate benchmarking, design space exploration and optimization of AI and ML Systems across diverse software and hardware stacks;</li> <li>help the community reproduce MLPerf benchmarks, prepare their own submissions and deploy Pareto-optimal ML/AI systems in the real world;</li> <li>support student competitions, reproducibility initiatives and artifact evaluation at ML and Systems conferences using the rigorous MLPerf methodology and the MLCommons automation meta-framework.</li> </ul>"},{"location":"archive/taskforce-2022/#co-chairs-and-tech-leads","title":"Co-chairs and tech leads","text":"<ul> <li>Grigori Fursin (CM project coordinator)</li> <li>Arjun Suresh</li> </ul>"},{"location":"archive/taskforce-2022/#discord-server","title":"Discord server","text":"<ul> <li>Invite link</li> </ul>"},{"location":"archive/taskforce-2022/#meeting-notes-and-news","title":"Meeting notes and news","text":"<ul> <li>Shared doc</li> </ul>"},{"location":"archive/taskforce-2022/#conf-calls","title":"Conf-calls","text":"<p>Following our successful community submission to MLPerf inference v3.0,  we will set up new weekly conf-calls shortly - please stay tuned for more details!</p> <p>Please add your topics for discussion in the meeting notes or via GitHub tickets.</p>"},{"location":"archive/taskforce-2022/#mailing-list","title":"Mailing list","text":"<p>Please join our mailing list here.</p>"},{"location":"archive/taskforce-2022/#gui-for-mlperf-inference","title":"GUI for MLPerf inference","text":"<ul> <li>Run MLPerf inference </li> <li>Prepare submission</li> <li>Visualize past and on-going results</li> </ul>"},{"location":"archive/taskforce-2022/#on-going-projects","title":"On-going projects","text":"<p>See our R&amp;D roadmap for Q4 2022 and Q1 2023</p> <ul> <li>Modularize MLPerf benchmarks and make it easier to run, optimize, customize and reproduce them across rapidly evolving software, hardware and data. </li> <li>Implement and enhance cross-platform CM scripts to make MLOps and DevOps more interoperable, reusable, portable, deterministic and reproducible. </li> <li>Lower the barrier of entry for new MLPerf submitters and reduce their associated costs. </li> <li>Develop universal, modular and portable benchmarking workflow that can run on any software/hardware stack from the cloud to embedded devices.</li> <li>Automate design space exploration and optimization of the whole ML/SW/HW stack to trade off performance, accuracy, energy, size and costs.</li> <li>Automate submission of Pareto-efficient configurations to MLPerf.</li> <li>Help end-users of ML Systems visualize all MLPerf results, reproduce them and deploy Pareto-optimal ML/SW/HW stacks in production.</li> </ul>"},{"location":"archive/taskforce-2022/#purpose","title":"Purpose","text":"<p>MLCommons is a non-profit consortium of 50+ companies that was originally created  to develop a common, reproducible and fair benchmarking methodology for new AI and ML hardware.</p> <p>MLCommons has developed an open-source reusable module called loadgen that efficiently and fairly measures the performance of inference systems. It generates traffic for scenarios that were formulated by a diverse set of experts from MLCommons to emulate the workloads seen in mobile devices, autonomous vehicles, robotics, and cloud-based setups.</p> <p>MLCommons has also prepared several reference ML tasks, models and datasets  for vision, recommendation, language processing and speech recognition to let companies benchmark and compare their new hardware in terms of accuracy, latency, throughput and energy in a reproducible way twice a year.</p> <p>The first goal of this open automation and reproducibility taskforce is to  develop a light-weight and open-source automation meta-framework that can make MLOps and DevOps more interoperable, reusable, portable, deterministic and reproducible. </p> <p>We then use this automation meta-framework to develop plug&amp;play workflows for the MLPerf benchmarks to make it easier for the newcomers to run them  across diverse hardware, software and data and automatically plug in  their own ML tasks, models, data sets, engines, libraries and tools.</p> <p>Another goal is to use these portable MLPerf workflows to help students, researchers and engineers participate in crowd-benchmarking and exploration of the design space tradeoffs  (accuracy, latency, throughput, energy, size, etc.) of their ML Systems from the cloud to the edge using the mature MLPerf methodology while automating the submission of their Pareto-efficient configurations to the open division of the MLPerf inference benchmark.</p> <p>The final goal is to help end-users reproduce MLPerf results  and deploy the most suitable ML/SW/HW stacks in production  based on their requirements and constraints.</p>"},{"location":"archive/taskforce-2022/#technology","title":"Technology","text":"<p>This MLCommons taskforce is developing an open-source and technology-neutral  Collective Mind meta-framework (CM) to modularize ML Systems and automate their benchmarking, optimization  and design space exploration across continuously changing software, hardware and data.</p> <p>CM is the second generation of the MLCommons CK workflow automation framework  that was originally developed to make it easier to reproduce research papers and validate them in the real world.</p> <p>As a proof-of-concept, this technology was successfully used to automate  MLPerf benchmarking and submissions from Qualcomm, HPE, Dell, Lenovo, dividiti, Krai, the cTuning foundation and OctoML. For example, it was used and extended by Arjun Suresh  with several other engineers to automate the record-breaking MLPerf inference benchmark submission for Qualcomm AI 100 devices.</p> <p>The goal of this group is to help users automate all the steps to prepare and run MLPerf benchmarks across any ML models, data sets, frameworks, compilers and hardware using the MLCommons CM framework.</p> <p>Here is an example of current manual and error-prone MLPerf benchmark preparation steps:</p> <p></p> <p>Here is the concept of CM-based automated workflows:</p> <p></p> <p>We have finished prototyping the new CM framework in summer 2022 based on the feedback of CK users and successfully used it to modularize MLPerf and automate the submission of benchmarking results to the MLPerf inference v2.1. See this tutorial for more details.</p> <p>We continue developing CM as an open-source educational toolkit  to help the community learn how to modularize, crowd-benchmark, optimize and deploy  Pareto-efficient ML Systems based on the mature MLPerf methodology and portable CM scripts -  please check the deliverables section to keep track of our community developments and do not hesitate to join this community effort!</p>"},{"location":"archive/taskforce-2022/#agenda","title":"Agenda","text":"<p>See our R&amp;D roadmap for Q4 2022 and Q1 2023</p>"},{"location":"archive/taskforce-2022/#2022","title":"2022","text":"<ul> <li>Prototype the new CM toolkit to modularize AI&amp;ML systems based on the original CK concepts: </li> <li>DONE - GitHub .</li> <li>Decompose MLPerf inference benchmark into portable, reusable and plug&amp;play CM components:</li> <li>DONE for image classification and object detection - GitHub.</li> <li>Demonstrate CM-based automation to submit results to MLPerf inference:</li> <li>DONE - showcased CM automation concept for MLPerf inference v2.1 submission.</li> <li>Prepare CM-based MLPerf modularization and automation tutorial:</li> <li>DONE - link</li> <li>Add tests to cover critical functionality of portable CM scripts for MLPerf:</li> <li>DONE - link</li> <li>Improve CM workflow/script automaton to modularize ML Systems:</li> <li>DONE - link</li> <li>Prototype CM-based modularization of the MLPerf inference benchmark with C++ back-end and loadgen    to automatically plug in different ML models, data sets, frameworks, SDKs, compilers and tools    and automatically run it across different hardware and run-times:</li> <li>Ongoing internship of Thomas Zhu from Oxford University</li> <li>Prototype CM-based automation for TinyMLPerf:</li> <li>Ongoing</li> <li>Add basic TVM back-end to the latest MLPerf inference repo:</li> <li>Ongoing</li> <li>Convert outdated CK components for MLPerf and MLOps into the new CM format</li> <li>Ongoing</li> <li>Develop a methodology to create modular containers and MLCommons MLCubes that contain CM components to run the MLPerf inference benchmarks out of the box:</li> <li>Ongoing</li> <li>Prototype CM integration with power infrastructure (power WG) and logging infrastructure (infra WG):</li> <li>TBD</li> <li>Process feedback from the community about CM-based modularization and crowd-benchmarking of MLPerf:</li> <li>TBD</li> </ul>"},{"location":"archive/taskforce-2022/#2023","title":"2023","text":"<ul> <li>Upload all stable CM components for MLPerf to Zenodo or any other permanent archive to ensure the stability of all CM workflows for MLPerf and modular ML Systems.</li> <li>Develop CM automation for community crowd-benchmarking of the MLPerf benchmarks across different models, data sets, frameworks, compilers, run-times and platforms.</li> <li>Develop a customizable dashboard to visualize and analyze all MLPerf crowd-benchmarking results based on these examples from the legacy CK prototype:    1,    2.</li> <li>Share MLPerf benchmarking results in a database compatible with FAIR principles (mandated by the funding agencies in the USA and Europe) --    ideally, eventually, the MLCommons general datastore.</li> <li>Connect CM-based MLPerf inference submission system with our reproducibility initiatives at ML and Systems conferences.    Organize open ML/SW/HW optimization and co-design tournaments using CM and the MLPerf methodology    based on our ACM ASPLOS-REQUEST'18 proof-of-concept.</li> <li>Enable automatic submission of the Pareto-efficient crowd-benchmarking results (performance/accuracy/energy/size trade-off -    see this example from the legacy CK prototype)   to MLPerf on behalf of MLCommons.</li> <li>Share deployable MLPerf inference containers with Pareto-efficient ML/SW/HW stacks.</li> </ul>"},{"location":"archive/taskforce-2022/#resources","title":"Resources","text":"<ul> <li>Motivation:</li> <li>MLPerf Inference Benchmark (ArXiv paper)</li> <li>ACM TechTalk with CK/CM intro moderated by Peter Mattson (MLCommons president)</li> <li>Journal article with CK/CM concepts and our long-term vision</li> <li> <p>HPCA'22 presentation \"MLPerf design space exploration and production deployment\"</p> </li> <li> <p>Tools:</p> </li> <li>MLCommons CM toolkit to modularize ML&amp;AI Systems (Apache 2.0 license)</li> <li>Portable, reusable and customizable CM components to modularize ML and AI Systems (Apache 2.0 license)</li> <li> <p>Legacy CK automation for MLPerf</p> </li> <li> <p>Google Drive (public access)</p> </li> </ul>"},{"location":"archive/taskforce-2022/#acknowledgments","title":"Acknowledgments","text":"<p>This project is supported by MLCommons, OctoML  and many great contributors.</p>"},{"location":"artifact-evaluation/checklist/","title":"Checklist","text":"<p>Moved to https://github.com/ctuning/artifact-evaluation/blob/master/docs/checklist.md</p>"},{"location":"artifact-evaluation/faq/","title":"Faq","text":"<p>Moved to https://github.com/ctuning/artifact-evaluation/blob/master/docs/faq.md</p>"},{"location":"artifact-evaluation/reviewing/","title":"Reviewing","text":"<p>Moved to https://github.com/ctuning/artifact-evaluation/blob/master/docs/reviewing.md</p>"},{"location":"artifact-evaluation/submission/","title":"Submission","text":"<p>Moved to https://github.com/ctuning/artifact-evaluation/blob/master/docs/submission.md</p>"},{"location":"artifact-evaluation/hotcrp-config/","title":"Index","text":"<p>Moved to https://github.com/ctuning/artifact-evaluation/blob/master/docs/hotcrp-config/README.md</p>"},{"location":"cm/","title":"Index","text":""},{"location":"cm/#about","title":"About","text":"<p>Collective Mind (CM) is a small, modular, cross-platform and decentralized workflow automation framework  with a human-friendly interface to make it easier to build, run, benchmark and optimize applications  across diverse models, data sets, software and hardware.</p> <p>CM is a part of Collective Knowledge (CK) -  an educational community project to learn how to run emerging workloads  in the most efficient and cost-effective way across diverse  and continuously changing systems.</p> <p>CM includes a collection of portable, extensible and technology-agnostic automation recipes with a common API and CLI (aka CM scripts) to unify and automate different steps  required to compose, run, benchmark and optimize complex ML/AI applications  on any platform with any software and hardware. </p> <p>CM scripts extend the concept of <code>cmake</code> with simple Python automations, native scripts and JSON/YAML meta descriptions. They require Python 3.7+ with minimal dependencies and are  continuously extended by the community and MLCommons members to run natively on Ubuntu, MacOS, Windows, RHEL, Debian, Amazon Linux and any other operating system, in a cloud or inside automatically generated containers while keeping backward compatibility.</p> <p>CM scripts were originally developed based on the following requirements from the MLCommons members  to help them automatically compose and optimize complex MLPerf benchmarks, applications and systems across diverse and continuously changing models, data sets, software and hardware from Nvidia, Intel, AMD, Google, Qualcomm, Amazon and other vendors: * must work out of the box with the default options and without the need to edit some paths, environment variables and configuration files; * must be non-intrusive, easy to debug and must reuse existing    user scripts and automation tools (such as cmake, make, ML workflows,    python poetry and containers) rather than substituting them;  * must have a very simple and human-friendly command line with a Python API and minimal dependencies; * must require minimal or zero learning curve by using plain Python, native scripts, environment variables    and simple JSON/YAML descriptions instead of inventing new workflow languages; * must have the same interface to run all automations natively, in a cloud or inside containers.</p>"},{"location":"cm/#maintainers","title":"Maintainers","text":"<ul> <li>Grigori Fursin</li> </ul>"},{"location":"cm/#resources","title":"Resources","text":"<ul> <li>CM v2.x (2022-cur) (stable): installation on Linux, Windows, MacOS ;    docs ; popular commands ;    getting started guide</li> <li>CM v3.x aka CMX (2024-cur) (stable): docs</li> <li>MLPerf inference benchmark automated via CM</li> <li>Run MLPerf for submissions</li> <li>Run MLPerf at the Student Cluster Competition'24</li> <li>Examples of modular containers and GitHub actions with CM commands:</li> <li>GitHub action with CM commands to test MLPerf inference benchmark</li> <li>Dockerfile to run MLPerf inference benchmark via CM</li> </ul>"},{"location":"cm/#license","title":"License","text":"<p>Apache 2.0</p>"},{"location":"cm/#citing-cm-project","title":"Citing CM project","text":"<p>If you found CM automations useful, please cite this article:  [ ArXiv ], [ BibTex ].</p> <p>You can learn more about the motivation behind these projects from the following presentations:</p> <ul> <li>\"Enabling more efficient and cost-effective AI/ML systems with Collective Mind, virtualized MLOps, MLPerf, Collective Knowledge Playground and reproducible optimization tournaments\": [ ArXiv ]</li> <li>ACM REP'23 keynote about the MLCommons CM automation framework: [ slides ] </li> <li>ACM TechTalk'21 about Collective Knowledge project: [ YouTube ] [ slides ]</li> </ul>"},{"location":"cm/#acknowledgments","title":"Acknowledgments","text":"<p>The Collective Mind (CM) automation framework was originally  developed by Grigori Fursin,  as a part of the Collective Knowledge educational initiative, sponsored by cTuning.org and cKnowledge.org,  and contributed to MLCommons for the benefit of all.  This open-source technology, including CM4MLOps/CM4MLPerf, CM4ABTF, CM4Research, and more,  is a collaborative community-driven project made possible by our  amazing volunteers, collaborators, and contributors! </p>"},{"location":"cmx/","title":"Collective Mind eXtension aka Common Metadata eXchange (CMX)","text":"<p>We are developing the extension to the MLCommons Collective Mind automation framework  called Collective Mind eXtension or Common Metadata eXchange (CMX) to support open science and facilitate collaborative, reproducible, and reusable research, development,  and experimentation based on FAIR principles and the Collective Knowledge concept.</p> <p>It helps users non-intrusively convert their software projects  into file-based repositories of portable and reusable artifacts  (code, data, models, scripts) with extensible metadata,  reusable automations, a unified command-line interface, and a simple Python API.</p> <p>Such artifacts can be easily chained together into portable and technology-agnostic automation workflows, enabling users to  rerun, reproduce, and reuse complex experimental setups across diverse and rapidly  evolving models, datasets, software, and hardware.</p> <p>Such workflows, in turn, can be easily integrated with CI/CD pipelines and GitHub Actions  and used to create powerful, portable, modular and GUI-based applications.</p> <p>For example, you can run image classification and the MLPerf inference benchmark on Linux, macOS,  and Windows using a few CMX commands as follows:</p> <pre><code>pip install cmind\ncmx pull repo mlcommons@ck --dir=cm4mlops/cm4mlops\ncmx run script \"app image-classification python torch\" --quiet\ncmx run script \"run-mlperf inference _performance-only _short\" --model=resnet50 --precision=float32 --backend=onnxruntime --scenario=Offline --device=cpu --env.CM_SUDO_USER=no --quiet\ncmx show cache\n</code></pre> <p>CMX extends the Collective Mind (CM) framework, which have been successfully validated to  modularize, automate, and modernize MLPerf benchmarks.</p> <p>CMX is written in Python, requires minimal dependencies, and has been tested on various flavors of Linux, macOS, Windows, and other operating systems to automate benchmarking, building, running, and optimizing AI, ML, and other emerging workloads and systems.</p> <p>CMX encourages community collaboration to share, reuse, and improve artifacts, automations,  and experimental setups through public and private Git repositories,  rather than redeveloping them from scratch.</p> <p>CMX is available alongside the legacy CM framework via the cmind PyPI package. Please follow this guide  to install and start using it.</p> <p>If you encounter any issues or have suggestions, please don't hesitate  to open a GitHub ticket or contact the CMX author.</p>"},{"location":"cmx/#news","title":"News","text":""},{"location":"cmx/#202502-cmx-v400-release","title":"202502: CMX V4.0.0 release","text":"<p>We have released a new version 4.0.0 of CMX as a drop-in, backward-compatible replacement for the earlier Collective Mind framework (CM) and other MLCommons automations. Designed with user feedback in mind, CMX offers a simpler, more robust interface. It is available alongside Collective Mind (CM) in the Python cmind package: sources,  pypi.</p>"},{"location":"cmx/#documentation","title":"Documentation","text":"<ul> <li>MLCommons website</li> <li>GitHub</li> </ul>"},{"location":"cmx/#license","title":"License","text":"<p>Apache 2.0</p>"},{"location":"cmx/#author","title":"Author","text":"<p>Grigori Fursin.</p> <p>We thank all our contributors  for their invaluable feedback and support!</p>"},{"location":"cmx/#copyright","title":"Copyright","text":"<p>Copyright (c) 2024-2025 MLCommons</p> <p>Grigori Fursin and the cTuning foundation donated this project to MLCommons to benefit everyone.</p>"},{"location":"cmx/#citation","title":"Citation","text":"<p>If you found the CM/CMX automations for MLOps, DevOps and MLPerf helpful, kindly reference this article: [ ArXiv ], [ BibTex ].</p> <p>You are welcome to contact the author to discuss long-term plans and potential collaboration.</p>"},{"location":"cmx/TOC/","title":"CMX documentation","text":"<ul> <li>Installation (Linux, Windows, MacOS)</li> <li>Understanding CMX</li> <li>CMX commands to share and reuse artifacts with common metadata</li> <li>CMX automation actions for related artifacts</li> <li>Improving CMX framework</li> <li>Motivation</li> </ul>"},{"location":"cmx/common-automation-actions/","title":"CMX commands to share and reuse artifacts with common metadata","text":"<p>[ Back to documentation ]</p>"},{"location":"cmx/common-automation-actions/#cmx-commands-automation-actions-applicable-to-all-artifacts","title":"CMX commands (automation actions) applicable to all artifacts","text":""},{"location":"cmx/common-automation-actions/#cmx-command-line","title":"CMX command line","text":"<pre><code>$ cmx {action} {automation} [CMX options] [CMX automation action flags]\n</code></pre> <p>CMX options have format -key=value or -key (value=True)</p> <p>CMX automation flags have format --key=value or --key (value=True) </p> <p>Common actions for all artifacts and automations (<code>cmx -h</code>):  * <code>find</code> automation (artifact) - find artifacts based on alias, UID and tags  * <code>load</code> automation (artifact) - load metadata of a give artifact referenced either by alias or UID or alias,UID or --tags  * <code>update</code> automation (artifact) - update metadata of a given artifact   * <code>add</code> automation (artifact) - add new artifact  * <code>rm</code> automation (artifact) - remove artifact(s)  * <code>mv</code> automation (repo:)artifact (new_repo:)new_artifact - rename artifact or move to another CMX repository  * <code>copy</code>automation (repo:)artifact (new repo:)new_artifact - copy artifact to another repository with a new UID  * <code>help</code> automation - print available actions for a given automation</p> <p>You can add -h to above actions to see related flags for a given automation action</p> <p>These actions are inherited by all CMX automations from this   CMX Automation Class.</p>"},{"location":"cmx/common-automation-actions/#default-automations","title":"Default automations","text":"<p>When you install CMX/CM, you have the following automations available by default inside the <code>default</code> repository embedded in cmind package: </p> <ul> <li><code>core</code> - core CMX automations such as generating UID</li> <li><code>repo</code> - CMX automations to manage CMX repositories (pull, rm, pack ...)</li> <li><code>automation</code> - CMX automation to manage automations (add, rm, mv ...</li> </ul> <p>You can list all pulled repositories as follows: <pre><code>cmx find repo\n</code></pre></p> <p>You can also show extra information about repositories as follows: <pre><code>cmx show repo\n</code></pre></p> <p>You can list all available automations as follows: <pre><code>cmx find {automation}\n</code></pre></p> <p>You can list available actions for a given automation as follows: <pre><code>cmx help {automation}\n</code></pre></p> <p>You can list available flags for a given automation action as follows: <pre><code>cmx {action} {automation} -h\n</code></pre></p>"},{"location":"cmx/common-automation-actions/#python-api","title":"Python API","text":"<p>CMX provides a simple Python JSON API to manage artifacts, automations and metadata that converts above command line into 1 call with a unified Dictionary input and Dictionary output:</p> <pre><code>import cmind\n\nr = cmind.x({'action':'my action',\n             'automation':'my automation',\n             'key1':'value1',\n             'key2':'value2'})\nif r['return']&gt;0: handle error in r['error']\n\n...\n</code></pre>"},{"location":"cmx/common-automation-scripts/","title":"Common automation scripts","text":"<p>[ Back to documentation ]</p>"},{"location":"cmx/common-automation-scripts/#cmx-automations","title":"CMX automations","text":""},{"location":"cmx/common-automation-scripts/#why-cm","title":"Why CM?","text":"<p>Collective Mind (CM) is a community project to develop  a collection of portable, extensible, technology-agnostic and ready-to-use automation recipes for MLOps and DevOps with a human-friendly interface (aka CM scripts) that can help to automate all the manual steps required to prepare, build, run, benchmark and optimize complex ML/AI applications  on any platform with any software and hardware.  They require Python 3.7+ with minimal dependencies and can run natively on Ubuntu, MacOS, Windows, RHEL, Debian, Amazon Linux and any other operating system, in a cloud or inside automatically generated containers.</p> <p>CM scripts were originally developed based on the following requirements from the MLCommons engineers and researchers  to help them automatically build, benchmark and optimize complex MLPerf benchmarks across diverse and continuously changing models, data sets, software and hardware from Nvidia, Intel, AMD, Google, Qualcomm, Amazon and other vendors: * must work out of the box with the default options and without the need to edit some paths, environment variables and configuration files; * must be non-intrusive, easy to debug and must reuse existing    user scripts and automation tools (such as cmake, make, ML workflows,    python poetry and containers) rather than substituting them;  * must have a very simple and human-friendly command line with a Python API and minimal dependencies; * must require minimal or zero learning curve by using plain Python, native scripts, environment variables    and simple JSON/YAML descriptions instead of inventing new workflow languages; * must have the same interface to run all automations natively, in a cloud or inside containers.</p> <p>Let's use a relatively simple image classification example to explain how CM achieves that and how it helps to automate much more complex projects including MLPerf benchmarks and reproducibility initatives at ML and Systems conferences.</p> Expand to see the feedback and requirements from MLCommons researchers and engineers   While image classification sounds like a trivial example nowadays, it may still require many manual steps to download some validation data sets and models, install frameworks and low-level dependencies and update various environment variables and paths depending on your platform and target hardware  (for example CPU vs CUDA).  You may also need to make sure that all dependencies are compatible (for example that ONNX run-time  or PyTorch framework is compatible with your CUDA version, etc). Of course, you can also develop a container and fix all the versions but what if you or someone else  want to try a different CUDA version or newer ONNX/TF/PyTorch framework or different operating system or different model or different data set or different framework or different hardware?  While helping MLCommons automate [MLPerf inference benchmarks](https://github.com/mlcommons/inference)  and run them across diverse models, data sets, software and hardware,  we've realized that there is no portable and technology-agnostic automation tool  that can handle such cases.  The feedback from [MLCommons engineers and researchers](taskforce.md) motivated us to develop a simple automation framework that can help them  assemble, run, benchmark and optimize complex AI/ML applications  across diverse and continuously changing models, data sets, software and hardware from Nvidia, Intel, AMD, Google, Qualcomm, Amazon and other vendors."},{"location":"cmx/common-automation-scripts/#cm-automation-recipe-for-image-classification","title":"CM automation recipe for image classification","text":"<p>We designed CM as a small Python library  with a human-friendly command line, simple Python API and minimal dependencies  needed to implement automation recipes (Python 3.7+, PIP, pyyaml, git, wget) and chain them into portable workflows. CM scripts can run natively (development mode)  or inside containers that CM generates on the fly (stable mode).</p> <p>Most of the time, these dependencies are already installed on your platform. In such case, you should be able to prepare and run image classification with ONNX, ImageNet validation data set and ResNet-50 on Linux, MacOS, Windows and any other operating system using a few CM commands:</p> <p><sup> <pre><code>pip install cmind\ncm pull repo mlcommons@cm4mlops --checkout=dev\ncm run script \"python app image-classification onnx _cpu\"\n</code></pre> <p></p> <p>Note that you may need to re-login when you install cmind for the first time  to let your platform pick up path to the <code>cm</code> command line front-end.</p> <p>You can also run and customize above automation recipe in alternative ways as follows:</p> <p><sup> <pre><code>cm run script \"python app image-classification onnx _cpu\" --help\n\ncm run script \"download file _wget\" --url=https://cKnowledge.org/ai/data/computer_mouse.jpg --verify=no --env.CM_DOWNLOAD_CHECKSUM=45ae5c940233892c2f860efdf0b66e7e\ncm run script \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg\n\ncmr \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg\ncmr --tags=python,app,image-classification,onnx,_cpu --input=computer_mouse.jpg\ncmr 3d5e908e472b417e --input=computer_mouse.jpg\n\ncm docker script \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg\n\ncm gui script \"python app image-classification onnx _cpu\"\n</code></pre> <p></p> <p>If you encounter some issues, please check CM installation guide -  if it doesn't help you, please report your issues here  and/or contact us via our public Discord server -  CM is a community project being developed  and improved across diverse software and hardware  based on your feedback! </p>"},{"location":"cmx/common-automation-scripts/#how-cm-scripts-works","title":"How CM scripts works?","text":"<p>Next, we briefly explain how CM commands work - it will help you understand what happens when you see similar commands in MLPerf results, README files,  technical reports, research papers, Jupyter notebooks,  Google colab, containers, scripts and artifact appendices.</p> <p>Whenever you run <code>cm run script \"python app image-classification onnx _cpu\"</code>  or <code>cmr \"python app image-classification onnx _cpu\"</code>,  the CM script automation  will simply search for <code>_cm.yaml</code> and <code>_cm.json</code> files (CM meta-description dictionary) in all <code>script</code>  directories in all software projects registered in CM via <code>cm pull repo</code>.</p> <p>In our case, we've pulled github.com/mlcommons/ck project that has most MLCommons' CM automation recipes embedded  in a <code>cm-mlops/script</code> directory. </p> <p>Note that you can pull any public or private Git repository, download any software project  or register any local directory in the CM to search for embedded automation recipes.</p> <p>CM will then try to match all your tags without <code>_</code> prefix (<code>_</code> in tags mark  the so-called CM script variations that customize a give script behavior  and will be described later)  with a <code>tags</code> list in the CM meta-description dictionary. In our case, it will match the corresponding <code>_cm.yaml</code>  in <code>$HOME/CM/repos/mlcommons@cm4mlops/script/app-image-classification-onnx-py/_cm.yaml</code> -  a wrapper for a given CM automation recipe.</p> <p>Note that if you use unique ID instead of tags to identify automation (such as <code>3d5e908e472b417e</code>),   CM will try to match <code>uid</code> string in the CM meta descriptions instead of tags.</p>"},{"location":"cmx/common-automation-scripts/#how-cm-runs-automation-recipes","title":"How CM runs automation recipes?","text":"<p>Whenever CM finds a directory with a requested automation recipe,  it performs the following steps: * run <code>preprocess</code> function in <code>customize.py</code> if exists * run <code>run.sh</code> (Linux) or <code>run.bat</code> (Windows) if exists * run <code>postprocess</code> function in <code>customize.py</code> if exists</p> <p>Such organization makes it possible to use either Python or native OS scripts or both to implement CM automation recipes while minimizing the learning curve for CM understanding, development and debugging as requested by CM users.</p> <p>Furthermore, CM scripts can keep the source code of image classification (as shown here) that we can easily move around between projects without hardwiring paths and names.</p>"},{"location":"cmx/common-automation-scripts/#how-cm-unifies-inputs-outputs-and-environment-variables","title":"How CM unifies inputs, outputs and environment variables?","text":"<p>CM allows you to pass environment variables to <code>customize.py</code> and native scripts using <code>--env.ENV=VALUE</code>. </p> <p>When you use some flags such as <code>--input</code> in our image classification example, it will be also converted into an environment variable using <code>input_mapping</code> dictionary  in the CM meta description of this script.</p> <p>All environment variables are aggregated in <code>env</code> dictionary inside CM and then passed to <code>preprocess</code> function in <code>customize.py</code> where you can modify it programmatically. </p> <p>They are then passed to the <code>run</code> script. Since new environment variables are not preserved after <code>run</code> script, one can pass new environment variables back to CM using <code>tmp-run-env.out</code> with ENV=KEY strings as shown here or using <code>tmp-run-state.json</code> as shown here.</p>"},{"location":"cmx/common-automation-scripts/#how-cm-chains-automation-recipes-into-portable-workflows","title":"How CM chains automation recipes into portable workflows?","text":"<p>CM scripts provide a technology-agnostic wrapper with simple tags, CLI and Python API to prepare and run  user code snippets and native scripts/tools while unifying their inputs and outputs, paths and environment variables.</p> <p>Such architecture makes it possible to easily chain existing user scripts and tools into portable, technology-agnostic and powerful workflows instead of substituting or rewriting them.</p> <p>It is possible to chain CM scripts using simple  <code>deps</code> list  in a meta description of a given script:</p> <p><sup> <pre><code>deps:\n- tags: detect,os\n- tags: get,sys-utils-cm\n- names:\n  - python\n  - python3\n  tags: get,python3\n\n- tags: get,cuda\n  names:\n  - cuda\n  enable_if_env:\n    USE_CUDA:\n    - yes\n- tags: get,cudnn\n  names:\n  - cudnn\n  enable_if_env:\n    USE_CUDA:\n    - yes\n\n- tags: get,dataset,imagenet,image-classification,original\n- tags: get,dataset-aux,imagenet-aux,image-classification\n- tags: get,ml-model,resnet50,_onnx,image-classification\n  names:\n  - ml-model\n\n- tags: get,generic-python-lib,_package.Pillow\n- tags: get,generic-python-lib,_package.numpy\n- tags: get,generic-python-lib,_package.opencv-python\n\n\n- tags: get,generic-python-lib,_onnxruntime\n  names:\n  - onnxruntime\n  skip_if_env:\n    USE_CUDA:\n    - yes\n- tags: get,generic-python-lib,_onnxruntime_gpu\n  names:\n  - onnxruntime\n  enable_if_env:\n    USE_CUDA:\n    - yes\n</code></pre> <p></p> <p>Each entry in this list is a dictionary that specifies which CM script to run using <code>tags</code>. Internally, CM will be updating <code>env</code> dictionary (flat environment) and <code>state</code> dictionary  (to let scripts exchange complex data structures besides environment variables).</p> <p>If you run CM via command line, you can see internal <code>env</code> and <code>state</code> dictionaries by adding <code>-j</code> flag:</p> <pre><code>cmr \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg -j\n</code></pre> <p>Note that we use similar approach for updating environment variables similar   to calling native scripts - by default, they do not alter environment  variables at the host. However, CM allows you to do that   by explicitly specifying which environment variables and state keys  will be updated at the host using <code>new_env_keys</code> and <code>new_state_keys</code>  in the meta of a given script as shown here.  This helped us make behavior of complex CM workflows more deterministic  and reproducible.</p> <p>Each sub-dependency can be turned on or off using environment variables using <code>enable_if_env</code> dictionary or <code>disable_if_env</code> dictionary.</p> <p>You can also specify <code>version_min</code>, <code>version_max</code> and <code>version</code> in these dependencies. You can also give them some specific names such as <code>python</code> and pass versions and environment variables only to a specific script in a pipeline as follows: <pre><code>cmr \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg --adr.python.version_min=3.9\n</code></pre></p> <p>This functionality is usually implemented inside ad-hoc bash or shell scripts  with many hardwired paths and names - CM simply makes such scripts and tools  portable and reusable while enabling technology-agnostic automation workflows  with a unified interface that can adapt to any operating system and are easy  to understand.</p> <p>We can now assemble complex automation workflows by reusing all portable scripts from the community.</p> <p>In our example, we reused CM scripts to detect OS features,  install system dependencies on any supported OS  (Ubuntu, MacOS, RHEL, Arch, Debian, SLES, Windows, etc), detect or install Python and PIP packages, download and preprocess data sets and models, etc.</p>"},{"location":"cmx/common-automation-scripts/#how-to-add-new-cm-scripts","title":"How to add new CM scripts?","text":"<p>One of the main requirement for CM was to provide a very light-weight connectors  between existing automation scripts and tools rather than substituting them.</p> <p>You can add your own scripts and tools to CM using the following command that will create a ready-to-use dummy CM script:</p> <pre><code>cm add script my-script --tags=my,script\n</code></pre> <p>You can already run this dummy script and plug it into other CM workflows: <pre><code>cmr \"my script\"\n</code></pre></p> <p>You can also run it from python as follows: <pre><code>import cmind\noutput=cmind.access({'action':'run', \n                     'automation':'script', \n                     'tags':'my,script'})\nif output['return']==0: print (output)\n</code></pre></p>"},{"location":"cmx/common-automation-scripts/#how-to-customize-cm-scripts-using-variations","title":"How to customize CM scripts using variations?","text":"<p>Sometimes we need to set multiple environment variables or run a set of extra CM scripts for a specific purpose (different hardware target or model or dataset).</p> <p>We introduced special tags with <code>_</code>, called variations or variation tags,  that allow you to update a set of environment variables and add extra scripts to the chain of dependencies.</p> <p>Such variations are defined using <code>variations</code> dictionary  in the meta description of a given CM script.</p> <p>For example, our script has 2 variations <code>_cuda</code> and <code>_cpu</code>.</p> <p>If you want to use CUDA implementation of the image classification example,  you can add this variation to the tags that will set <code>USE_CUDA</code> environment to <code>yes</code> and will turn on a specific CM script in <code>deps</code> to install ONNX for CUDA:</p> <pre><code>cmr \"python app image-classification onnx _cuda\" --input=computer_mouse.jpg\n</code></pre>"},{"location":"cmx/common-automation-scripts/#how-to-cache-and-reuse-cm-scripts-output","title":"How to cache and reuse CM scripts' output?","text":"<p>By default, CM scripts run in the current directory and record all new files there.</p> <p>For example, the following universal download script will download  computer mouse image to the current directory:</p> <p><sup> <pre><code>cm run script \"download file _wget\" --url=https://cKnowledge.org/ai/data/computer_mouse.jpg --verify=no --env.CM_DOWNLOAD_CHECKSUM=45ae5c940233892c2f860efdf0b66e7e\n</code></pre> <p></p> <p>In some cases, we want to cache and reuse the output of automation recipes (such as downloading models, preprocessing data sets or building some applications) rather than just downloading it to the current directory.</p> <p>Following the feedback from our users, we implemented a <code>cache</code> automation in CM similar to <code>script</code>. Whenever CM encounters <code>\"cache\":true</code> in a meta description of a given script, it will create a <code>cache</code> directory in <code>$HOME/CM/repos/local</code> with some unique ID and the same tags as <code>script</code>, and will execute that script there to record all the data in cache. </p> <p>Whenever the same CM script is executed and CM finds an associated cache entry,  it will skip execution and will reuse files from that entry.</p> <p>Furthermore, it is possible to reuse large cached files in other projects that call the same CM scripts!</p> <p>You can see cache entries and find a specific one as follows:</p> <pre><code>cmr \"get ml-model resnet50 _onnx\" -j\n\ncm show cache\ncm show cache \"get ml-model resnet50 _onnx\" \ncm find cache \"download file ml-model resnet50 _onnx\" \ncm info cache \"download file ml-model resnet50 _onnx\" \n</code></pre> <p>You can clean some cache entries as follows: <pre><code>cm rm cache --tags=ml-model,resnet50\n</code></pre></p> <p>You can also clean all CM <code>cache</code> entries and start from scratch as follows: <pre><code>cm rm cache -f\n</code></pre></p> <p>In fact, you can remove <code>$HOME/CM</code> to reset CM framework completely and remove all downloaded repositories and cached entries.</p>"},{"location":"cmx/common-automation-scripts/#how-to-use-cm-with-python-virtual-environments","title":"How to use CM with Python virtual environments?","text":"<p>Using CM <code>cache</code> makes it possible to run CM automations for multiple virtual environments installed inside CM <code>cache</code> entries. It is possible to run CM automations with different Python virtual environments transparently to users while avoiding messing up native user environment.</p> <p>We created the following CM automation recipe to create virtual environments:</p> <pre><code>cmr \"install python-venv\" --name=mlperf\ncm show cache \"python-venv name-mlperf\"\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre> <p>If you now run our image classification automation recipe,  it will reuse model and dataset from the cache, but will use the newly created virtual environment <code>mlperf</code> for running the script.</p>"},{"location":"cmx/common-automation-scripts/#how-to-debug-cm-scripts","title":"How to debug CM scripts?","text":"<p>One of the requirements from CM users was to avoid new and/or complex ways to debug CM automations. Using native scripts and Python code makes it possible to apply standard techniques and tools to debug CM automations.</p> <p>We were also asked to add <code>--debug</code> flag to open a shell after the last native script is executed -  this allows users to rerun the last command line with all environment variables and paths assembled by CM while having a full and native access to change environment and run the final command  (such as pinning threads, changing batch sizes, modifying files, etc).</p> <p>You can try it as follows on Linux, MacOS, Windows or other operating system as follows:</p> <pre><code>cmr \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg --debug\n</code></pre> <p>You can also use GDB via environment variable <code>--env.CM_RUN_PREFIX=\"gdb --args \"</code> to run the final command via GDB.</p>"},{"location":"cmx/common-automation-scripts/#how-to-extendimprove-cm-scripts","title":"How to extend/improve CM scripts?","text":"<p>CM is a community project where CM scripts  are continuously improved to run on different hardware with different software  while keeping backward compatibility through the unified CM interface, tags and variations.</p> <p>Whenever you encounter an issue or want to have support for your own project and environment,  please update these scripts and send a PR to the CM GitHub.</p> <p>You can also reach us via public Discord server  if you questions or suggestions.</p>"},{"location":"cmx/common-automation-scripts/#how-to-use-cm-with-containers","title":"How to use CM with containers?","text":"<p>One of the key requirements for CM was to run automation natively or inside containers in the same way.</p> <p>We want CM scripts to adapt to the current/latest environment natively or run in the container automatically generated on the fly when requested by user for more stability and determinism.</p> <p>In such case, we can get rid of separate development of native scripts/workflows and Dockerfile  and use the same CM commands instead.</p> <p>To run a given script in an automatically-generated container, you can simply substitute <code>cm run script</code>  with <code>cm docker script</code> or <code>cmr</code> with <code>cmrd</code>:</p> <pre><code>cm docker script \"python app image-classification onnx _cpu\"\n</code></pre> <p>CM will automatically generate a Dockerfile with Ubuntu 22.04 in the <code>dockerfiles</code>  directory of a given script, will build container with the same CM command and will run it inside container.</p> <ul> <li>If you want to stay in the container, you can add flag <code>--docker_it</code>.</li> <li>You can change OS inside container using <code>--docker_base_image</code>, <code>--docker_os</code> and <code>--docker_os_version</code>.</li> </ul> <p>The tricky part is when we want to use host files and directories with a given CM script inside container.  To make it easier for users, we have implemented automatic detection and mounting of files and directories  in CM script.</p> <p>Developers of a CM script just need to specify which flags and environment variables are local files or directories using <code>input_paths</code> in <code>docker</code> dictionary of the meta-description of this script:</p> <pre><code>docker:\n  skip_run_cmd: 'no'\n  all_gpus: 'yes'\n  input_paths:\n    - input\n    - env.CM_IMAGE\n    - output\n  skip_input_for_fake_run:\n    - input\n    - env.CM_IMAGE\n    - output\n    - j\n  pre_run_cmds:\n    - echo \\\"CM pre run commands\\\"\n</code></pre> <p>When you run the same script via container with the local computer_mouse.jpg file as an input, CM will automatically mount current directory and will update the input to the CM script inside container with the internal path:</p> <p><sup> <pre><code>cm docker script \"python app image-classification onnx _cpu\" --input=computer_mouse.jpg\n\n...\n\ndocker build  -f D:\\Work1\\CM\\ck\\cm-mlops\\script\\app-image-classification-onnx-py\\dockerfiles\\ubuntu_22.04.Dockerfile \\\n              -t cknowledge/cm-script-app-image-classification-onnx-py:ubuntu-22.04-latest .\n\n...\n\nContainer launch command:\ndocker run  --entrypoint \"\"  --gpus=all -v D:\\Work1\\CM\\ck\\docs\\computer_mouse.jpg:/cm-mount/Work1/CM/ck/docs/computer_mouse.jpg \n                            cknowledge/cm-script-app-image-classification-onnx-py:ubuntu-22.04-latest \n                            bash -c \"echo \\\"CM pre run commands\\\" &amp;&amp; \n                            cm run script --tags=python,app,image-classification,onnx,_cpu \n                            --input=/cm-mount/Work1/CM/ck/docs/computer_mouse.jpg \"\n\nCM pre run commands\n</code></pre> <p></p> <p>It is now possible to download large data sets and models to the host from CM containers or pass host scratch pads and data to CM containers transparently to a user!</p>"},{"location":"cmx/common-automation-scripts/#how-to-use-cm-gui-to-run-automation-recipes","title":"How to use CM GUI to run automation recipes?","text":"<p>Another request from CM/MLCommons users was to have a simple GUI that can generate CM commands with user-friendly selector.</p> <p>We've implemented a CM script called <code>gui</code>  that provides a universal Streamlit GUI for any CM script. </p> <p>You just need to describe the inputs for a given script via meta-description as shown for our image classification example:</p> <pre><code>input_description:\n  input: \n    desc: \"Path to JPEG image to classify\"\n  output: \n    desc: \"Output directory (optional)\"\n  j:\n    desc: \"Print JSON output\"\n    boolean: true\n</code></pre> <p>You can run this GUI for your CM script as follows: <pre><code>cm gui script \"python app image-classification onnx _cpu\"\n</code></pre></p> <p>This GUI will allow you to customize your script and run it on your host.</p>"},{"location":"cmx/common-automation-scripts/#how-to-run-mlperf-benchmarks-via-cm","title":"How to run MLPerf benchmarks via CM?","text":"<p>CM was originally designed to make it easier to run MLPerf inference benchmarks.</p> <p>While MLPerf inference has a common benchmarking engine called loadgen, setting up a given platform, installing all tools, downloading and preprocessing all models and data sets,  updating paths and environment variables, figuring out default parameters for various scenarios, preparing a loadgen command line, keeping track of continuous updates in MLPerf rules, running multiple experiments and submitting results is a major challenge for old and new submitters (see MLPerf inference v4.0 submitter orientation for automation.</p> <p>We created several CM scripts to prepare and run different implementations of MLPerf inference (reference, Nvidia, Intel, Qualcomm, Deep Sparse, etc) with a master CM script to run them all out-of-the-box natively or inside automatically-generated containers  run-mlperf-inference-app. CM helped us to implement it as a simple pipeline with a common and human-friendly interface while reusing all existing automation recipes.</p> <p>This script was successfully validated to modularize MLPerf inference benchmarks  and help the community automate more than 95% of all performance and power submissions in the v3.1 round across more than 120 system configurations (models, frameworks, hardware)  while reducing development and maintenance costs.</p> <p>Please check this documentation for more details.</p>"},{"location":"cmx/common-automation-scripts/#how-to-use-cm-to-reproduce-research-papers","title":"How to use CM to reproduce research papers?","text":"<p>Following the successful validation of CM concept to modularize and run MLPerf inference benchmarks across diverse software and hardware, the community test it to make it easier to reproduce results from research papers during artifact evaluation and other reproducibility initiatives at systems conferences.</p> <p>The idea is to provide a common interface to prepare and run experiments from research papers. See the latest CM scripts to rerun some experiments from the ACM/IEEE MICRO'23 conference and from the Student Cluster Competition at Supercomputing'23.</p>"},{"location":"cmx/common-automation-scripts/#how-to-use-cm-as-a-common-interface-to-other-projects","title":"How to use CM as a common interface to other projects?","text":"<p>While CM was successfully validated to unify, modularize and automate MLPerf benchmarks, it turned out to be applicable to any software project. </p> <p>The community started using CM as a common and human-friendly interface to run other software projects  and manage experiments across diverse models, data sets, software and hardware while making them more modular,  portable and reusable.</p> <p>Please check other CM tutorials, CM documentation and our ACM REP'23 keynote for more details.</p>"},{"location":"cmx/common-automation-scripts/#where-to-read-about-the-cm-vision-and-history","title":"Where to read about the CM vision and history?","text":"<ul> <li>ACM REP'23 keynote about MLCommons CM: slides YouTube</li> <li>ACM TechTalk'21 about automating research projects: YouTube slides</li> <li>Project history</li> </ul>"},{"location":"cmx/common-automation-scripts/#how-to-get-in-touch-with-the-cm-community","title":"How to get in touch with the CM community?","text":"<p>This is a community project being developed by the MLCommons Task Force on Automation and Reproducibility based on your feedback and contributions - please join our public Discord server if you  would like to help with developments or have questions, suggestions and feature requests.</p>"},{"location":"cmx/improving-cmx/","title":"Improving CMX framework","text":"<p>[ Back to documentation ]</p>"},{"location":"cmx/improving-cmx/#improving-cmx","title":"Improving CMX","text":"<ul> <li>CMX GitHub repository (Python cmind package)</li> <li>CMX GitHub internal repo</li> <li>CMX internal architecture - diagram</li> <li>CMX internal classes and data structures</li> </ul>"},{"location":"cmx/install/","title":"Installation","text":"<p>[ Back to index ]</p>"},{"location":"cmx/install/#cmx-installation","title":"CMX installation","text":"<p>This guide will help you install CMX (Collective Mind eXtension aka Common Metadata eXchange).</p> <p>Check online CMX installation GUI.</p> <p>CMX framework requires minimal dependencies to run on any platform: <code>python 3.7+, pip, venv, git, git-lfs, wget, curl</code>.</p> <p>By default, CMX will pull Git repositories and cache installations and downloaded files in your <code>$HOME/CM</code> directory on Linux and MacOS or <code>%userprofile%\\CM</code> directory on Windows. You can change it to any another directory using the <code>CM_REPOS</code> environment variable, for example <code>export CM_REPOS=/scratch/CM</code>.</p> <p>Feel free to use the online installation GUI.</p>"},{"location":"cmx/install/#ubuntu-debian","title":"Ubuntu, Debian","text":"<p>We have successfully tested CMX with the following system dependencies on Ubuntu 20.x, 22.x , 24.x:</p> <pre><code>sudo apt update\nsudo apt install python3 python3-pip python3-venv git git-lfs wget curl\n\npython3 -m venv cm\nsource cm/bin/activate\n\npip install cmind\n</code></pre> <p>Note that you may need to restart your shell to update PATH to the \"cmx\" binary. </p> <p>Alternatively you can run </p> <pre><code>source $HOME/.profile\n</code></pre> <p>You can now check that all system dependencies are installed using the following command: <pre><code>cmx init\n</code></pre> You can also check that CMX core works using the following command: <pre><code>cmx test core\n</code></pre></p>"},{"location":"cmx/install/#red-hat","title":"Red Hat","text":"<p>We have successfully tested CM on Red Hat 9 and CentOS 8</p> <pre><code>sudo dnf update\n\nsudo dnf install python3 python-pip git git-lfs wget curl\n\npython3 -m pip install cmind --user\n</code></pre>"},{"location":"cmx/install/#macos","title":"MacOS","text":"<p>Note that CM currently does not work with Python installed from the Apple Store.  Please install Python via brew as described below.</p> <p>If <code>brew</code> package manager is not installed, please install it as follows (see details here): <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre></p> <p>Don't forget to add brew to PATH environment as described in the end.</p> <p>Then install python, pip, git and wget:</p> <pre><code>brew install python3 git git-lfs wget curl\n\npython3 -m pip install cmind\n</code></pre> <p>Sometimes python does not add <code>cm</code> and <code>cmr</code> binaries to the <code>PATH</code> environment variable.  You may need to find these files and add their path to <code>PATH</code> variable.  We plan to simplify this installation in the future.</p>"},{"location":"cmx/install/#windows","title":"Windows","text":"<ul> <li>Configure Windows 10+ to support long paths from command line as admin:    <pre><code>reg add \"HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\" /v LongPathsEnabled /t REG_DWORD /d 1 /f\n</code></pre> </li> <li>Download and install Git from git-for-windows.github.io.</li> <li>Configure Git to accept long file names: <code>git config --system core.longpaths true</code></li> <li>Download and install Python 3+ from www.python.org/downloads/windows.</li> <li>Don't forget to select option to add Python binaries to PATH environment!</li> <li> <p>Configure Windows to accept long fie names during Python installation!</p> </li> <li> <p>Install CM via PIP:</p> </li> </ul> <pre><code>python -m pip install cmind\n</code></pre> <p>Note that we have reports   that CM does not work when Python was first installed from the Microsoft Store.  If CM fails to run, you can find a fix here.</p> <p>We plan to provide a self-sustained package in the future to simplify CM installation on Windows.</p>"},{"location":"cmx/install/#cmx-cli","title":"CMX CLI","text":"<p>If the installation is successful, you can run the CM CLI as follows:</p> <pre><code>gfursin@cmind:~$ cmx\n\ncmx {action} {automation} {artifact(s)} {CMX control flags (-)} {CMX automation flags (--)}\n</code></pre> <pre><code>gfursin@cmind:~$ cmx test core\n</code></pre>"},{"location":"cmx/motivation/","title":"Motivation","text":"<p>[ Back to documentation ]</p>"},{"location":"cmx/motivation/#ckcmcmx-motivation","title":"CK/CM/CMX motivation","text":"<p>To learn more about the concepts and motivation behind this project, please explore the following articles and presentations:</p> <ul> <li>HPCA'25 article \"MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from Microwatts to Megawatts for Sustainable AI\": [ Arxiv ], [ tutorial to reproduce results using CM/CMX ]</li> <li>\"Enabling more efficient and cost-effective AI/ML systems with Collective Mind, virtualized MLOps, MLPerf, Collective Knowledge Playground and reproducible optimization tournaments\": [ ArXiv ]</li> <li>ACM REP'23 keynote about the MLCommons CM automation framework: [ slides ] </li> <li>ACM TechTalk'21 about Collective Knowledge project: [ YouTube ] [ slides ]</li> <li>Journal of Royal Society'20: [ paper ]</li> </ul> <p>You are welcome to contact the author to discuss long-term plans and potential collaboration.</p>"},{"location":"cmx/understanding-cmx/","title":"Understanding CMX","text":"<p>[ Back to documentation ]</p>"},{"location":"cmx/understanding-cmx/#getting-started-with-cmx","title":"Getting Started with CMX","text":""},{"location":"cmx/understanding-cmx/#understanding-common-metadata-exchange-cmx","title":"Understanding Common Metadata eXchange (CMX)","text":"<p>CMX allows users to embed any metadata into any artifact\u2014such as programs, datasets, models, and scripts\u2014within their projects using <code>_cm.yaml</code> and/or <code>_cm.json</code> files (Common Metadata).</p> <p>This common metadata includes extensible tags, a user-friendly alias, and an automatically generated unique ID (16 lowercase hexadecimal characters),  enabling everyone to find and reuse both public and private artifacts in accordance with FAIR principles  (findable, accessible, interoperable, and reusable).</p> <p>CMX also allows users to add, share and reuse common automations  with a unified CLI and Python API, applying them to related artifacts based on their common metadata.</p>"},{"location":"cmx/understanding-cmx/#understanding-cmx-repositories","title":"Understanding CMX repositories","text":"<p>A typical CMX-based GitHub repository, which includes common metadata and automations in the CMX format, is structured as follows:</p> <pre><code>\u2502   cmr.yaml\n\u2502   \n\u251c\u2500\u2500\u2500automation\n\u2502   \u2514\u2500\u2500\u2500script\n\u2502           COPYRIGHT.md\n\u2502           module.py\n\u2502           README.md\n\u2502           _cm.json\n\u2502               \n\u2502           \n\u2514\u2500\u2500\u2500script\n    \u251c\u2500\u2500\u2500app-image-classification-torch-py\n    \u2502   \u2502   COPYRIGHT.md\n    \u2502   \u2502   README.md\n    \u2502   \u2502   requirements.txt\n    \u2502   \u2502   run.bat\n    \u2502   \u2502   run.sh\n    \u2502   \u2502   _cm.yaml\n    \u2502   \u2502   \n    \u2502   \u2514\u2500\u2500\u2500src\n    \u2502           pytorch_classify_preprocessed.py\n    \u2502           \n    \u251c\u2500\u2500\u2500app-mlperf-inference\n    \u2502       COPYRIGHT.md\n    \u2502       customize.py\n    \u2502       README.md\n    \u2502       run.sh\n    \u2502       _cm.yaml\n    \u2502       \n    \u251c\u2500\u2500\u2500compile-program\n    \u2502       COPYRIGHT.md\n    \u2502       customize.py\n    \u2502       run.bat\n    \u2502       run.sh\n    \u2502       _cm.yaml\n    \u2502       \n    \u2514\u2500\u2500\u2500detect-os\n            COPYRIGHT.md\n            customize.py\n            _cm.yaml\n</code></pre> <p>All CMX repositories follow a file-based structure with a two-level directory hierarchy.  Each repository includes a <code>cmr.yaml</code> file (Common Metadata Repository),  which contains a unique ID and a user-friendly alias for easy identification.</p> <p>All CMX repositories are structured as a file-based system with a two-level directory hierarchy and a <code>cmr.yaml</code> (Common Metadata Repository) with a unique ID and user-friendly alias for this repository . </p> <p>The first-level directories categorize artifacts and include their relevant automation, while the second-level directories house the specific artifacts within each category.</p> <p>In the above example, we have common CMX artifacts called <code>script</code>, along with a related CMX <code>automation</code> of the same name.</p> <p>Each subdirectory within the <code>script</code> directory always contains either  a <code>_cm.yaml</code> file (typically manually generated), a <code>_cm.json</code> file  (usually automatically generated), or both.  If both are present, CMX first reads <code>_cm.yaml</code> and then merges it with <code>_cm.json</code>.  These files describe a given artifact and include all related user files associated with it.</p> <p>For CMX scripts, we typically have native scripts and an optional <code>customize.py</code> file.  The <code>customize.py</code> enables the unification of environment variables and APIs before executing  a given script via CMX automation, ensuring it runs consistently across different operating  systems through a standardized CMX interface.</p> <p>After installing CMX and cloning this repository from GitHub, you can find all shared <code>script</code> artifacts  as follows:</p> <pre><code>fursin@laptop:~$ pip install cmind\nfursin@laptop:~$ cmx pull repo mlcommons@ck --dir=cm4mlops/cx4mlops\nfursin@laptop:~$ cmx show repo\nfursin@laptop:~$ cmx find script\n\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/script/app-image-classification-torch-py\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/script/app-mlperf-inference\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/script/compile-program\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/script/detect-os\n...\n</code></pre> <p>You can also find all categories (automations) for shared artifacts as follows: <pre><code>fursin@laptop:~$ cmx find automation\n\n/home/fursin/cmx/lib/python3.12/site-packages/cmind/repo/automation/automation\n/home/fursin/cmx/lib/python3.12/site-packages/cmind/repo/automation/ckx\n/home/fursin/cmx/lib/python3.12/site-packages/cmind/repo/automation/core\n/home/fursin/cmx/lib/python3.12/site-packages/cmind/repo/automation/repo\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/automation/cache\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/automation/cfg\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/automation/challenge\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/automation/data\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/automation/docker\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/automation/experiment\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/automation/report\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/automation/script\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/automation/utils\n...\n</code></pre></p> <p>By default, when you pull repositories via CMX, they are stored in the <code>$HOME/CM</code> directory.  This default location helps CMX efficiently search for all shared artifacts and automations.</p> <p>You can change this directory by setting the <code>CM_REPOS</code> environment variable.</p> <p>Additionally, you can pull multiple public and private repositories with CMX artifacts,  allowing you to reuse artifacts and automations across different projects.</p>"},{"location":"cmx/understanding-cmx/#understanding-cmx-metadata","title":"Understanding CMX metadata","text":"<p>All CMX artifacts contain a minimal set of keys in their metadata files.</p> <p>For example, the CMX script artifact <code>app-image-classification-torch-py</code>,  which includes PyTorch code for classifying images using the reference ResNet50 model,  has the following <code>_cm.yaml</code> format:</p> <pre><code>uid: e3986ae887b84ca8\nalias: app-image-classification-torch-py\n\nautomation_alias: script\nautomation_uid: 5b4e0237da074764\n\ntags:\n- app\n- image-classification\n- torch\n- python\n\n...\n</code></pre> <p>This metadata allows users to find this artifact from command line using the following CMX commands: <pre><code>$ cmx find script app-image-classification-torch-py\n$ cmx find script e3986ae887b84ca8\n$ cmx find script app-image-classification-torch-py,e3986ae887b84ca8\n$ cmx find script *image-classification-torch*\n$ cmx find script --tags=app,image-classification,torch\n$ cmx find script \"python app image-classification torch\"\n\n/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/script/app-image-classification-torch-py\n</code></pre></p> <p>You can also use a simple Python API to locate these artifacts as follows:</p> <pre><code>$ python\n\nimport cmind\n\nr = cmind.x({'action':'find', 'automation':'script', 'artifact':'app-image-classification-torch-py,e3986ae887b84ca8'})\nif r['return']&gt;0: cmind.errorx(r)\n\nartifacts = r['list']\n\nfor artifact in artifacts:\n     print (artifact.path)\n     print (artifact.meta)\n</code></pre> <pre><code>/home/fursin/CM/repos/mlcommons@ck/cm4mlops/cm4mlops/repo/script/app-image-classification-torch-py\n{'alias': 'app-image-classification-torch-py', 'automation_alias': 'script', 'automation_uid': '5b4e0237da074764', ...\n</code></pre>"},{"location":"cmx/understanding-cmx/#understanding-cmx-automations","title":"Understanding CMX automations","text":"<p>All CMX artifacts must be associated with a corresponding CMX automation  that defines common actions based on the artifact's metadata.</p> <p>When one creates a new type of artifact, CMX will automatically generate a related automation  that inherits common actions  to manage all types of artifacts, including  <code>find/search</code>, <code>add</code>, <code>delete/rm</code>, <code>move/mv</code>, <code>copy/cp</code>, <code>load</code>, and <code>update</code>.</p> <p>For example, the metadata of the CMX script artifact <code>app-image-classification-torch-py</code> specifies its associated CMX automation:</p> <pre><code>automation_alias: script\nautomation_uid: 5b4e0237da074764\n</code></pre> <p>This allows CMX to find a related automation and determine which common actions  can be applied to this artifact based on its metadata.</p> <p>If we run the command <code>cmx load script app-image-classification-torch-py</code>, CMX will: * search for existing <code>script</code> automation in all <code>automation</code> directories across all CMX repositories. * Locate it in <code>automation/script</code>. * Load <code>modulex.py</code> (for CMX) or <code>module.py</code> (for CM) * Invoke the  <code>load</code> function in the CAutomation class (which inherits common actions    for all artifacts from the CMX Automation class). * Pass a unified CM/CMX input dictionary <code>{'artifact':'app-image-classification-torch-py' ...}</code></p> <p>The common <code>load</code> function  will, in turn: * Search for this artifact in all CMX repositories * Load tha artifact's metadata from <code>_cm.yaml</code> and/or <code>_cm.json</code> * Print the metadata to the console as JSON</p> <p>If the same command is invoked through the CMX Python API, it will return metadata as a unified CMX dictionary:</p> <pre><code>$ python\n\nimport cmind\nr = cmind.x({'action':'load', 'automation':'script', 'artifact':'app-image-classification-torch-py'})\n\nprint (r)\n\n{'return': 0, 'path': '...', 'meta': {'alias': 'app-image-classification-torch-py', 'automation_alias': 'script', \n 'automation_uid': '5b4e0237da074764', 'category': 'Modular AI/ML application pipeline', ...\n</code></pre> <p>We can also implement automation actions tailored specifically for this group of artifacts. For example, we have implemented the 'run' action for scripts, enabling the execution  of both native and Python scripts on any platform, regardless of the operating system,  in a unified, portable, and deterministic manner:</p> <p>If we now run the command <code>cmx run script app-image-classification-torch-py</code>, CMX will invoke the <code>run</code>  function within the <code>script</code> automation and execute it based on the metadata of the <code>app-image-classification-torch-py</code> artifact.</p> <p>This covers the core CM/CMX concepts, which facilitate collaborative and reproducible research, development, and experimentation. They help users progressively modularize, unify, and extend complex projects using common and interconnected metadata and reusable automation. For example, the community has successfully applied this approach to modularize and automate MLPerf benchmarks.</p>"},{"location":"cmx/mlperf-inference/","title":"MLPerf automations","text":"<p>Collective Mind automation framework (CM and CMX)  have helped the community  learn how to modularize and automate MLPerf inference benchmarks through unified MLOps and MLPerf automations with extensible metadata.</p> <p>Here, we maintain an archive of various unified CM and CMX commands for running different versions of MLPerf benchmarks.</p> <p>You can also check the CM and CMX commands  to run experiments from the HPCA'25 paper \"MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from Microwatts to Megawatts for Sustainable AI\",  NeuralMagic's MLPerf inference 4.1 submission and cTuning's MLPerf inference 4.1 submission.</p>"},{"location":"cmx/mlperf-inference/LICENSE/","title":"LICENSE","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"cmx/mlperf-inference/v4.1/","title":"MLPerf inference benchmark v4.1","text":"<p>This documentation is adapted to the MLCommons CMX framework, the next generation of MLCommons CM (drop-in replacement for CM).</p>"},{"location":"cmx/mlperf-inference/v4.1/LICENSE/","title":"LICENSE","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"cmx/mlperf-inference/v5.0/","title":"MLPerf Inference Benchmarks","text":""},{"location":"cmx/mlperf-inference/v5.0/#overview","title":"Overview","text":"<p>The currently valid MLPerf Inference Benchmarks as of MLPerf inference v5.0 round are listed below, categorized by tasks. Under each model you can find its details like the dataset used, reference accuracy, server latency constraints etc.</p>"},{"location":"cmx/mlperf-inference/v5.0/#image-classification","title":"Image Classification","text":""},{"location":"cmx/mlperf-inference/v5.0/#resnet50-v15","title":"ResNet50-v1.5","text":"<ul> <li>Dataset: Imagenet-2012 (224x224) Validation<ul> <li>Dataset Size: 50,000</li> <li>QSL Size: 1,024</li> </ul> </li> <li>Number of Parameters: 25.6 million</li> <li>FLOPs: 3.8 billion</li> <li>Reference Model Accuracy: 76.46% ACC</li> <li>Server Scenario Latency Constraint: 15ms</li> <li>Equal Issue mode: False</li> <li>High accuracy variant: No</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/#text-to-image","title":"Text to Image","text":""},{"location":"cmx/mlperf-inference/v5.0/#stable-diffusion","title":"Stable Diffusion","text":"<ul> <li>Dataset: Subset of Coco2014<ul> <li>Dataset Size: 5,000</li> <li>QSL Size: 5,000</li> </ul> </li> <li>Number of Parameters: 3.5 billion </li> <li>FLOPs: 1.28 - 2.4 trillion</li> <li>Reference Model Accuracy (fp32):  CLIP: 31.74981837, FID: 23.48046692</li> <li>Required Accuracy (Closed Division):<ul> <li>CLIP: 31.68631873 \u2264 CLIP \u2264 31.81331801 (within 0.2% of the reference model CLIP score)</li> <li>FID: 23.01085758 \u2264 FID \u2264 23.95007626 (within 2% of the reference model FID score)</li> </ul> </li> <li>Equal Issue mode: False</li> <li>High accuracy variant: No</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/#object-detection","title":"Object Detection","text":""},{"location":"cmx/mlperf-inference/v5.0/#retinanet","title":"Retinanet","text":"<ul> <li>Dataset: OpenImages<ul> <li>Dataset Size: 24,781</li> <li>QSL Size: 64</li> </ul> </li> <li>Number of Parameters: TBD</li> <li>Reference Model Accuracy (fp32) : 0.3755 mAP</li> <li>Server Scenario Latency Constraint: 100ms</li> <li>Equal Issue mode: False</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/#medical-image-segmentation","title":"Medical Image Segmentation","text":""},{"location":"cmx/mlperf-inference/v5.0/#3d-unet","title":"3d-unet","text":"<ul> <li>Dataset: KiTS2019<ul> <li>Dataset Size: 42</li> <li>QSL Size: 42</li> </ul> </li> <li>Number of Parameters: 32.5 million</li> <li>FLOPs: 100-300 billion</li> <li>Reference Model Accuracy (fp32) : 0.86330 Mean DICE Score</li> <li>Server Scenario: Not Applicable</li> <li>Equal Issue mode: True</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/#language-tasks","title":"Language Tasks","text":""},{"location":"cmx/mlperf-inference/v5.0/#question-answering","title":"Question Answering","text":""},{"location":"cmx/mlperf-inference/v5.0/#bert-large","title":"Bert-Large","text":"<ul> <li>Dataset: Squad v1.1 (384 Sequence Length)<ul> <li>Dataset Size: 10,833</li> <li>QSL Size: 10,833</li> </ul> </li> <li>Number of Parameters: 340 million </li> <li>FLOPs: ~128 billion</li> <li>Reference Model Accuracy (fp32) : F1 Score = 90.874%</li> <li>Server Scenario Latency Constraint: 130ms</li> <li>Equal Issue mode: False</li> <li>High accuracy variant: yes</li> <li>Submission Category: Edge</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/#llama2-70b","title":"LLAMA2-70B","text":"<ul> <li>Dataset: OpenORCA (GPT-4 split, max_seq_len=1024)<ul> <li>Dataset Size: 24,576</li> <li>QSL Size: 24,576</li> </ul> </li> <li>Number of Parameters: 70 billion</li> <li>FLOPs: ~500 trillion</li> <li>Reference Model Accuracy (fp32) :<ul> <li>Rouge1: 44.4312</li> <li>Rouge2: 22.0352</li> <li>RougeL: 28.6162</li> <li>Tokens_per_sample: 294.45</li> </ul> </li> <li>Server Scenario Latency Constraint:<ul> <li>TTFT: 2000ms</li> <li>TPOT: 200ms</li> </ul> </li> <li>Equal Issue mode: True</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/#text-summarization","title":"Text Summarization","text":""},{"location":"cmx/mlperf-inference/v5.0/#gpt-j","title":"GPT-J","text":"<ul> <li>Dataset: CNN Daily Mail v3.0.0<ul> <li>Dataset Size: 13,368</li> <li>QSL Size: 13,368</li> </ul> </li> <li>Number of Parameters: 6 billion</li> <li>FLOPs: ~148 billion</li> <li>Reference Model Accuracy (fp32) :<ul> <li>Rouge1: 42.9865</li> <li>Rouge2: 20.1235</li> <li>RougeL: 29.9881</li> <li>Gen_len: 4,016,878</li> </ul> </li> <li>Server Scenario Latency Constraint: 20s</li> <li>Equal Issue mode: True</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/#mixed-tasks-question-answering-math-and-code-generation","title":"Mixed Tasks (Question Answering, Math, and Code Generation)","text":""},{"location":"cmx/mlperf-inference/v5.0/#mixtral-8x7b","title":"Mixtral-8x7B","text":"<ul> <li>Datasets:<ul> <li>OpenORCA (5k samples of GPT-4 split, max_seq_len=2048)</li> <li>GSM8K (5k samples of the validation split, max_seq_len=2048)</li> <li>MBXP (5k samples of the validation split, max_seq_len=2048)</li> <li>Dataset Size: 15,000</li> <li>QSL Size: 15,000</li> </ul> </li> <li>Number of Parameters: 47 billion </li> <li>Reference Model Accuracy (fp16) :<ul> <li>OpenORCA<ul> <li>Rouge1: 45.4911</li> <li>Rouge2: 23.2829</li> <li>RougeL: 30.3615</li> </ul> </li> <li>GSM8K Accuracy: 73.78%</li> <li>MBXP Accuracy: 60.12%</li> </ul> </li> <li>Tokens_per_sample: 294.45</li> <li>Server Scenario Latency Constraint:<ul> <li>TTFT: 2000ms</li> <li>TPOT: 200ms</li> </ul> </li> <li>Equal Issue mode: True</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/#recommendation","title":"Recommendation","text":""},{"location":"cmx/mlperf-inference/v5.0/#dlrm_v2","title":"DLRM_v2","text":"<ul> <li>Dataset: Synthetic Multihot Criteo<ul> <li>Dataset Size: 204,800</li> <li>QSL Size: 204,800</li> </ul> </li> <li>Number of Parameters: ~23 billion</li> <li>Reference Model Accuracy: AUC = 80.31%</li> <li>Server Scenario Latency Constraint: 60ms</li> <li>Equal Issue mode: False</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/#graph-neural-networks","title":"Graph Neural Networks","text":""},{"location":"cmx/mlperf-inference/v5.0/#r-gat","title":"R-GAT","text":"<ul> <li>Dataset: Illinois Graph Benchmark Heterogeneous validation dataset<ul> <li>Dataset Size: 788,379</li> <li>QSL Size: 788,379</li> </ul> </li> <li>Number of Parameters: </li> <li>Reference Model Accuracy: ACC = 72.86%</li> <li>Server Scenario Latency Constraint: N/A</li> <li>Equal Issue mode: True</li> <li>High accuracy variant: No</li> <li>Submission Category: Datacenter</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/#automotive","title":"Automotive","text":""},{"location":"cmx/mlperf-inference/v5.0/#3d-object-detection","title":"3D Object Detection","text":""},{"location":"cmx/mlperf-inference/v5.0/#pointpainting","title":"PointPainting","text":"<ul> <li>Dataset: Waymo<ul> <li>Dataset Size: 39,986</li> <li>QSL Size: 1,024</li> </ul> </li> <li>Number of Parameters: 44 million</li> <li>FLOPs: 3 trillion</li> <li>Reference Model Accuracy (fp32):  mAP: 54.25%</li> <li>Required Accuracy (Closed Division):<ul> <li>mAP: 54.25%</li> </ul> </li> <li>Equal Issue mode: False</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Edge</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/#submission-categories","title":"Submission Categories","text":"<ul> <li>Datacenter Category: All benchmarks except bert are applicable to the datacenter category for inference v5.0.</li> <li>Edge Category: All benchmarks except DLRMv2, LLAMA2-70B, Mixtral-8x7B and R-GAT are applicable to the edge category for v5.0.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/#high-accuracy-variants","title":"High Accuracy Variants","text":"<ul> <li>Benchmarks: <code>bert</code>, <code>llama2-70b</code>, <code>gpt-j</code>,  <code>dlrm_v2</code>, and <code>3d-unet</code> have a normal accuracy variant as well as a high accuracy variant.</li> <li>Requirement: Must achieve at least 99.9% of the reference model accuracy, compared to the default 99% accuracy requirement.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/LICENSE/","title":"LICENSE","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/get-pointpainting-data/","title":"3-D Object Detection using PointPainting","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/get-pointpainting-data/#dataset","title":"Dataset","text":"<p>Note: By default, the waymo dataset is downloaded from the mlcommons official drive. One has to accept the MLCommons Waymo Open Dataset EULA to access the dataset files. </p> <p>The benchmark implementation run command will automatically download the preprocessed dataset. In case you want to download only the datasets, you can use the below command.</p> <pre><code>pip install cmx4mlperf\ncr get,dataset,waymo -j\n</code></pre> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_WAYMO_DATASET&gt;</code> could be provided to download the dataset to a specific location.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/get-pointpainting-data/#model","title":"Model","text":"<p>Note: By default, the PointPainting is downloaded from the mlcommons official drive. One has to accept the MLCommons Waymo Open Dataset EULA to access the model files. </p> <p>The benchmark implementation run command will automatically download the model. In case you want to download only the PointPainting model, you can use the below command.</p> <pre><code>pip install cmx4mlperf\ncr get,ml-model,pointpainting -j\n</code></pre> <ul> <li><code>--outdirname=&lt;PATH_TO_DOWNLOAD_POINTPAINTING_MODEL&gt;</code> could be provided to download the model files to a specific location.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/","title":"3D Object Detection using PointPainting","text":"MLCommons-Python"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>POINTPAINTING</p> edge"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#edge-category","title":"Edge category","text":"<p>In the edge category, pointpainting has SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: To be updated</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10 \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt;\n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#singlestream","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt;\n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10 \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt;\n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#singlestream_1","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet  \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt;\n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: To be updated</p> </li> <li> <p>Disk Space: To be updated</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt;\n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the SingleStream scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#singlestream_2","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt;\n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>The maximum duration for a performance run can be disabled by using <code>--env.MLC_MLPERF_USE_MAX_DURATION=no</code>.</p> </li> <li> <p>In valid execution mode, the query count for performance mode can be adjusted using <code>--env.MLC_MLPERF_LOADGEN_QUERY_COUNT=&lt;query_count&gt;</code>.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt;\n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> SingleStream <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for pointpainting you can follow this README.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/#singlestream_3","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=pointpainting \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --pointpainting_checkpoint_path=&lt;PATH_TO_POINTPAINTING_MODEL&gt; \\\n   --deeplab_resnet50_path=&lt;PATH_TO_SEGMENTOR MODEL&gt; \\\n   --waymo_path=&lt;PATH_TO_WAYMO_DATASET_FOLDER&gt;\n</code></pre> <p>Tip</p> <ul> <li>The <code>pointpainting_checkpoint_path</code>, <code>deeplab_resnet50_path</code> and <code>waymo_path</code> do not need to be provided inside the Docker container as they are already registered in the MLC cache.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/get-rgat-data/","title":"Graph Neural Network using R-GAT","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/get-rgat-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Full DatasetDebug Dataset <p>R-GAT validation run uses the IGBH dataset consisting of 547,306,935 nodes and 5,812,005,639 edges.</p> <p>R-GAT debug run uses the IGBH debug dataset(tiny).</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/get-rgat-data/#get-full-dataset","title":"Get Full Dataset","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,igbh,_full -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/get-rgat-data/#get-full-dataset_1","title":"Get Full Dataset","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,igbh,_debug -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/get-rgat-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf R-GAT Model</p> PyTorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/get-rgat-data/#pytorch","title":"PyTorch","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,rgat -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/","title":"Graph Neural Network using R-GAT","text":"MLCommons-Python"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>RGAT</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#edge-category","title":"Edge category","text":"<p>In the edge category, rgat has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#singlestream","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#all-scenarios","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#singlestream_1","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#singlestream_2","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#singlestream_3","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, rgat has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#offline_4","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#server","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#offline_5","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#server_1","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#offline_6","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#server_2","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--env.MLC_DATASET_IGBH_PATH=&lt;Path to IGBH dataset&gt;</code> if you have already downloaded the dataset. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--env.MLC_ML_MODEL_RGAT_CHECKPOINT_PATH=&lt;Path to R-GAT model checkpoint&gt;</code> if you have already downloaded the model. The path will be automatically mounted when using docker run.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for rgat you can follow this README.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#offline_7","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#server_3","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=rgat \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/get-resnet50-data/","title":"Image Classification using ResNet50","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/get-resnet50-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <p>ResNet50 validation run uses the Imagenet 2012 validation dataset consisting of 50,000 images.</p> <p>ResNet50 calibration dataset consist of 500 images selected from the Imagenet 2012 validation dataset. There are 2 alternative options for the calibration dataset.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/get-resnet50-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,imagenet,validation -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/get-resnet50-data/#get-calibration-dataset-using-option-1","title":"Get Calibration Dataset Using Option 1","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,imagenet,calibration,_mlperf.option1 -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/get-resnet50-data/#get-calibration-dataset-using-option-2","title":"Get Calibration Dataset Using Option 2","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,imagenet,calibration,_mlperf.option2 -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/get-resnet50-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf ResNet50 Model</p> TensorflowOnnx"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/get-resnet50-data/#tensorflow","title":"Tensorflow","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,resnet50,_tensorflow -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/get-resnet50-data/#onnx","title":"Onnx","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,resnet50,_onnx -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/","title":"Image Classification using Mobilenet models","text":"<p>Install MLC following the installation page.</p> <p>Mobilenet models are not official MLPerf models and so cannot be used for a Closed division MLPerf inference submission. But since they can be run with Imagenet dataset, we are allowed to use them for Open division submission. Only CPU runs are supported now. </p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/#tflite-backend","title":"TFLite Backend","text":"Mobilenet-V1Mobilenet-V2Mobilenet-V2MobilenetsEfficientnet"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/#mobilenet-v1","title":"Mobilenet V1","text":"<pre><code>pip install cmx4mlperf\ncr run,mobilenet-models,_tflite,_mobilenet-v1 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/#mobilenet-v2","title":"Mobilenet V2","text":"<pre><code>pip install cmx4mlperf\ncr run,mobilenet-models,_tflite,_mobilenet-v2 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/#mobilenet-v2_1","title":"Mobilenet V2","text":"<pre><code>pip install cmx4mlperf\ncr run,mobilenet-models,_tflite,_mobilenet-v2 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/#mobilenet-v1v2v3","title":"Mobilenet V1,V2,V3","text":"<pre><code>pip install cmx4mlperf\ncr run,mobilenet-models,_tflite,_mobilenet \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/#efficientnet","title":"Efficientnet","text":"<pre><code>pip install cmx4mlperf\ncr run,mobilenet-models,_tflite,_efficientnet \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/#armnn-backend","title":"ARMNN Backend","text":"Mobilenet-V1Mobilenet-V2Mobilenet-V2MobilenetsEfficientnet"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/#mobilenet-v1_1","title":"Mobilenet V1","text":"<pre><code>pip install cmx4mlperf\ncr run,mobilenet-models,_tflite,_armnn,_mobilenet-v1 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/#mobilenet-v2_2","title":"Mobilenet V2","text":"<pre><code>pip install cmx4mlperf\ncr run,mobilenet-models,_tflite,_armnn,_mobilenet-v2 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/#mobilenet-v2_3","title":"Mobilenet V2","text":"<pre><code>pip install cmx4mlperf\ncr run,mobilenet-models,_tflite,_armnn,_mobilenet-v2 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/#mobilenet-v1v2v3_1","title":"Mobilenet V1,V2,V3","text":"<pre><code>pip install cmx4mlperf\ncr run,mobilenet-models,_tflite,_armnn,_mobilenet \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/mobilenets/#efficientnet_1","title":"Efficientnet","text":"<pre><code>pip install cmx4mlperf\ncr run,mobilenet-models,_tflite,_armnn,_efficientnet \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/","title":"Image Classification using ResNet50","text":"MLCommons-PythonNvidiaIntelQualcommMLCommons-C++"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>RESNET50</p> edge"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#edge-category","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimeTensorflowDeepsparse"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_1","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_1","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_2","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_2","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_3","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_3","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_4","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_4","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_4","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#tensorflow-framework","title":"Tensorflow framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_5","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_5","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_5","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_6","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_6","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_6","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_7","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_7","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_7","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_8","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_8","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_8","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_9","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_9","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_9","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#deepsparse-framework","title":"Deepsparse framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub                === \"Native\"</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_10","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_10","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_10","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_6","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub    === \"datacenter\"</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_11","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_11","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_11","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimeTensorflowDeepsparse"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#onnxruntime-framework_1","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_12","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_7","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_13","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_1","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_14","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_2","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_8","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_8","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_15","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_3","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_9","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_9","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_16","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_4","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#tensorflow-framework_1","title":"Tensorflow framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cpu-device_4","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_17","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_5","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_10","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_10","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_18","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_6","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_8","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_19","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_7","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_11","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_11","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_20","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_8","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_12","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_12","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_12","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_21","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_9","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#deepsparse-framework_1","title":"Deepsparse framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cpu-device_5","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_9","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub                === \"Native\"</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_22","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_10","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_13","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_13","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_13","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub     * If you want to download the official MLPerf model and dataset for resnet50 you can follow this README.</p> <ul> <li>Please see mobilenets.md for running mobilenet models for Image Classification.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_23","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_11","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>RESNET50</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#edge-category_1","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_10","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_24","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_12","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_12","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_24","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_11","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_25","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_12","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_25","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>RESNET50</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#edge-category_2","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#pytorch-framework","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cpu-device_6","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_12","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_12","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_26","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_13","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_13","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_26","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_14","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_14","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_14","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_27","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_14","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_14","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_27","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#pytorch-framework_1","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cpu-device_7","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_13","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_13","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_28","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_13","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_28","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_15","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_15","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_15","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_29","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_14","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_29","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"<p>RESNET50</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#edge-category_3","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#qaic-device","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_16","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_16","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_16","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_30","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_15","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_15","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_30","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#glow-framework_1","title":"Glow framework","text":"QAIC"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#qaic-device_1","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_17","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_17","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_17","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_31","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_15","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_31","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#mlperf-modular-implementation-in-c","title":"MLPerf Modular Implementation in C++","text":"<p>RESNET50</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#edge-category_4","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#onnxruntime-framework_2","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cpu-device_8","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_14","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_14","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_32","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_16","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_16","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_32","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_18","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_18","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_18","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_33","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_17","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_17","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_33","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cuda-device_6","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_15","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_15","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_34","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_18","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_18","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_34","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_19","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_19","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_19","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_35","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#singlestream_19","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#multistream_19","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_35","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#onnxruntime-framework_3","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cpu-device_9","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_16","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_16","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_36","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_16","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_36","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_20","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_20","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_20","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_37","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_17","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_37","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#cuda-device_7","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-environment_17","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_17","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_38","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_18","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_38","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#native-environment_21","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_21","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_21","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#offline_39","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#server_19","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/#all-scenarios_39","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/","title":"Question Answering using Bert-Large","text":"MLCommons-Python"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>BERT-99</p> edge"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#edge-category","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> PytorchDeepsparse"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#singlestream","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#singlestream_1","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#singlestream_2","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#singlestream_3","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_4","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#singlestream_4","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#deepsparse-framework","title":"Deepsparse framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub                      Please click here to view available generic model stubs for bert deepsparse <pre><code>                * **obert-large-pruned95_quant-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni\n\n                * **mobilebert-none-14layer_pruned50_quant-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni\n\n                * **mobilebert-none-base_quant-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n\n                * **bert-base-pruned95_obs_quant-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none\n\n                * **mobilebert-none-14layer_pruned50-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni\n\n                * **obert-base-pruned90-none:** zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **obert-large-pruned97_quant-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none\n\n                * **bert-base-pruned90-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **bert-large-pruned80_quant-none-vnni:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni\n\n                * **obert-large-pruned95-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni\n\n                * **obert-large-pruned97-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none\n\n                * **bert-large-base-none:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none\n\n                * **obert-large-base-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none\n\n                * **mobilebert-none-base-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none\n                &lt;/details&gt;\n            === \"Native\"\n                ###### Native Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                ####### Setup a virtual environment for Python\n\n\n                ```bash\n                pip install cmx4mlperf\n                mlcr install,python-venv --name=mlperf\n                export MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n                ```\n                ####### Performance Estimation for Offline Scenario\n\n                !!! tip\n\n                    - Compliance runs can be enabled by adding `--compliance=yes`.\n\n                    - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.\n\n                    - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.\n\n                    - `_r4.1-dev` could also be given instead of `_r5.0-dev` if you want to run the benchmark with the MLPerf version being 4.1.\n\n                    - Add `--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the official MLPerf Inference implementation in a custom fork.\n\n                    - Add `--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the model config accuracy script in the submission checker within a custom fork.\n\n                    - Add `--adr.inference-src.version=custom` if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.\n\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99 \\\n                   --implementation=reference \\\n                   --framework=deepsparse \\\n                   --category=edge \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cpu  \\\n                   --quiet \\\n                   --test_query_count=100\\\n                   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=edge \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                === \"SingleStream\"\n                    ###### SingleStream\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=edge \\\n                       --scenario=SingleStream \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=edge  \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n</code></pre> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub                      Please click here to view available generic model stubs for bert deepsparse <pre><code>                * **obert-large-pruned95_quant-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni\n\n                * **mobilebert-none-14layer_pruned50_quant-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni\n\n                * **mobilebert-none-base_quant-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n\n                * **bert-base-pruned95_obs_quant-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none\n\n                * **mobilebert-none-14layer_pruned50-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni\n\n                * **obert-base-pruned90-none:** zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **obert-large-pruned97_quant-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none\n\n                * **bert-base-pruned90-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **bert-large-pruned80_quant-none-vnni:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni\n\n                * **obert-large-pruned95-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni\n\n                * **obert-large-pruned97-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none\n\n                * **bert-large-base-none:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none\n\n                * **obert-large-base-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none\n\n                * **mobilebert-none-base-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none\n                &lt;/details&gt;\n=== \"datacenter\"\n\n    ### Datacenter category\n\n     In the datacenter category, bert-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.\n\n    === \"Pytorch\"\n        #### Pytorch framework\n\n        === \"CPU\"\n            ##### CPU device\n\n            &lt;details&gt;\n            &lt;summary&gt;Please click here to see the minimum system requirements for running the benchmark&lt;/summary&gt;\n\n            * **Disk Space**: 50GB\n\n            &lt;/details&gt;\n            === \"Docker\"\n                ###### Docker Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                ####### Docker Container Build and Performance Estimation for Offline Scenario\n\n                !!! tip\n\n                    - Compliance runs can be enabled by adding `--compliance=yes`.\n\n                    - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.\n\n                    - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.\n\n                    - `_r4.1-dev` could also be given instead of `_r5.0-dev` if you want to run the benchmark with the MLPerf version being 4.1.\n\n                    - Add `--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the official MLPerf Inference implementation in a custom fork.\n\n                    - Add `--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the model config accuracy script in the submission checker within a custom fork.\n\n                    - Add `--adr.inference-src.version=custom` if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.\n\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99 \\\n                   --implementation=reference \\\n                   --framework=pytorch \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cpu  \\\n                   --docker --quiet \\\n                   --test_query_count=100\n                ```\n                The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.\n\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the docker launch &lt;/summary&gt;\n\n                * `--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;`: to use a custom fork of cm4mlops repository inside the docker image\n\n                * `--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;`: to checkout a custom branch of the cloned cm4mlops repository inside the docker image\n\n                * `--docker_cache=no`: to not use docker cache during the image build\n                * `--docker_os=ubuntu`: ubuntu and rhel are supported. \n                * `--docker_os_version=20.04`: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL\n                &lt;/details&gt;\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n\n            === \"Native\"\n                ###### Native Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                ####### Setup a virtual environment for Python\n\n\n                ```bash\n                pip install cmx4mlperf\n                mlcr install,python-venv --name=mlperf\n                export MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n                ```\n                ####### Performance Estimation for Offline Scenario\n\n                !!! tip\n\n                    - Compliance runs can be enabled by adding `--compliance=yes`.\n\n                    - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.\n\n                    - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.\n\n                    - `_r4.1-dev` could also be given instead of `_r5.0-dev` if you want to run the benchmark with the MLPerf version being 4.1.\n\n                    - Add `--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the official MLPerf Inference implementation in a custom fork.\n\n                    - Add `--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the model config accuracy script in the submission checker within a custom fork.\n\n                    - Add `--adr.inference-src.version=custom` if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.\n\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99 \\\n                   --implementation=reference \\\n                   --framework=pytorch \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cpu  \\\n                   --quiet \\\n                   --test_query_count=100\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n\n        === \"CUDA\"\n            ##### CUDA device\n\n            &lt;details&gt;\n            &lt;summary&gt;Please click here to see the minimum system requirements for running the benchmark&lt;/summary&gt;\n\n            * **Device Memory**: 8GB\n\n            * **Disk Space**: 50GB\n\n            &lt;/details&gt;\n            === \"Docker\"\n                ###### Docker Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                ####### Docker Container Build and Performance Estimation for Offline Scenario\n\n                !!! tip\n\n                    - Compliance runs can be enabled by adding `--compliance=yes`.\n\n                    - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.\n\n                    - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.\n\n                    - `_r4.1-dev` could also be given instead of `_r5.0-dev` if you want to run the benchmark with the MLPerf version being 4.1.\n\n                    - Add `--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the official MLPerf Inference implementation in a custom fork.\n\n                    - Add `--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the model config accuracy script in the submission checker within a custom fork.\n\n                    - Add `--adr.inference-src.version=custom` if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.\n\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99 \\\n                   --implementation=reference \\\n                   --framework=pytorch \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cuda  \\\n                   --docker --quiet \\\n                   --test_query_count=500\n                ```\n                The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.\n\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the docker launch &lt;/summary&gt;\n\n                * `--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;`: to use a custom fork of cm4mlops repository inside the docker image\n\n                * `--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;`: to checkout a custom branch of the cloned cm4mlops repository inside the docker image\n\n                * `--docker_cache=no`: to not use docker cache during the image build\n                &lt;/details&gt;\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cuda \\\n                       --quiet \n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cuda \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cuda \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n\n            === \"Native\"\n                ###### Native Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n\n                !!! tip\n\n                    - It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.\n\n                ####### Setup a virtual environment for Python\n\n\n                ```bash\n                pip install cmx4mlperf\n                mlcr install,python-venv --name=mlperf\n                export MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n                ```\n                ####### Performance Estimation for Offline Scenario\n\n                !!! tip\n\n                    - Compliance runs can be enabled by adding `--compliance=yes`.\n\n                    - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.\n\n                    - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.\n\n                    - `_r4.1-dev` could also be given instead of `_r5.0-dev` if you want to run the benchmark with the MLPerf version being 4.1.\n\n                    - Add `--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the official MLPerf Inference implementation in a custom fork.\n\n                    - Add `--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the model config accuracy script in the submission checker within a custom fork.\n\n                    - Add `--adr.inference-src.version=custom` if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.\n\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99 \\\n                   --implementation=reference \\\n                   --framework=pytorch \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cuda  \\\n                   --quiet \\\n                   --test_query_count=500\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cuda \\\n                       --quiet \n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cuda \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cuda \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n\n        === \"ROCm\"\n            ##### ROCm device\n\n            &lt;details&gt;\n            &lt;summary&gt;Please click here to see the minimum system requirements for running the benchmark&lt;/summary&gt;\n\n            * **Disk Space**: 50GB\n\n            &lt;/details&gt;\n            === \"Native\"\n                ###### Native Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                ####### Setup a virtual environment for Python\n\n\n                ```bash\n                pip install cmx4mlperf\n                mlcr install,python-venv --name=mlperf\n                export MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n                ```\n                ####### Performance Estimation for Offline Scenario\n\n                !!! tip\n\n                    - Compliance runs can be enabled by adding `--compliance=yes`.\n\n                    - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.\n\n                    - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.\n\n                    - `_r4.1-dev` could also be given instead of `_r5.0-dev` if you want to run the benchmark with the MLPerf version being 4.1.\n\n                    - Add `--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the official MLPerf Inference implementation in a custom fork.\n\n                    - Add `--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the model config accuracy script in the submission checker within a custom fork.\n\n                    - Add `--adr.inference-src.version=custom` if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.\n\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99 \\\n                   --implementation=reference \\\n                   --framework=pytorch \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=rocm  \\\n                   --quiet \\\n                   --test_query_count=100\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=rocm \\\n                       --quiet \n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=rocm \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=rocm \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n\n    === \"Deepsparse\"\n        #### Deepsparse framework\n\n        === \"CPU\"\n            ##### CPU device\n\n            &lt;details&gt;\n            &lt;summary&gt;Please click here to see the minimum system requirements for running the benchmark&lt;/summary&gt;\n\n            * **Disk Space**: 50GB\n\n            &lt;/details&gt;\n            === \"Docker\"\n                ###### Docker Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                ####### Docker Container Build and Performance Estimation for Offline Scenario\n\n                !!! tip\n\n                    - Compliance runs can be enabled by adding `--compliance=yes`.\n\n                    - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.\n\n                    - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.\n\n                    - `_r4.1-dev` could also be given instead of `_r5.0-dev` if you want to run the benchmark with the MLPerf version being 4.1.\n\n                    - Add `--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the official MLPerf Inference implementation in a custom fork.\n\n                    - Add `--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the model config accuracy script in the submission checker within a custom fork.\n\n                    - Add `--adr.inference-src.version=custom` if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.\n\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99 \\\n                   --implementation=reference \\\n                   --framework=deepsparse \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cpu  \\\n                   --docker --quiet \\\n                   --test_query_count=100\\\n                   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                ```\n                The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.\n\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the docker launch &lt;/summary&gt;\n\n                * `--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;`: to use a custom fork of cm4mlops repository inside the docker image\n\n                * `--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;`: to checkout a custom branch of the cloned cm4mlops repository inside the docker image\n\n                * `--docker_cache=no`: to not use docker cache during the image build\n                * `--docker_os=ubuntu`: ubuntu and rhel are supported. \n                * `--docker_os_version=20.04`: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL\n                &lt;/details&gt;\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n</code></pre> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub                      Please click here to view available generic model stubs for bert deepsparse <pre><code>                * **obert-large-pruned95_quant-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni\n\n                * **mobilebert-none-14layer_pruned50_quant-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni\n\n                * **mobilebert-none-base_quant-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n\n                * **bert-base-pruned95_obs_quant-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none\n\n                * **mobilebert-none-14layer_pruned50-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni\n\n                * **obert-base-pruned90-none:** zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **obert-large-pruned97_quant-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none\n\n                * **bert-base-pruned90-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **bert-large-pruned80_quant-none-vnni:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni\n\n                * **obert-large-pruned95-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni\n\n                * **obert-large-pruned97-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none\n\n                * **bert-large-base-none:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none\n\n                * **obert-large-base-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none\n\n                * **mobilebert-none-base-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none\n                &lt;/details&gt;\n            === \"Native\"\n                ###### Native Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                ####### Setup a virtual environment for Python\n\n\n                ```bash\n                pip install cmx4mlperf\n                mlcr install,python-venv --name=mlperf\n                export MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n                ```\n                ####### Performance Estimation for Offline Scenario\n\n                !!! tip\n\n                    - Compliance runs can be enabled by adding `--compliance=yes`.\n\n                    - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.\n\n                    - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.\n\n                    - `_r4.1-dev` could also be given instead of `_r5.0-dev` if you want to run the benchmark with the MLPerf version being 4.1.\n\n                    - Add `--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the official MLPerf Inference implementation in a custom fork.\n\n                    - Add `--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the model config accuracy script in the submission checker within a custom fork.\n\n                    - Add `--adr.inference-src.version=custom` if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.\n\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99 \\\n                   --implementation=reference \\\n                   --framework=deepsparse \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cpu  \\\n                   --quiet \\\n                   --test_query_count=100\\\n                   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n</code></pre> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub                      Please click here to view available generic model stubs for bert deepsparse <pre><code>                * **obert-large-pruned95_quant-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni\n\n                * **mobilebert-none-14layer_pruned50_quant-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni\n\n                * **mobilebert-none-base_quant-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n\n                * **bert-base-pruned95_obs_quant-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none\n\n                * **mobilebert-none-14layer_pruned50-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni\n\n                * **obert-base-pruned90-none:** zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **obert-large-pruned97_quant-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none\n\n                * **bert-base-pruned90-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **bert-large-pruned80_quant-none-vnni:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni\n\n                * **obert-large-pruned95-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni\n\n                * **obert-large-pruned97-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none\n\n                * **bert-large-base-none:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none\n\n                * **obert-large-base-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none\n\n                * **mobilebert-none-base-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none\n                &lt;/details&gt;\n\n\nBERT-99.9\n\n=== \"datacenter\"\n\n    ### Datacenter category\n\n     In the datacenter category, bert-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.\n\n    === \"Pytorch\"\n        #### Pytorch framework\n\n        === \"CPU\"\n            ##### CPU device\n\n            &lt;details&gt;\n            &lt;summary&gt;Please click here to see the minimum system requirements for running the benchmark&lt;/summary&gt;\n\n            * **Disk Space**: 50GB\n\n            &lt;/details&gt;\n            === \"Docker\"\n                ###### Docker Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                 You can reuse the same environment as described for bert-99.\n                ###### Performance Estimation for Offline Scenario\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99.9 \\\n                   --implementation=reference \\\n                   --framework=pytorch \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cpu  \\\n                   --quiet \\\n                   --test_query_count=100\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n\n            === \"Native\"\n                ###### Native Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                 You can reuse the same environment as described for bert-99.\n                ###### Performance Estimation for Offline Scenario\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99.9 \\\n                   --implementation=reference \\\n                   --framework=pytorch \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cpu  \\\n                   --quiet \\\n                   --test_query_count=100\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n\n        === \"CUDA\"\n            ##### CUDA device\n\n            &lt;details&gt;\n            &lt;summary&gt;Please click here to see the minimum system requirements for running the benchmark&lt;/summary&gt;\n\n            * **Device Memory**: 8GB\n\n            * **Disk Space**: 50GB\n\n            &lt;/details&gt;\n            === \"Docker\"\n                ###### Docker Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                 You can reuse the same environment as described for bert-99.\n                ###### Performance Estimation for Offline Scenario\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99.9 \\\n                   --implementation=reference \\\n                   --framework=pytorch \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cuda  \\\n                   --quiet \\\n                   --test_query_count=500\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cuda \\\n                       --quiet \n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cuda \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cuda \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n\n            === \"Native\"\n                ###### Native Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n\n                !!! tip\n\n                    - It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.\n\n                 You can reuse the same environment as described for bert-99.\n                ###### Performance Estimation for Offline Scenario\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99.9 \\\n                   --implementation=reference \\\n                   --framework=pytorch \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cuda  \\\n                   --quiet \\\n                   --test_query_count=500\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cuda \\\n                       --quiet \n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cuda \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cuda \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n\n        === \"ROCm\"\n            ##### ROCm device\n\n            &lt;details&gt;\n            &lt;summary&gt;Please click here to see the minimum system requirements for running the benchmark&lt;/summary&gt;\n\n            * **Disk Space**: 50GB\n\n            &lt;/details&gt;\n            === \"Native\"\n                ###### Native Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                 You can reuse the same environment as described for bert-99.\n                ###### Performance Estimation for Offline Scenario\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99.9 \\\n                   --implementation=reference \\\n                   --framework=pytorch \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=rocm  \\\n                   --quiet \\\n                   --test_query_count=100\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=rocm \\\n                       --quiet \n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=rocm \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=pytorch \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=rocm \\\n                       --quiet \n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n\n    === \"Deepsparse\"\n        #### Deepsparse framework\n\n        === \"CPU\"\n            ##### CPU device\n\n            &lt;details&gt;\n            &lt;summary&gt;Please click here to see the minimum system requirements for running the benchmark&lt;/summary&gt;\n\n            * **Disk Space**: 50GB\n\n            &lt;/details&gt;\n            === \"Docker\"\n                ###### Docker Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                 You can reuse the same environment as described for bert-99.\n                ###### Performance Estimation for Offline Scenario\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99.9 \\\n                   --implementation=reference \\\n                   --framework=deepsparse \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cpu  \\\n                   --quiet \\\n                   --test_query_count=100\\\n                   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n</code></pre> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub                      Please click here to view available generic model stubs for bert deepsparse <pre><code>                * **obert-large-pruned95_quant-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni\n\n                * **mobilebert-none-14layer_pruned50_quant-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni\n\n                * **mobilebert-none-base_quant-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n\n                * **bert-base-pruned95_obs_quant-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none\n\n                * **mobilebert-none-14layer_pruned50-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni\n\n                * **obert-base-pruned90-none:** zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **obert-large-pruned97_quant-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none\n\n                * **bert-base-pruned90-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **bert-large-pruned80_quant-none-vnni:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni\n\n                * **obert-large-pruned95-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni\n\n                * **obert-large-pruned97-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none\n\n                * **bert-large-base-none:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none\n\n                * **obert-large-base-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none\n\n                * **mobilebert-none-base-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none\n                &lt;/details&gt;\n            === \"Native\"\n                ###### Native Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                 You can reuse the same environment as described for bert-99.\n                ###### Performance Estimation for Offline Scenario\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99.9 \\\n                   --implementation=reference \\\n                   --framework=deepsparse \\\n                   --category=datacenter \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cpu  \\\n                   --quiet \\\n                   --test_query_count=100\\\n                   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=datacenter \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                === \"Server\"\n                    ###### Server\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=datacenter \\\n                       --scenario=Server\\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                === \"All Scenarios\"\n                    ###### All Scenarios\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n                       --model=bert-99.9 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=datacenter \\\n                       --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                    !!! tip\n\n                        * `&lt;SERVER_TARGET_QPS&gt;` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n</code></pre> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub                      Please click here to view available generic model stubs for bert deepsparse <pre><code>                * **obert-large-pruned95_quant-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni\n\n                * **mobilebert-none-14layer_pruned50_quant-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni\n\n                * **mobilebert-none-base_quant-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n\n                * **bert-base-pruned95_obs_quant-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none\n\n                * **mobilebert-none-14layer_pruned50-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni\n\n                * **obert-base-pruned90-none:** zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **obert-large-pruned97_quant-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none\n\n                * **bert-base-pruned90-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **bert-large-pruned80_quant-none-vnni:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni\n\n                * **obert-large-pruned95-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni\n\n                * **obert-large-pruned97-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none\n\n                * **bert-large-base-none:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none\n\n                * **obert-large-base-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none\n\n                * **mobilebert-none-base-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none\n                &lt;/details&gt;\n * If you want to download the official MLPerf model and dataset for bert-99.9 you can follow [this README](get-bert-data.md).\n</code></pre> NvidiaIntelQualcomm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_5","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#singlestream_5","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>BERT-99</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#edge-category_1","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_6","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#singlestream_6","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>BERT-99.9</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_7","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#server","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#performance-estimation-for-offline-scenario_3","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_8","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#server_1","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>BERT-99</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#edge-category_2","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#pytorch-framework_1","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_9","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#singlestream_7","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_10","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#singlestream_8","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#pytorch-framework_2","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_11","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#server_2","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>BERT-99.9</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_12","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#server_3","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#cpu-device_4","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_13","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#server_4","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_14","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#server_5","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"<p>BERT-99</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#edge-category_3","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#qaic-device","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#performance-estimation-for-offline-scenario_8","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_15","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#singlestream_9","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#glow-framework_1","title":"Glow framework","text":"QAIC"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#qaic-device_1","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#performance-estimation-for-offline-scenario_9","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>BERT-99.9</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_16","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#server_6","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#datacenter-category_5","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#glow-framework_2","title":"Glow framework","text":"QAIC"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#qaic-device_2","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#offline_17","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#server_7","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/bert/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-bert-data/","title":"Question Answering using Bert-Large","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-bert-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>BERT validation run uses the SQuAD v1.1 dataset.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-bert-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,squad,validation -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-bert-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Bert-Large Model</p> PytorchOnnxTensorflow"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-bert-data/#pytorch","title":"Pytorch","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,bert-large,_pytorch -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-bert-data/#onnx","title":"Onnx","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,bert-large,_onnx -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-bert-data/#tensorflow","title":"Tensorflow","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,bert-large,_tensorflow -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-gptj-data/","title":"Text Summarization using GPT-J","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-gptj-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>GPT-J validation run uses the CNNDM dataset.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-gptj-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,cnndm,validation -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-gptj-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf GPT-J Model</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-gptj-data/#pytorch","title":"Pytorch","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,gptj,_pytorch -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-llama2-70b-data/","title":"Text Summarization using LLAMA2-70b","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-llama2-70b-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>LLAMA2-70b validation run uses the Open ORCA dataset.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-llama2-70b-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,openorca,validation -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-llama2-70b-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf LLAMA2-70b Model</p> Pytorch <p>Tip</p> <p>Access Request Link for MLCommons members</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-llama2-70b-data/#pytorch","title":"Pytorch","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,llama2-70b,_pytorch -j --outdirname=&lt;My download path&gt;\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-llama3_1-405b-data/","title":"Text Summarization using LLAMA3.1-405b","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-llama3_1-405b-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-llama3_1-405b-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,mlperf,inference,llama3,_validation --outdirname=&lt;path to download&gt; -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-llama3_1-405b-data/#get-calibration-dataset","title":"Get Calibration Dataset","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,mlperf,inference,llama3,_calibration --outdirname=&lt;path to download&gt; -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-llama3_1-405b-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf LLAMA3.1-405b Model</p> Pytorch <p>Tip</p> <p>Access Request Link for MLCommons members</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-llama3_1-405b-data/#pytorch","title":"Pytorch","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,llama3 --outdirname=&lt;path to download&gt;  -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-mixtral-8x7b-data/","title":"Get mixtral 8x7b data","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-mixtral-8x7b-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the preprocessed validation and calibration datasets. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>mixtral-8x7b validation run uses the combined dataset - Open ORCA, GSM8K and MBXP.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-mixtral-8x7b-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset-mixtral,openorca-mbxp-gsm8k-combined -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-mixtral-8x7b-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf MIXTRAL-8x7b Model</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/get-mixtral-8x7b-data/#pytorch","title":"Pytorch","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,mixtral -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/","title":"Text Summarization using GPT-J","text":"MLCommons-PythonNvidiaIntelQualcomm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>GPTJ-99</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#edge-category","title":"Edge category","text":"<p>In the edge category, gptj-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_1","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB(fp32). 40GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_2","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_3","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_4","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_4","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_5","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_6","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_1","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB(fp32). 40GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_7","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_2","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_8","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_3","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>GPTJ-99.9</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_9","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_4","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#edge-category_1","title":"Edge category","text":"<p>In the edge category, gptj-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#pytorch-framework_2","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_10","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_5","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_11","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_6","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB(fp32). 40GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_12","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_7","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_9","title":"Performance Estimation for Offline Scenario","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_13","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_8","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_14","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_9","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#pytorch-framework_3","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_15","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_5","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_12","title":"Performance Estimation for Offline Scenario","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_16","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_6","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB(fp32). 40GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_13","title":"Performance Estimation for Offline Scenario","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_17","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_7","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_14","title":"Performance Estimation for Offline Scenario","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_18","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_8","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_15","title":"Performance Estimation for Offline Scenario","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for gptj-99.9 you can follow this README.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_19","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_9","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>GPTJ-99</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#edge-category_2","title":"Edge category","text":"<p>In the edge category, gptj-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_20","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_10","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>GPTJ-99.9</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_21","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_10","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#edge-category_3","title":"Edge category","text":"<p>In the edge category, gptj-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cuda-device_6","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_16","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_22","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_11","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#tensorrt-framework_3","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cuda-device_7","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_17","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_23","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_11","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>GPTJ-99</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#edge-category_4","title":"Edge category","text":"<p>In the edge category, gptj-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#pytorch-framework_4","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cpu-device_4","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_12","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_24","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_12","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_24","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_12","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_18","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_25","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#singlestream_13","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_25","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#pytorch-framework_5","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#cpu-device_5","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-environment_13","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_26","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_12","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_26","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#native-environment_13","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_19","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#offline_27","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#server_13","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#all-scenarios_27","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"<pre><code>WIP\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/","title":"Text Summarization using LLAMA2-70b","text":"MLCommons-PythonNvidiaNeural MagicAMD"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>LLAMA2-70B-99</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_1","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8x80GB</p> </li> <li> <p>Disk Space: 700GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_2","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_3","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>LLAMA2-70B-99.9</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_4","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_4","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_3","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_5","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_5","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_4","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_6","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_6","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8x80GB</p> </li> <li> <p>Disk Space: 700GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_5","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_7","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_7","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_8","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_8","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for llama2-70b-99.9 you can follow this README.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_9","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_9","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>LLAMA2-70B-99</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB</p> </li> <li> <p>Disk Space: 700GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>The dataset for NVIDIA's implementation of Llama2 is not publicly available. The user must fill this form and be verified as a MLCommons member to access the dataset.</p> </li> <li> <p><code>PATH_TO_PICKE_FILE</code> should be replaced with path to the downloaded pickle file.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 \\\n   --tp_size=2 \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKLE_FILE&gt;\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>LLAMA2-70B-99.9</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_10","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKE_FILE&gt;\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_10","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKE_FILE&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKE_FILE&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB</p> </li> <li> <p>Disk Space: 700GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --tp_size=2 \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKLE_FILE&gt;\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_11","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKE_FILE&gt;\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_11","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKE_FILE&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKE_FILE&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#neural-magic-mlperf-implementation","title":"Neural Magic MLPerf Implementation","text":"<p>LLAMA2-70B-99</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#pytorch-framework_2","title":"pytorch framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#run-the-inference-server","title":"# Run the Inference Server","text":"<pre><code>pip install cmx4mlperf\ncr run,vllm-server \\\n --model=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n --quiet\n</code></pre> <p>Tip</p> <ul> <li>Host and Port number of the server can be configured through <code>--host</code> and <code>--port</code> options. Otherwise, server will run on the default host <code>localhost</code> and port <code>8000</code>.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_12","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_12","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#run-the-inference-server_1","title":"# Run the Inference Server","text":"<pre><code>pip install cmx4mlperf\ncr run,vllm-server \\\n --model=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n --quiet\n</code></pre> <p>Tip</p> <ul> <li>Host and Port number of the server can be configured through <code>--host</code> and <code>--port</code> options. Otherwise, server will run on the default host <code>localhost</code> and port <code>8000</code>.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_9","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>LLAMA2-70B-99.9</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_13","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_13","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#datacenter-category_5","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#pytorch-framework_3","title":"pytorch framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_14","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_14","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_15","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_15","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#amd-mlperf-implementation","title":"AMD MLPerf Implementation","text":"<p>LLAMA2-70B-99</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#datacenter-category_6","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#pytorch-framework_4","title":"pytorch framework","text":"cuda"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#cuda-device_6","title":"cuda device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_12","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>LLAMA2-70B-99.9</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_16","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_16","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#datacenter-category_7","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#pytorch-framework_5","title":"pytorch framework","text":"cuda"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#cuda-device_7","title":"cuda device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_13","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#offline_17","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#server_17","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/","title":"Text Summarization using LLAMA3_1-405b","text":"MLCommons-Python"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>LLAMA3_1-405B-99</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, llama3_1-405b-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_MLPERF_MODEL_LLAMA3_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_MLPERF_DATASET_LLAMA3_DOWNLOAD_TO_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#server","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#all-scenarios","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#server_1","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_MLPERF_MODEL_LLAMA3_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p><code>--env.MLC_MLPERF_DATASET_LLAMA3_DOWNLOAD_TO_HOST=yes</code> option can be used to download the dataset on the host so that it can be reused across different container lanuches. </p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#server_2","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>LLAMA3_1-405B-99.9</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#server_3","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama3_1-405b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, llama3_1-405b-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama3_1-405b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#performance-estimation-for-offline-scenario_2","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#offline_4","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#server_4","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama3_1-405b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#performance-estimation-for-offline-scenario_3","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#offline_5","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#server_5","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama3_1-405b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#performance-estimation-for-offline-scenario_4","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#offline_6","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#server_6","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for llama3_1-405b-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#performance-estimation-for-offline-scenario_5","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for llama3_1-405b-99.9 you can follow this README.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#offline_7","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#server_7","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=llama3_1-405b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/","title":"Question Answering, Math, and Code Generation using Mixtral-8x7B","text":"MLCommons-Python"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>MIXTRAL-8X7B</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, mixtral-8x7b has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 100GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#server","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#all-scenarios","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#server_1","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 4x80GB</p> </li> <li> <p>Disk Space: 100GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#server_2","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#server_3","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 100GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for mixtral-8x7b you can follow this README.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#offline_4","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#server_4","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/","title":"Question and Answering using Bert Large for IndySCC 2024","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#introduction","title":"Introduction","text":"<p>This guide is designed for the IndySCC 2024 to walk participants through running and optimizing the MLPerf Inference Benchmark using Bert Large across various software and hardware configurations. The goal is to maximize system throughput (measured in samples per second) without compromising accuracy.</p> <p>For a valid MLPerf inference submission, two types of runs are required: a performance run and an accuracy run. In this competition, we focus on the <code>Offline</code> scenario, where throughput is the key metric\u2014higher values are better. The official MLPerf inference benchmark for Bert Large requires processing a minimum of 10833 samples in both performance and accuracy modes using the Squad v1.1 dataset.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#scoring","title":"Scoring","text":"<p>In the IndySCC 2024, your objective will be to run a reference (unoptimized) Python implementation of the MLPerf inference benchmark to complete a successful submission passing the submission checker. Only one of the available framework needs to be submitted.</p> <p>Info</p> <p>Both MLPerf and MLC automation are evolving projects. If you encounter issues or have questions, please submit them here</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#artifacts-to-submit-to-the-scc-committee","title":"Artifacts to submit to the SCC committee","text":"<p>All the needed files are automatically pushed to the GitHub repository if you manage to complete the given commands. No additional files need to be submitted.</p> MLCommons-Python"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>BERT-99</p> edge"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#edge-category","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline scenarios and all the scenarios are mandatory for a closed division submission.</p> PytorchDeepsparse"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#offline_4","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#deepsparse-framework","title":"Deepsparse framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub                      Please click here to view available generic model stubs for bert deepsparse <pre><code>                * **obert-large-pruned95_quant-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni\n\n                * **mobilebert-none-14layer_pruned50_quant-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni\n\n                * **mobilebert-none-base_quant-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n\n                * **bert-base-pruned95_obs_quant-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none\n\n                * **mobilebert-none-14layer_pruned50-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni\n\n                * **obert-base-pruned90-none:** zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **obert-large-pruned97_quant-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none\n\n                * **bert-base-pruned90-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **bert-large-pruned80_quant-none-vnni:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni\n\n                * **obert-large-pruned95-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni\n\n                * **obert-large-pruned97-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none\n\n                * **bert-large-base-none:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none\n\n                * **obert-large-base-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none\n\n                * **mobilebert-none-base-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none\n                &lt;/details&gt;\n            === \"Native\"\n                ###### Native Environment\n\n                Please refer to the [installation page](site:inference/install/) to install MLCFlow for running the automated benchmark commands.\n\n                ####### Setup a virtual environment for Python\n\n\n                ```bash\n                pip install cmx4mlperf\n                mlcr install,python-venv --name=mlperf\n                export MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n                ```\n                ####### Performance Estimation for Offline Scenario\n\n                !!! tip\n\n                    - Compliance runs can be enabled by adding `--compliance=yes`.\n\n                    - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.\n\n                    - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.\n\n                    - `_r4.1-dev` could also be given instead of `_r5.0-dev` if you want to run the benchmark with the MLPerf version being 4.1.\n\n                    - Add `--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the official MLPerf Inference implementation in a custom fork.\n\n                    - Add `--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;` if you are modifying the model config accuracy script in the submission checker within a custom fork.\n\n                    - Add `--adr.inference-src.version=custom` if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.\n\n\n\n                ```bash\n                pip install cmx4mlperf\n                cr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n                   --model=bert-99 \\\n                   --implementation=reference \\\n                   --framework=deepsparse \\\n                   --category=edge \\\n                   --scenario=Offline \\\n                   --execution_mode=test \\\n                   --device=cpu  \\\n                   --quiet \\\n                   --test_query_count=100\\\n                   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                ```\n                The above command should do a test run of Offline scenario and record the estimated offline_target_qps.\n\n                === \"Offline\"\n                    ###### Offline\n\n\n\n                    ```bash\n                    pip install cmx4mlperf\n                    cr run-mlperf,inference,_full,_r5.0-dev \\\n                       --model=bert-99 \\\n                       --implementation=reference \\\n                       --framework=deepsparse \\\n                       --category=edge \\\n                       --scenario=Offline \\\n                       --execution_mode=valid \\\n                       --device=cpu \\\n                       --quiet \\\n                       --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n                    ```\n                &lt;details&gt;\n                &lt;summary&gt; Please click here to see more options for the RUN command&lt;/summary&gt;\n\n                * Use `--division=closed` to do a closed division submission which includes compliance runs\n\n                * Use `--rerun` to do a rerun even when a valid run exists\n                * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful\n                &lt;/details&gt;\n</code></pre> <p>You can use any model from NeuralMagic sparse zoo (trained on Imagenet dataset) as --nm_model_zoo_stub                      Please click here to view available generic model stubs for bert deepsparse <pre><code>                * **obert-large-pruned95_quant-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni\n\n                * **mobilebert-none-14layer_pruned50_quant-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni\n\n                * **mobilebert-none-base_quant-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n\n                * **bert-base-pruned95_obs_quant-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none\n\n                * **mobilebert-none-14layer_pruned50-none-vnni:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni\n\n                * **obert-base-pruned90-none:** zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **obert-large-pruned97_quant-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none\n\n                * **bert-base-pruned90-none:** zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none\n\n                * **bert-large-pruned80_quant-none-vnni:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni\n\n                * **obert-large-pruned95-none-vnni:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni\n\n                * **obert-large-pruned97-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none\n\n                * **bert-large-base-none:** zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none\n\n                * **obert-large-base-none:** zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none\n\n                * **mobilebert-none-base-none:** zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none\n                &lt;/details&gt;\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#offline_5","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --nm_model_zoo_stub=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#submission-commands","title":"Submission Commands","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#generate-actual-submission-tree","title":"Generate actual submission tree","text":"<pre><code>pip install cmx4mlperf\ncr generate,inference,submission \\\n   --clean \\\n   --run-checker \\\n   --tar=yes \\\n   --env.CM_TAR_OUTFILE=submission.tar.gz \\\n   --division=open \\\n   --category=edge \\\n   --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n   --run_style=test \\\n   --quiet \\\n   --submitter=&lt;Team Name&gt;\n</code></pre> <ul> <li>Use <code>--hw_name=\"My system name\"</code> to give a meaningful system name.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/#push-results-to-github","title":"Push Results to GitHub","text":"<p>Fork the <code>mlperf-inference-results-scc24</code> branch of the repository URL at https://github.com/mlcommons/cm4mlperf-inference.</p> <p>Run the following command after replacing <code>--repo_url</code> with your GitHub fork URL.</p> <pre><code>pip install cmx4mlperf\ncr push,github,mlperf,inference,submission \\\n   --repo_url=https://github.com/&lt;myfork&gt;/cm4mlperf-inference \\\n   --repo_branch=mlperf-inference-results-scc24 \\\n   --commit_message=\"Results on system &lt;HW Name&gt;\" \\\n   --quiet\n</code></pre> <p>Once uploaded give a Pull Request to the origin repository. Github action will be running there and once finished you can see your submitted results at https://docs.mlcommons.org/cm4mlperf-inference.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/","title":"Medical Imaging using 3d-unet (KiTS 2019 kidney tumor segmentation task)","text":"MLCommons-PythonNvidiaIntel"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>3D-UNET-99</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#edge-category","title":"Edge category","text":"<p>In the edge category, 3d-unet-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_1","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_2","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_3","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_4","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_4","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_5","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_6","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_1","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_7","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_2","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_8","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_3","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>3D-UNET-99.9</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_9","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_4","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#edge-category_1","title":"Edge category","text":"<p>In the edge category, 3d-unet-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#pytorch-framework_2","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_10","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_5","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_11","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_6","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_12","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_7","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_9","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_13","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_8","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_14","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_9","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#pytorch-framework_3","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_15","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_5","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_12","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_16","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_6","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_13","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_17","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_7","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_14","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_18","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_8","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_15","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for 3d-unet-99.9 you can follow this README.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_19","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_9","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>3D-UNET-99</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#edge-category_2","title":"Edge category","text":"<p>In the edge category, 3d-unet-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_20","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_10","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>3D-UNET-99.9</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_21","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_10","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#edge-category_3","title":"Edge category","text":"<p>In the edge category, 3d-unet-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cuda-device_6","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_16","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_22","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_11","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#tensorrt-framework_3","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cuda-device_7","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_17","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_23","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_11","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>3D-UNET-99</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#edge-category_4","title":"Edge category","text":"<p>In the edge category, 3d-unet-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#pytorch-framework_4","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cpu-device_4","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_12","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_24","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_12","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_24","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_12","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_18","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_25","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_13","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_25","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#pytorch-framework_5","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cpu-device_5","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_13","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_26","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_12","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_26","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_13","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_19","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>3D-UNET-99.9</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_27","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_13","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_27","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#edge-category_5","title":"Edge category","text":"<p>In the edge category, 3d-unet-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#pytorch-framework_6","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cpu-device_6","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_14","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_20","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_28","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_14","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_28","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_14","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_21","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_29","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#singlestream_15","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_29","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#datacenter-category_5","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#pytorch-framework_7","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#cpu-device_7","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#docker-environment_15","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_22","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_30","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_14","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_30","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#native-environment_15","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_23","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#offline_31","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#server_15","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/#all-scenarios_31","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/get-3d-unet-data/","title":"Medical Imaging using 3d-unet (KiTS 2019 kidney tumor segmentation task)","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/get-3d-unet-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>3d-unet validation run uses the KiTS19 dataset performing KiTS 2019 kidney tumor segmentation task</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/get-3d-unet-data/#get-validation-datasetoriginal","title":"Get Validation Dataset(Original)","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,kits19,_validation -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/get-3d-unet-data/#get-validation-datasetpreprocessed","title":"Get Validation Dataset(Preprocessed)","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,kits19,preprocessed -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/get-3d-unet-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf 3d-unet Model</p> PytorchOnnxTensorflow"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/get-3d-unet-data/#pytorch","title":"Pytorch","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,3d-unet,_pytorch -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/get-3d-unet-data/#onnx","title":"Onnx","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,3d-unet,_onnx -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/get-3d-unet-data/#tensorflow","title":"Tensorflow","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,3d-unet,_tensorflow -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/get-retinanet-data/","title":"Object Detection using Retinanet","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/get-retinanet-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <p>Retinanet validation run uses the OpenImages v6 MLPerf validation dataset resized to 800x800 and consisting of 24,576 images.</p> <p>Retinanet calibration dataset consist of 500 images selected from the OpenImages v6 dataset.</p> <pre><code>pip install cmx4mlperf\ncr get,dataset,openimages,_calibration -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/get-retinanet-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,openimages,_validation -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/get-retinanet-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Retinanet Model</p> PytorchOnnx"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/get-retinanet-data/#pytorch","title":"Pytorch","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,retinanet,_pytorch -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/get-retinanet-data/#onnx","title":"Onnx","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,retinanet,_onnx -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/","title":"Object Detection using Retinanet","text":"MLCommons-PythonNvidiaIntelQualcommMLCommons-C++"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>RETINANET</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#edge-category","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_1","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_1","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_2","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_2","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_3","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_3","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_4","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_4","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_4","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_5","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_5","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_5","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_6","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_6","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_6","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_7","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_7","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_7","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_8","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_8","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_8","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_9","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_9","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_9","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#onnxruntime-framework_1","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_10","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_6","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_11","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_1","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_12","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_2","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_7","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_13","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_3","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_8","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_8","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_14","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_4","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_15","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_5","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_9","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_9","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_16","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_6","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_17","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_7","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_10","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_10","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_18","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_8","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_11","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_11","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for retinanet you can follow this README.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_19","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_9","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>RETINANET</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#edge-category_1","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_8","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_20","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_10","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_10","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_9","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_21","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_10","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>RETINANET</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#edge-category_2","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#pytorch-framework_2","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cpu-device_4","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_10","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_22","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_11","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_11","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_12","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_12","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_12","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_23","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_12","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_12","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cpu-device_5","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_11","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_24","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_11","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_24","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_13","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_13","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_13","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_25","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_12","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_25","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.0,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"<p>RETINANET</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#edge-category_3","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#qaic-device","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_14","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_14","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_14","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_26","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_13","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_13","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_26","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#glow-framework_1","title":"Glow framework","text":"QAIC"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#qaic-device_1","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_15","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_15","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_15","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_27","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_13","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_27","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#mlperf-modular-implementation-in-c","title":"MLPerf Modular Implementation in C++","text":"<p>RETINANET</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#edge-category_4","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#onnxruntime-framework_2","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cpu-device_6","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_12","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_12","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_28","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_14","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_14","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_28","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_16","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_16","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_16","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_29","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_15","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_15","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_29","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cuda-device_6","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_13","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_13","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_30","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_16","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_16","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_30","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_17","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_17","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_17","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_31","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#singlestream_17","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#multistream_17","title":"MultiStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_31","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#onnxruntime-framework_3","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cpu-device_7","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_14","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_14","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_32","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_14","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_32","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_18","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_18","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_18","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_33","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_15","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_33","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#cuda-device_7","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-environment_15","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_15","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_34","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_16","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_34","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#native-environment_19","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_19","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_19","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#offline_35","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#server_17","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/#all-scenarios_35","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/","title":"Recommendation using DLRM v2","text":"MLCommons-PythonNvidiaIntel"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>DLRM-V2-99</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_1","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_2","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_3","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>DLRM-V2-99.9</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_4","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_4","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_3","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_5","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_5","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_4","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_6","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_6","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_5","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_7","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_7","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_8","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_8","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for dlrm-v2-99.9 you can follow this README.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_9","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_9","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>DLRM-V2-99</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB</p> </li> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>DLRM-V2-99.9</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_10","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_10","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB</p> </li> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_11","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_11","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>DLRM-V2-99</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#pytorch-framework_2","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_12","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_12","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_9","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <p>DLRM-V2-99.9</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_13","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_13","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#datacenter-category_5","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_14","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_14","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#offline_15","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#server_15","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/get-dlrm-v2-data/","title":"Recommendation using DLRM v2","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/get-dlrm-v2-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>DLRM validation run uses the Criteo dataset (Day 23).</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/get-dlrm-v2-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,criteo,_validation -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/get-dlrm-v2-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf DLRM v2 Model</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/recommendation/get-dlrm-v2-data/#pytorch","title":"Pytorch","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,dlrm,_pytorch -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/get-sdxl-data/","title":"Text to Image using Stable Diffusion","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/get-sdxl-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>Stable Diffusion validation run uses the Coco 2014 dataset.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/get-sdxl-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>pip install cmx4mlperf\ncr get,dataset,coco2014,_validation -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/get-sdxl-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Stable Diffusion Model</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/get-sdxl-data/#pytorch","title":"Pytorch","text":"<pre><code>pip install cmx4mlperf\ncr get,ml-model,sdxl,_pytorch -j\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/","title":"Text to Image using Stable Diffusion","text":"MLCommons-PythonNvidiaIntel"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>SDXL</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#edge-category","title":"Edge category","text":"<p>In the edge category, sdxl has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#singlestream","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#singlestream_1","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB(fp32), 16GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#singlestream_2","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#singlestream_3","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_4","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#singlestream_4","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_5","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#server","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_6","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#server_1","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB(fp32), 16GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_7","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#server_2","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_8","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#server_3","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for sdxl you can follow this README.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_9","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#server_4","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r5.0-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>SDXL</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#edge-category_1","title":"Edge category","text":"<p>In the edge category, sdxl has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_10","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#singlestream_5","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li> <p><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_11","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#server_5","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>SDXL</p> edgedatacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#edge-category_2","title":"Edge category","text":"<p>In the edge category, sdxl has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#pytorch-framework_2","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_12","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#singlestream_6","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_6","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_13","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#singlestream_7","title":"SingleStream","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_14","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#server_6","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_7","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#offline_15","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#server_7","title":"Server","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_full,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/","title":"Text-to-Image with Stable Diffusion for Student Cluster Competition 2024","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#introduction","title":"Introduction","text":"<p>This guide is designed for the Student Cluster Competition 2024 to walk participants through running and optimizing the MLPerf Inference Benchmark using Stable Diffusion XL 1.0 across various software and hardware configurations. The goal is to maximize system throughput (measured in samples per second) without compromising accuracy. Since the model performs poorly on CPUs, it is essential to run it on GPUs.</p> <p>For a valid MLPerf inference submission, two types of runs are required: a performance run and an accuracy run. In this competition, we focus on the <code>Offline</code> scenario, where throughput is the key metric\u2014higher values are better. The official MLPerf inference benchmark for Stable Diffusion XL requires processing a minimum of 5,000 samples in both performance and accuracy modes using the COCO 2014 dataset. However, for SCC, we have reduced this and we also have two variants. <code>scc-base</code> variant has dataset size reduced to 50 samples, making it possible to complete both performance and accuracy runs in approximately 5-10 minutes. <code>scc-main</code> variant has dataset size of 500 and running it will fetch extra points as compared to running just the base variant. Setting up for Nvidia GPUs may take 2-3 hours but can be done offline. Your final output will be a tarball (<code>mlperf_submission.tar.gz</code>) containing MLPerf-compatible results, which you will submit to the SCC organizers for scoring.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#scoring","title":"Scoring","text":"<p>In the SCC, your first objective will be to run <code>scc-base</code> variant for reference (unoptimized) Python implementation or a vendor-provided version (such as Nvidia's) of the MLPerf inference benchmark to secure a baseline score.</p> <p>Once the initial run is successful, you'll have the opportunity to optimize the benchmark further by maximizing system utilization, applying quantization techniques, adjusting ML frameworks, experimenting with batch sizes, and more, all of which can earn you additional points.</p> <p>Since vendor implementations of the MLPerf inference benchmark vary and are often limited to single-node benchmarking, teams will compete within their respective hardware categories (e.g., Nvidia GPUs, AMD GPUs). Points will be awarded based on the throughput achieved on your system.</p> <p>Additionally, significant bonus points will be awarded if your team enhances an existing implementation, adds support for new hardware (such as an unsupported GPU), enables multi-node execution, or adds/extends scripts to cm4mlops repository supporting new devices, frameworks, implementations etc. All improvements must be made publicly available under the Apache 2.0 license and submitted alongside your results to the SCC committee to earn these bonus points, contributing to the MLPerf community.</p> <p>Info</p> <p>Both MLPerf and MLC automation are evolving projects. If you encounter issues or have questions, please submit them here</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#artifacts-to-submit-to-the-scc-committee","title":"Artifacts to submit to the SCC committee","text":"<p>You will need to submit the following files:</p> <ul> <li><code>mlperf_submission.run</code> - MLC commands to run MLPerf inference benchmark saved to this file.</li> <li><code>mlperf_submission.md</code> - description of your platform and some highlights of the MLPerf benchmark execution.</li> <li><code>&lt;Team Name&gt;</code> under which results are pushed to the github repository. </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#scc-interview","title":"SCC interview","text":"<p>You are encouraged to highlight and explain the obtained MLPerf inference throughput on your system and describe any improvements and extensions to this benchmark (such as adding new hardware backend or supporting multi-node execution) useful for the community and MLCommons.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#run-commands","title":"Run Commands","text":"MLCommons-PythonNvidia"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>SDXL</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#pytorch-framework","title":"Pytorch framework","text":"ROCmCUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_r5.0-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n    --precision=float16\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#offline","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_r5.0-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm \\\n   --quiet --precision=float16\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB(fp32), 16GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_r5.0-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n    --precision=float16\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#offline_1","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_r5.0-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda \\\n   --quiet --precision=float16\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>pip install cmx4mlperf\nmlcr install,python-venv --name=mlperf\nexport MLC_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p> </li> <li> <p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p> </li> <li> <p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p> </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_r5.0-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n    --precision=float16\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful </li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#offline_2","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_r5.0-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda \\\n   --quiet --precision=float16\n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>SDXL</p> datacenter"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install MLCFlow for running the automated benchmark commands.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p> </li> <li> <p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p>Tip</p> <ul> <li><code>--env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <p><pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_find-performance,_r4.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> Offline <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>Info</p> <p>Once the above run is successful, you can change <code>_scc24-base</code> to <code>_scc24-main</code> to run the main variant.</p>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#offline_3","title":"Offline","text":"<pre><code>pip install cmx4mlperf\ncr run-mlperf,inference,_r4.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#submission-commands","title":"Submission Commands","text":""},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#generate-actual-submission-tree","title":"Generate actual submission tree","text":"<pre><code>pip install cmx4mlperf\ncr generate,inference,submission \\\n   --clean \\\n   --run-checker \\\n   --tar=yes \\\n   --env.CM_TAR_OUTFILE=submission.tar.gz \\\n   --division=open \\\n   --category=datacenter \\\n   --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n   --run_style=test \\\n   --adr.submission-checker.tags=_short-run \\\n   --quiet \\\n   --submitter=&lt;Team Name&gt;\n</code></pre> <ul> <li>Use <code>--hw_name=\"My system name\"</code> to give a meaningful system name.</li> </ul>"},{"location":"cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/#push-results-to-github","title":"Push Results to GitHub","text":"<p>Fork the <code>mlperf-inference-results-scc24</code> branch of the repository URL at https://github.com/mlcommons/cm4mlperf-inference. </p> <p>Run the following command after replacing <code>--repo_url</code> with your GitHub fork URL.</p> <pre><code>pip install cmx4mlperf\ncr push,github,mlperf,inference,submission \\\n   --repo_url=https://github.com/&lt;myfork&gt;/cm4mlperf-inference \\\n   --repo_branch=mlperf-inference-results-scc24 \\\n   --commit_message=\"Results on system &lt;HW Name&gt;\" \\\n   --quiet\n</code></pre> <p>Once uploaded give a Pull Request to the origin repository. Github action will be running there and once  finished you can see your submitted results at https://docs.mlcommons.org/cm4mlperf-inference.</p>"},{"location":"install/","title":"Installation","text":"<p>We use MLCommons CM Automation framework to run MLPerf inference benchmarks.</p>"},{"location":"install/#cm-install","title":"CM Install","text":"<p>We have successfully tested CM on</p> <ul> <li>Ubuntu 18.x, 20.x, 22.x , 23.x, </li> <li>RedHat 8, RedHat 9, CentOS 8</li> <li>macOS</li> <li>Wndows 10, Windows 11</li> </ul> UbuntuRed HatmacOSWindows"},{"location":"install/#ubuntu-debian","title":"Ubuntu, Debian","text":"<pre><code>   sudo apt update &amp;&amp; sudo apt upgrade\n   sudo apt install python3 python3-pip python3-venv git wget curl\n</code></pre> <p>** Set up virtual env as it is recommended now before using any Python project:** <pre><code>   python3 -m venv cm\n   source cm/bin/activate\n</code></pre></p> <p>You can now install CM via PIP:</p> <pre><code>   python3 -m pip install cmind\n</code></pre> <p>You can check that CM is available by checking the <code>cm</code> command</p>"},{"location":"install/#red-hat","title":"Red Hat","text":"<pre><code>   sudo dnf update\n   sudo dnf install python3 python-pip git wget curl\n   python3 -m pip install cmind --user\n</code></pre>"},{"location":"install/#macos","title":"macOS","text":"<p>Note that CM currently does not work with Python installed from the Apple Store.  Please install Python via brew as described below.</p> <p>If <code>brew</code> package manager is not installed, please install it as follows (see details here): <pre><code>   /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre></p> <p>Don't forget to add brew to PATH environment as described in the end of the installation output.</p> <p>Then install python, pip, git and wget:</p> <pre><code>   brew install python3 git wget curl\n   python3 -m pip install cmind\n</code></pre>"},{"location":"install/#windows","title":"Windows","text":"<ul> <li>Configure Windows 10+ to support long paths from command line as admin:    <pre><code>   reg add \"HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\" /v LongPathsEnabled /t REG_DWORD /d 1 /f\n</code></pre> </li> <li>Download and install Git from git-for-windows.github.io.</li> <li>Configure Git to accept long file names: <code>git config --system core.longpaths true</code></li> <li>Download and install Python 3+ from www.python.org/downloads/windows.</li> <li>Don't forget to select option to add Python binaries to PATH environment!</li> <li> <p>Configure Windows to accept long fie names during Python installation!</p> </li> <li> <p>Install CM via PIP:</p> </li> </ul> <pre><code>   python -m pip install cmind\n</code></pre> <p>Note that we have reports   that CM does not work when Python was first installed from the Microsoft Store.  If CM fails to run, you can find a fix here.</p>"},{"location":"meetings/20240731/","title":"Topic","text":"<p>Syncing about the next steps for CM, CM4MLOps, CM4MLPerf, CM4ABTF, etc.</p>"},{"location":"meetings/20240731/#people","title":"People","text":"<ul> <li>Grigori Fursin</li> <li>Arjun Suresh</li> </ul>"},{"location":"meetings/20240731/#discussion","title":"Discussion","text":""},{"location":"meetings/20240731/#need-proper-attribution","title":"Need proper attribution","text":"<p>Need to agree on the common text in the CM documentation and CM, CM4MLOps and CM4MLPerf GitHub using these examples:</p> <ul> <li>https://github.com/spack/spack?tab=readme-ov-file#authors</li> <li> <p>https://cython.org (see Financial contributions section)</p> </li> <li> <p>Author/creator</p> </li> <li>Core developers: </li> <li>Contributors (from the community, MLCommons and the Automation and Reproducibility TaskForce):   See https://github.com/mlcommons/ck/blob/master/CONTRIBUTING.md .</li> <li>Sponsorship &amp; financial Contributions</li> </ul> <p>Should add to main GitHub and docs.mlcommons.org ...</p>"},{"location":"meetings/20240731/#removereduce-dependencies-on-non-mlcommons-github-repositories","title":"Remove/reduce dependencies on non-MLCommons GitHub repositories","text":"<p>At this moment, various non-MLCommons GATEOverflow GitHub repositories are used  in the official MLPerf workflows by default - that creates many possible legal issues for CM and MLPerf users.</p> <p>We should either move all such repositories to MLCommons  or, if it's not easily possible, create another neutral GitHub ID  such as mlcommons-aux with a clear governances and agreement with MLCommons to keep all dependencies in the MLCommons space.</p>"},{"location":"meetings/20240731/#improve-cm4mlops-package","title":"Improve cm4mlops package","text":"<p>Current cm4mlops package hides extra installation of various system dependencies  and CM repositories while using non-default branches and is difficult to debug if something goes wrong.</p> <p>A most standard way is to install cmind package and have a function to bootstrap cm4mlops with a proper control over the flow, CM repositories and branches that can be changed via flags.</p> <p>For example: <pre><code>pip install cmind\ncm bootstrap cm4mlperf\ncm bootstrap cm4mlops --branch=mlperf-inference\n...\n</code></pre></p> <p>That will perform the same functions as cm4mlops package but will be easy to debug and will have easy to trace errors that can be used in GitHub actions or other CI</p> <p>To be brainstormed further ...</p>"},{"location":"meetings/20240731/#coordinate-further-developments","title":"Coordinate further developments","text":"<ul> <li>Document the roadmap and responsibilities for Q3-Q4 2024</li> <li>Regular dev meetings (once a week or every two weeks)?</li> <li>Resume Discord channel discussions or mailing list (to be able to track discussions)?</li> </ul>"},{"location":"meetings/20240731/#cm4mlperf-inference-v41-v50-automation","title":"CM4MLPerf inference v4.1 &amp; v5.0 automation","text":"<ul> <li>Add CM for as many v4.1 submissions as possible to make it easier for everyone to reproduce results shortly after publication of results.</li> <li>Sync on the plans for inf v5.0 with MLCommons</li> </ul>"},{"location":"meetings/20240731/#cm4abtf-automation","title":"CM4ABTF automation","text":"<ul> <li>Sync on the next steps during next meetings</li> </ul>"},{"location":"meetings/20240731/#collaboration-with-croissant","title":"Collaboration with Croissant","text":"<ul> <li>Sync on the next steps during next meetings</li> </ul>"},{"location":"meetings/20240731/#testing-infrastructure-for-cm4mlops-and-cm4mlperf","title":"Testing infrastructure for CM4MLOps and CM4MLPerf","text":"<ul> <li>GitHub actions are not enough to test all dependencies and their versions for diverse hardware for CM-MLPerf workflows.   Brainstorm infrastructure for continuous testing (Grigori started prototyping some infrastructure).</li> </ul>"},{"location":"meetings/20240731/#optimize-mlperf-inference-reference-implementations","title":"Optimize MLPerf inference reference implementations","text":"<ul> <li>We need to add known optimizations to the MLPerf inference implementations</li> </ul>"},{"location":"meetings/20240731/#support-mlperf-training","title":"Support MLPerf training","text":"<ul> <li>We should start prototyping the unified CM interface and automation for MLPerf training and wrap existing MLCube tasks</li> </ul>"},{"location":"meetings/20240731/#prepare-tutorials","title":"Prepare tutorials","text":"<ul> <li>Sync on the tutorial about CM internals and scripts</li> <li>Sync on the tutorial for SCC'24</li> </ul>"},{"location":"meetings/20240731/#common-paper","title":"Common paper","text":"<ul> <li>Start preparing a common paper about CM on GitHub </li> </ul>"},{"location":"meetings/20240731/#collect-feedback-from-companies","title":"Collect feedback from companies","text":"<ul> <li>There were various discussions with MLCommons companies about using CM for reproducibility.   We need to collect and aggregate all the feedback in one place.</li> </ul>"},{"location":"meetings/20240731/#next-generation-of-cm","title":"Next generation of CM","text":"<p>Grigori started testing some ideas and prototyping the next generation of CM, CM4MLOps and CM4MLPerf  bsaed on 3 years of using CM to modularize and automate MLPerf and will share notes in the future dev meetings.</p>"},{"location":"meetings/20240731/#sync-with-mlcommons","title":"Sync with MLCommons","text":"<ul> <li>Prepare official CM page - should we do it with the MLPerf in v4.1 release?</li> <li>Prepare Press-release about CM with MLPerf inf v4.1 release?</li> <li>Where to host CM developments and discussions within MLCommons?</li> <li>Infra WG?</li> <li>Create a new official taskforce or WG on automation and reproducibility?</li> </ul>"},{"location":"meetings/20240808/","title":"Topic","text":"<p>Urgent tasks</p>"},{"location":"meetings/20240808/#people","title":"People","text":"<ul> <li>Grigori Fursin</li> <li>Arjun Suresh</li> </ul>"},{"location":"meetings/20240808/#tbd","title":"TBD","text":""},{"location":"meetings/20240808/#finish-proper-attribution","title":"Finish proper attribution","text":"<p>For CM, CM4MLOps and CM4ABTF</p>"},{"location":"meetings/20240808/#add-cm-init-and-release-cm","title":"Add cm init and release CM","text":""},{"location":"meetings/20240808/#check-default-mlperf-repos-mlcommons","title":"Check default MLPerf repos (MLCommons)","text":""},{"location":"meetings/20240808/#how-is-responding-at-discrod","title":"How is responding at Discrod","text":"<p>Anandhu ?</p>"},{"location":"misc/ML/","title":"Useful projects","text":"<ul> <li>PlotNeuralNet</li> </ul>"},{"location":"misc/MLOps/","title":"KB: MLOps and MLPerf","text":"<p>One of our goals is to automate the development, optimization and deployment of Pareto-efficient ML Systems in production.  The main challenge is how to support numerous and continuously changing software and hardware.  Here we collect related notes about MLPerf, MLOps, DevOps, inference engines, containers and production deployment of ML systems.</p>"},{"location":"misc/MLOps/#news-and-articles","title":"News and articles","text":"<ul> <li>March 2022: MLOps Is a Mess But That's to be Expected</li> <li>June 2022: Making ML models more portable and reproducible with open APIs, reusable best practices and MLOps</li> </ul>"},{"location":"misc/MLOps/#mlperf-inference-benchmark","title":"MLPerf inference benchmark","text":"<ul> <li>MLCommons website</li> </ul>"},{"location":"misc/MLOps/#inference-engines","title":"Inference engines","text":"<ul> <li>Nvidia Triton inference server</li> </ul>"},{"location":"misc/MLOps/#deployment-platforms","title":"Deployment platforms","text":"<ul> <li>Azure</li> <li>AWS</li> <li>Seldon Deploy</li> <li>About</li> <li>Architecture</li> </ul>"},{"location":"misc/MLOps/#ml-platforms","title":"ML platforms","text":"<ul> <li>HPE (press-release 2022-04-27)</li> <li>Shopify's Merlin</li> </ul>"},{"location":"misc/MLOps/#ml-workflow-and-automation-frameworks","title":"ML workflow and automation frameworks","text":"<ul> <li>AWS SageMaker</li> <li>Collective Knowledge</li> <li>CWL: common workflow language</li> <li>Kedro</li> <li>Kubeflow</li> <li>MLFlow</li> <li>Redun: \"yet another redundant workflow engine\"</li> <li>ZenML: MLOps framework to create reproducible pipelines</li> </ul>"},{"location":"misc/MLOps/#specifications","title":"Specifications","text":"<ul> <li>MLSpec</li> </ul>"},{"location":"misc/MLOps/#ml-exchange-formats","title":"ML exchange formats","text":"<ul> <li>ONNX</li> <li>NNEF (Khronos)</li> </ul>"},{"location":"misc/MLOps/#containers","title":"Containers","text":"<ul> <li>Docker</li> <li>Kubernetes</li> <li> <p>Singularity</p> </li> <li> <p>Misc:</p> <ul> <li>Containerizing Huggingface Transformers for GPU inference with Docker and FastAPI on AWS (TowardsDataScience, 20211004)</li> <li>FOSDEM\u201922 containers track</li> </ul> </li> </ul>"},{"location":"misc/MLOps/#ml-containers","title":"ML containers","text":"<ul> <li>\"COG\" from Replicate</li> <li>MLCube</li> </ul>"},{"location":"misc/MLOps/#ml-artifact-management","title":"ML artifact management","text":"<ul> <li>CM toolkit</li> </ul>"},{"location":"misc/MLOps/#load-testing-tools","title":"Load testing tools","text":"<ul> <li>MLPerf loadgen</li> <li>Locust - scalable user load testing tool written in Python</li> </ul>"},{"location":"misc/MLOps/#misc-tools","title":"Misc tools","text":"<ul> <li>rclone - rclone is a command-line program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors' web storage interfaces.</li> </ul>"},{"location":"misc/history/","title":"History","text":"<p>[ Back to index ]</p>"},{"location":"misc/history/#motivation","title":"Motivation","text":"<p>Collective Knowledge concept (CK) is motivated by our tedious experience reproducing  150+ Systems and Machine Learning papers and automating MLPerf benchmarks.</p> <p>We have spent many months communicating with researchers and developers to understand their technical reports, README files, ad-hoc scripts, tools, command lines, APIs, specifications, dependencies, data formats, models and data  to be able to reproduce their experiments  and reuse their artifacts across continuously changing software, hardware and data.</p> <p></p> <p></p>"},{"location":"misc/history/#the-1st-generation-of-the-ck-automation-meta-framework","title":"The 1st generation of the CK automation meta-framework","text":"<p>This experience motivated us to develop a simple and cross-platform meta-framework (Collective Knowledge) that can help researchers and engineers solve above problems by transforming their projects into a database of portable, reusable and customizable components.</p> <p>All such components provide a unified CLI, Python API and extensible JSON/YAML meta descriptions to existing artifacts,  native scripts and workflows to make them more interoperable, deterministic, reusable and reproducible  across continuously changing software and hardware.</p> <p></p> <p>We have donated the CK framework to the MLCommons foundation to benefit everyone after it was successfully validated by Qualcomm, Arm, General Motors, OctoML, Krai, HPE, Dell, Lenovo and other partners to enable collaborative and reproducible development, optimization and deployment of Pareto-efficient ML Systems in terms of accuracy, latency, throughput, energy, size and costs.</p> <p>After this approach was successfully validated by Qualcomm, Arm, General Motors, OctoML, Krai, HPE, Dell, Lenovo and other organizations to modularize and automate MLPerf benchmarks, we have donated our prototype to the MLCommons foundation  to continue developing it as a community effort.</p>"},{"location":"misc/history/#the-2nd-generation-of-the-ck-framework-aka-cm","title":"The 2nd generation of the CK framework (aka CM)","text":"<p>Collective Mind workflow automation meta-framework (CM aka CK2)  is the 2nd implementation  of the CK concept being developed by the open MLCommons taskforce on automation and reproducibility.</p> <p>This taskforce is using and enhancing CM to modularize ML and AI systems and automate their benchmarking,  optimization and deployment across continuously changing software, hardware and data.</p>"},{"location":"misc/history/#related-resources-and-references","title":"Related resources and references","text":"<ul> <li>Blog article about \"MLOps Is a Mess But That's to be Expected\" (March 2022)</li> <li>Journal article describing the CK concept</li> <li>\"Reproducing 150 Research Papers and Testing Them in the Real World\" (ACM TechTalk; Feb 2021)</li> <li>\"Automating MLPerf design space exploration and production deployment\" (HPCA'22)</li> <li>\"Collaboratively Benchmarking and Optimizing Deep Learning Implementations\" (General Motors; Jun 2017)</li> <li>MLOps projects, articles and tools</li> </ul>"},{"location":"misc/history/#acknowledgments","title":"Acknowledgments","text":"<p>We would like to thank MLCommons,  OctoML, all contributors  and collaborators for their support, fruitful discussions,  and useful feedback! See more acknowledgments in the CK journal article and our ACM TechTalk.</p>"},{"location":"misc/overview/","title":"Overview","text":"<p>[ Back to index ]</p>"},{"location":"misc/overview/#overview","title":"Overview","text":"<p>The Collective Knowledge project is motivated by our tedious experience  reproducing research papers on machine learning and systems and validating them in the real world.</p> <p>We have developed the Collective Knowledge concept (CK)  to provide a simple way to unify, manage, connect and reuse any artifacts, scripts, tools and workflows  on any platform with any software and hardware stack. </p> <p>CK is intended to help researchers and developers turn their scripts, artifacts and workflow into a database of portable, reusable, customizable and deterministic components with minimal effort and no changes to their projects.</p> <p>All such components have a simple, human-friendly and platform-independent CLI, Python API, JSON or YAML meta description, tags, and Unique ID automatically generated by CK.</p> <p>This approach allows users to automatically plug any ad-hoc scripts and artifacts  from the community into their projects, build systems, CI/CD tools, containers, Jupyter/Colab notebooks and any other technology.</p> <p>CK runtime system also helps users interconnect any scripts and artifacts  into portable workflows, applications and web-services. They can run natively or inside containers while automatically  adapting to any given software and hardware.</p> <p>Any output of CK components and workflows (CSV/XLS/JSON/YAML files, pre-processed data set, notes and optimized code) can be also stored  as CK components with all related CM dependencies. Such database-like organization of projects makes it easier for the community to re-run, reproduce and reuse research results.</p> <p>We have donated CK to the MLCommons foundation  to benefit everyone after it was successfully validated by Qualcomm, Arm, General Motors, OctoML, Krai, HPE, Dell, Lenovo and other organizations.</p> <p>The MLCommons has establshed an open open taskforce on automation and reproducibility to develop the 2nd generation of the CK framework (aka Collective Mind or CM).</p> <p>Here is the list of reusable CM components developed and shared by this taskforce: * CM automations * CM cross-platform scripts</p> <p>The community uses CM to modularize ML and AI systems and automate their benchmarking,  optimization and deployment across diverse and continuously changing software, hardware and data from the cloud to embedded devices.</p> <p>Feel free to join this open taskforce  to provide your feedback and participate in further community developments!</p> <p>\u00a9 2021-2022 MLCommons</p>"},{"location":"misc/overview/#legacy-mlcommons-ck-framework-ck1","title":"Legacy MLCommons CK framework (CK1)","text":"<p>The MLCommons CK (the 1st generation) was discontinued in summer 2022 after the stable release of the 2nd generation of this framework (Collective Mind aka CM or CK2). You can access this project here.</p>"},{"location":"misc/overview/#resources","title":"Resources","text":"<ul> <li>MLOps resources</li> <li>ML resources</li> </ul>"},{"location":"mlperf/","title":"Index","text":"<p>[ Back to CM documentation ]</p>"},{"location":"mlperf/#run-and-customize-mlperf-benchmarks-using-the-mlcommons-cm-automation-framework","title":"Run and customize MLPerf benchmarks using the MLCommons CM automation framework","text":"<p>This documentation explains how to compose, run, customize and extend MLPerf benchmarks  in a unified way across diverse models, data sets, software and hardware from different vendors  using MLCommons Collective Mind automation recipes:</p> <ul> <li>MLPerf inference benchmark</li> <li>MLPerf training benchmark (prototyping phase)</li> <li>MLPerf tiny benchmark (prototyping phase)</li> <li>MLPerf automotive (prototyping phase)</li> <li>MLPerf mobile (preparation phase)</li> <li>MLPerf client (preparation phase)</li> </ul> <p>Note that the MLCommons Task Force on Automation and Reproducibility  is preparing a GUI   to make it easier to run, customize, reproduce and compare  MLPerf benchmarks - please stay tuned for more details!</p> <p>Don't hesitate to get in touch via the public Discord server if you have questions or feedback!</p>"},{"location":"mlperf/inference/","title":"Index","text":"<p>[ Back to MLPerf benchmarks index ]</p>"},{"location":"mlperf/inference/#unified-interface-to-run-mlperf-inference-benchmarks","title":"Unified interface to run MLPerf inference benchmarks","text":"<p>Running the MLPerf inference benchmarks and preparing valid submissions  is not trivial.</p> <p>This guide explains how to automate all the steps required to prepare,  customize, run and extend MLPerf inference benchmarks across  diverse models, datasets, software and hardware using  the MLCommons Collective Mind automation framework (CM).</p> <p>CM makes it possible to compose modular benchmarks from portable and reusable automation recipes (CM scripts)  with a common interface and a human-friendly GUI. Such benchmarks attempt to automatically adapt to any software and hardware natively or inside a container with any Operating System.</p> <p>CM automation for MLPerf benchmarks is being developed by the MLCommons Task Force on Automation and Reproducibility  based on the feedback from MLCommons organizations while automating &gt;90% of all performance and power submissions in the v3.1 round.</p> <p>Don't hesitate to get in touch via public Discord server to get free help to run MLPerf benchmarks and submit valid results.</p> <p>Table of Contents:</p> <ul> <li>How to run existing MLPerf inference benchmarks?</li> <li>How to measure power?</li> <li>How to submit results?</li> <li>How CM automation works?</li> <li>How to debug CM automation recipes?</li> <li>How to add new implementations (models, frameworks, hardware)?</li> <li>How to run MLPerf inference benchmarks with non-reference models?</li> <li>How to run MLPerf inference benchmark via Docker?</li> <li>How to automate MLPerf experiments?</li> <li>How to visualize and compare MLPerf results?</li> <li>Current developments</li> <li>Acknowledgments</li> <li>Questions? Suggestions?</li> </ul>"},{"location":"mlperf/inference/#how-to-run-existing-mlperf-inference-benchmarks","title":"How to run existing MLPerf inference benchmarks?","text":"<ul> <li>Install MLCommons CM framework with automation recipes for AI benchmarks.</li> <li>Use this GUI    to generate CM commands to customize and run MLPerf inference benchmarks.</li> <li>Use some ready-to-use CM commands for the following models:</li> <li>ResNet50</li> <li>RetinaNet</li> <li>3D Unet</li> <li>RNNT</li> <li>Bert</li> <li>GPT-J</li> <li>LLAMA2 70B</li> <li>Stable Diffusion XL</li> <li>Check on-going reproducibility studies for MLPerf benchmarks.</li> <li>Participate in open submission and reproducibility challenges.</li> </ul>"},{"location":"mlperf/inference/#how-to-measure-power","title":"How to measure power?","text":"<p>Power measurement is optional for MLPerf inference benchmark submissions and is known to be very difficult to set up and run. However, if your system have a good power efficiency, it is great to showcase it and compare against other systems. That's why we fully automated power measurements for MLPerf inference benchmark in CM.</p> <p>You can follow this tutorial  to set up your power analyzer and connect it with your host platform.</p> <p>Note that the cTuning foundation  and cKnowledge.org have several power analyzers and can help   you test your MLPerf benchmark implementations.</p>"},{"location":"mlperf/inference/#how-to-submit-results","title":"How to submit results?","text":"<p>We provided a unified CM interface to run the following MLPerf inference benchmarks: 1. Language processing using Bert-Large model and Squad v1.1 dataset 2. Language processing using GPT-J model and CNN Daily Mail dataset 3. Image Classification using ResNet50 model and Imagenet-2012 dataset 4. Image Classification using variations of MobileNets and EfficientNets and Imagenet-2012 dataset 5. Object Detection using Retinanet model and OpenImages dataset 6. Speech Recognition using RNNT model and LibriSpeech dataset 7. Medical Imaging  using 3d-unet model and KiTS19 dataset 8. Recommendation using DLRMv2 model and Criteo multihot dataset</p> <p>All seven benchmarks can participate in the datacenter category. All seven benchmarks except Recommendation can participate in the edge category. </p> <p>Note that <code>language processing</code> and <code>medical imaging</code> benchmarks must achieve a higher accuracy of at least <code>99.9%</code> of the FP32 reference model in comparison with <code>99%</code> default accuracy requirement for all other models.</p> <p>The <code>recommendation</code> benchmark has a high-accuracy variant only. Currently, we are not supporting the <code>recommendation</code> benchmark in CM  because we did not have a required high-end server for testing. </p> <p>After running MLPerf inference benchmarks and collecting results via CM, you can follow this guide to prepare your submission.</p>"},{"location":"mlperf/inference/#how-cm-automation-works","title":"How CM automation works?","text":"<p>Collective Mind was developed based on the feedback from MLCommons organizations - it simply wraps numerous native scripts for all steps required to prepare, build and run applications and benchmarks into unified and reusable automation recipes  with human-friendly tags, a common API, YAML/JSON meta descriptions and simple Python code. CM makes it easy to chain together different automation recipes into powerful workflows that automatically prepare all environment variables and commands lines on any software, hardware and operating system without the need for users  to learn new tools and languages.</p> <p>We suggest you to explore this automation recipe and check this CM README  and CM Getting Started Guide for more details about CM.</p> <p>Common CM interface and automation for MLPerf inference benchmark is implemented using the \"run-mlperf-inference-app\" CM script described by this YAML meta-description and customize.py.</p> <p>This script can be configured using this GUI and will run other CM scripts that set up different MLPerf inference implementations from different vendors:</p> <ul> <li>CM script \"app-mlperf-inference-reference\" to run MLCommons reference implementation</li> <li>CM script \"app-mlperf-inference-nvidia\" to run Nvidia implementation</li> <li>CM script \"reproduce-mlperf-inference-intel\" to run Intel implementation</li> <li>CM script \"reproduce-mlperf-inference-qualcomm\" to run Qualcomm implementation</li> <li>CM script \"app-mlperf-inference-cpp\" to run MLCommons ONNX C++ implementation</li> <li>CM script \"app-mlperf-inference-tflite-cpp\" to run TFLite C++ implementation</li> </ul> <p>When running above scripts, CM will cache the output (MLPerf loadgen, downloaded models, preprocessed data sets, installed tools)  that will be reused across different scripts. You can see the content of the cache at any time as follows: <pre><code>cm show cache\n</code></pre></p> <p>You can clean the cache and start from scratch as follows: <pre><code>cm rm cache -f\n</code></pre></p>"},{"location":"mlperf/inference/#how-to-debug-cm-automation-recipes","title":"How to debug CM automation recipes?","text":"<p>Since CM language uses native OS scripts with python wrappers, it is relatively straightforward to debug it using your existing tools.</p> <p>You can add <code>--debug</code> flag to your CM command line when running MLPerf benchmarks to open a shell with all MLPerf environment variables prepared to  run and debug the final MLPerf loadgen tool manually.</p> <p>You can also use GDB by adding environment variable <code>--env.CM_RUN_PREFIX=\"gdb --args \"</code> to the CM command line.</p> <p>Please check this documentation for more details.</p>"},{"location":"mlperf/inference/#how-to-add-new-implementations-models-frameworks-hardware","title":"How to add new implementations (models, frameworks, hardware)?","text":"<p>If you do not yet have your own implementation, we suggest you to run already existing implementation  via CM and then modify loadgen and inference sources in CM cache to develop your own implementation: <pre><code>cm show cache --tags=mlperf,loadgen\ncm show cache --tags=get,git,inference,repo\n</code></pre></p> <p>You can then push your changes to your own clone of the MLPerf inference repo; copy any of above CM scripts for similar implementation; update tags in <code>_cm.yaml</code>; and add your implementation tags to the meta description of the main CM interface  for the MLPerf inference benchmark here.</p> <p>If you need help, don't hesitate to contact us via public Discord server.</p> <p>It is in our plans to add a tutorial how to develop MLPerf inference benchmarks  and add your implementations to CM.</p>"},{"location":"mlperf/inference/#how-to-run-mlperf-inference-benchmarks-with-non-reference-models","title":"How to run MLPerf inference benchmarks with non-reference models?","text":"<p>If you want to benchmark some ML models using MLPerf loadgen without accuracy,  you can use our universal Python loadgen automation for ONNX models. You can benchmark Hugging Face ONNX models or your own local models.</p> <p>If you want to benchmark ML models with the MLPerf inference benchmark and submit results to open division, you need to make sure that they are trained on the same data sets as reference MLPerf models and that their input/output is the same as MLPerf reference models. In such case, you can use the following CM flags  to substitute reference model in MLPerf.</p> <ul> <li><code>--env.CM_MLPERF_CUSTOM_MODEL_PATH = {full path to the local model}</code></li> <li><code>--env.CM_ML_MODEL_FULL_NAME = {some user-friendly model name for submission}</code></li> </ul> <p>Check these 2 examples for more details: * Run custom Bert-family ONNX models with MLPerf reference implementation * Run multiple DeepSparse Zoo BERT models via MLPerf</p>"},{"location":"mlperf/inference/#how-to-run-mlperf-inference-benchmark-via-docker","title":"How to run MLPerf inference benchmark via Docker?","text":"<p>If a given vendor implementation uses Docker (Intel, Nvidia, Qualcomm), CM will build required container and run MLPerf inference benchmark automatically.</p> <p>CM also has an option to run native MLPerf inference benchmark implementations inside automatically-generated container by substituting <code>cm run script</code> command with <code>cm docker script</code> command. </p> <p>We plan to share snapshots of different MLPerf inference benchmarks via Docker Hub  during our reproducibility studies to help the community benchmark their own systems using MLPerf inference benchmark containers.</p>"},{"location":"mlperf/inference/#how-to-automate-mlperf-experiments","title":"How to automate MLPerf experiments?","text":"<p>We have developed experiment automation in CM to run multiple experiments, automatically explore multiple parameters,  record results and reproduce them by the workgroup.</p> <p>Please check this documentation for more details.</p>"},{"location":"mlperf/inference/#how-to-visualize-and-compare-mlperf-results","title":"How to visualize and compare MLPerf results?","text":"<p>You can pull all past MLPerf results in the CM format, import your current experiments under preparation and visualize results  with derived metrics on your system using the Collective Knowledge Playground as follows:</p> <pre><code>cm pull repo mlcommons@cm4mlperf-results\ncmr \"get git repo _repo.https://github.com/ctuning/mlperf_inference_submissions_v3.1\" \\\n    --env.CM_GIT_CHECKOUT=main \\\n    --extra_cache_tags=mlperf-inference-results,community,version-3.1\ncmr \"gui _graph\"\n</code></pre> <p>You can see example of this visualization GUI online.</p>"},{"location":"mlperf/inference/#current-developments","title":"Current developments","text":"<ul> <li>Current reproducibility studies for MLPerf benchmarks.</li> <li>Current CM coverage to run and reproduce MLPerf inference benchmarks.</li> <li>Development version of the modular MLPerf C++ inference implementation.</li> <li>Development version of the the reference network implementation with CM interface for BERT model.</li> </ul>"},{"location":"mlperf/inference/#acknowledgments","title":"Acknowledgments","text":"<p>Collective Mind is an open community project  to modularize AI benchmarks and provide a common interface to run them across diverse models, data sets, software and hardware -  we would like to thank all our great contributors for their feedback, support and extensions!</p>"},{"location":"mlperf/inference/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Please check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/README_a100/","title":"README a100","text":"<p>Moved here.</p>"},{"location":"mlperf/inference/Submission/","title":"Submission","text":"<p>[ Back to MLPerf inference benchmark index ]</p>"},{"location":"mlperf/inference/Submission/#generate-actual-submission-tree","title":"Generate actual submission tree","text":"<p>You should use the master branch of MLCommons inference repo for the submission checker:</p> <pre><code>cmr \"generate inference submission\" \\\n--clean \\\n--preprocess_submission=yes \\\n--run-checker \\\n--submitter=CTuning \\\n--tar=yes \\\n--env.CM_TAR_OUTFILE=submission.tar.gz \\\n--division=open \\\n--category=edge \\\n--env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n--quiet\n</code></pre> <ul> <li>Use <code>--division=closed</code> to generate a closed division result</li> <li>Use <code>--category=datacenter</code> to generate results for datacenter category</li> <li>Use <code>--hw_notes_extra</code> option to add your name to the notes like <code>--hw_notes_extra=\"Result taken by NAME\"</code></li> <li>Use <code>--hw_name=\"My system name\"</code> to give a meaningful system name. Examples can be seen here</li> <li>Use <code>--submitter=&lt;Your name&gt;</code> if your organization is an official MLCommons member and would like to submit under your organization</li> </ul> <p>The above command should generate \"submission.tar.gz\" if there are no submission checker issues and you can upload it to the </p>"},{"location":"mlperf/inference/Submission/#upload-your-results","title":"Upload your results","text":""},{"location":"mlperf/inference/Submission/#push-the-results-to-github-repo-only-if-you-are-submitting-public-results-under-ctuning","title":"Push the results to GitHub repo (Only if you are submitting public results under cTuning)","text":"<ol> <li>First, create a fork of this repo.</li> <li>If you have not set up GIT config already please do     <pre><code>git config --global user.name \"[YOUR NAME]\"\ngit config --global user.email \"[YOUR EMAIL]\"\n</code></pre></li> <li>Then run the following command after replacing <code>--repo_url</code> with your fork URL.</li> </ol> <pre><code>cmr \"push github mlperf inference submission\" \\\n--repo_url=https://github.com/ctuning/mlperf_inference_submissions_v4.0 \\\n--commit_message=\"Results on &lt;HW name&gt; added by &lt;Name&gt;\" \\\n--quiet\n</code></pre> <p>Create a PR to the cTuning repo</p>"},{"location":"mlperf/inference/Submission/#upload-the-tarball-to-mlperf-ui-only-if-your-organization-is-an-official-mlcommons-member","title":"Upload the tarball to MLPerf UI (Only if your organization is an official MLCommons member)","text":"<p>You can upload the <code>submission.tar.gz</code> file generated by the previous command to the submission UI.</p>"},{"location":"mlperf/inference/Submission/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/3d-unet/","title":"Index","text":"<p>[ Back to MLPerf inference benchmarks index ]</p>"},{"location":"mlperf/inference/3d-unet/#mlperf-inference-benchmark","title":"MLPerf inference benchmark","text":""},{"location":"mlperf/inference/3d-unet/#medical-imaging-with-3d-u-net","title":"Medical imaging with 3D U-Net","text":""},{"location":"mlperf/inference/3d-unet/#notes","title":"Notes","text":"<p>3d-unet has two variants - <code>3d-unet-99</code> and <code>3d-unet-99.9</code> where the <code>99</code> and <code>99.9</code> specifies the required accuracy constraint  with respect to the reference floating point model. Both models can be submitter under edge as well as datacenter category.</p> <p>Please check MLPerf inference GitHub for more details.</p>"},{"location":"mlperf/inference/3d-unet/#run-using-the-mlcommons-cm-framework","title":"Run using the MLCommons CM framework","text":"<p>From Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/3d-unet/#a-few-ready-to-use-cm-commands","title":"A few ready-to-use CM commands","text":"<p>Install MLCommons CM automation framework with automation recipes for MLPerf as described here.</p> <p>The following guides explain how to run different implementations of this benchmark via CM:</p> <ul> <li>MLCommons Reference implementation in Python</li> <li>NVIDIA optimized implementation (GPU)</li> </ul>"},{"location":"mlperf/inference/3d-unet/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/3d-unet/README_nvidia/","title":"README nvidia","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/3d-unet/README_nvidia/#build-nvidia-docker-container-from-31-inference-round","title":"Build Nvidia Docker Container (from 3.1 Inference round)","text":"<pre><code>cm docker script --tags=build,nvidia,inference,server\n</code></pre>"},{"location":"mlperf/inference/3d-unet/README_nvidia/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":"<p>Note: from Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/3d-unet/README_nvidia/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cmr \"run-mlperf inference _find-performance _all-scenarios\" \\\n--model=3d-unet-99 --implementation=nvidia-original --device=cuda --backend=tensorrt \\\n--category=edge --division=open --quiet\n</code></pre> * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--category=datacenter</code> to run datacenter scenarios * Use <code>--model=3d-unet-99.9</code> to run the high accuracy model</p>"},{"location":"mlperf/inference/3d-unet/README_nvidia/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cmr \"run-mlperf inference _submission _all-scenarios\" --model=3d-unet-99 \\\n--device=cuda --implementation=nvidia-original --backend=tensorrt \\\n--execution-mode=valid --category=edge --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, and <code>--singlestream_target_latency</code> can be used to override the determined performance numbers</li> </ul>"},{"location":"mlperf/inference/3d-unet/README_nvidia/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/3d-unet/README_nvidia/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/3d-unet/README_nvidia/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>CM automation for Nvidia's MLPerf inference implementation was developed by Arjun Suresh and Grigori Fursin.</li> <li>Nvidia's MLPerf inference implementation was developed by Zhihan Jiang, Ethan Cheng, Yiheng Zhang and Jinho Suh.</li> </ul>"},{"location":"mlperf/inference/3d-unet/README_reference/","title":"README reference","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/3d-unet/README_reference/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":"<p>Note: from Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/3d-unet/README_reference/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cmr \"run-mlperf inference _find-performance _all-scenarios\" \\\n--model=3d-unet-99.9 --implementation=reference --device=cpu --backend=onnxruntime \\\n--category=edge --division=open --quiet\n</code></pre> * Use <code>--device=cuda</code> to run the inference on Nvidia GPU * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--category=datacenter</code> to run datacenter scenarios * Use <code>--backend=pytorch</code> and <code>backend=tf</code> to use pytorch and tensorflow backends respectively * Use <code>--model=3d-unet-99</code> to run the low accuracy constraint 3d-unet-99 model. But since we are running the fp32 model, this is redundant and instead, we can reuse the results of 3d-unet-99.9 for 3d-unet-99</p>"},{"location":"mlperf/inference/3d-unet/README_reference/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cmr \"run-mlperf inference _submission _all-scenarios\" --model=3d-unet-99.9 \\\n--device=cpu --implementation=reference --backend=onnxruntime \\\n--execution-mode=valid --category=edge --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, and <code>--singlestream_target_latency</code> can be used to override the determined performance numbers</li> </ul>"},{"location":"mlperf/inference/3d-unet/README_reference/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/3d-unet/README_reference/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/all/README_nvidia_4090/","title":"README nvidia 4090","text":"<p>[ Back to MLPerf inference benchmarks index ]</p> <p>Note: from Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/all/README_nvidia_4090/#mlperf-inference-v31","title":"MLPerf inference v3.1","text":"<p>You can benchmark all models using Nvidia MLPerf inference implementation in one go using this CM command: <pre><code>cmr \"benchmark any _phoenix\"\n</code></pre></p> <p>This CM command tested for MLPerf inference v3.1.</p>"},{"location":"mlperf/inference/all/README_nvidia_a100/","title":"README nvidia a100","text":"<p>[ Back to MLPerf inference benchmarks index ]</p> <p>Note: from Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/all/README_nvidia_a100/#mlperf-inference-benchmark","title":"MLPerf inference benchmark","text":"<p>CM run commands to run MLPerf inference with main models  on Nvidia A100, SXM Edge system using Nvidia implementation.</p>"},{"location":"mlperf/inference/all/README_nvidia_a100/#bert-99","title":"Bert-99","text":""},{"location":"mlperf/inference/all/README_nvidia_a100/#quick-performance-test","title":"Quick performance test","text":"<pre><code>cmr \"run-mlperf inference _performance-only\"  \\\n--model=bert-99 --implementation=nvidia-original \\\n--device=cuda --backend=tensorrt --category=edge \\\n--division=open --quiet --scenario=Offline --execution-mode=fast \\\n--target_qps=3560 --rerun --gpu_name=a100 \\\n--adr.nvidia-harness.tags=_sxm \\\n--results_dir=$HOME/results_dir\n</code></pre>"},{"location":"mlperf/inference/all/README_nvidia_a100/#full-run","title":"Full run","text":"<p>This will do performance+accuracy+compliance for singlestream+offline scenarios. Please change the <code>target_qps</code> to the actual output value from the previous command.</p> <pre><code>cmr \"run-mlperf inference _submission _all-scenarios\"  \\\n--model=bert-99 --implementation=nvidia-original \\\n--device=cuda --backend=tensorrt --category=edge \\\n--division=open --quiet --execution-mode=valid \\\n--target_qps=3560 --rerun --gpu_name=a100 \\\n--adr.nvidia-harness.tags=_sxm \\\n--results_dir=$HOME/results_dir\n</code></pre>"},{"location":"mlperf/inference/all/README_nvidia_a100/#resnet50","title":"Resnet50","text":""},{"location":"mlperf/inference/all/README_nvidia_a100/#quick-performance-test_1","title":"Quick performance test","text":"<p>Imagenet dataset must be downloaded separately and detected in cm using  <pre><code>cmr \"get dataset original imagenet _full\" --input=\"&lt;Path to imagenet dir containing 50000 validation images&gt;\"\n</code></pre></p> <pre><code>cmr \"run-mlperf inference _performance-only\"  \\\n--model=resnet50 --implementation=nvidia-original \\\n--device=cuda --backend=tensorrt --category=edge \\\n--division=open --quiet --scenario=Offline --execution-mode=fast \\\n--target_qps=43000 --rerun --gpu_name=a100 \\\n--adr.nvidia-harness.tags=_sxm \\\n--results_dir=$HOME/results_dir\n</code></pre>"},{"location":"mlperf/inference/all/README_nvidia_a100/#full-run_1","title":"Full run","text":"<p>This will do performance+accuracy+compliance for singlestream+offline scenarios. Please change the <code>target_qps</code> to the actual output value from the previous command.</p> <pre><code>cmr \"run-mlperf inference _submission _all-scenarios\"  \\\n--model=resnet50 --implementation=nvidia-original \\\n--device=cuda --backend=tensorrt --category=edge \\\n--division=open --quiet --execution-mode=valid \\\n--target_qps=43000 --rerun --gpu_name=a100 \\\n--adr.nvidia-harness.tags=_sxm \\\n--results_dir=$HOME/results_dir\n</code></pre>"},{"location":"mlperf/inference/all/README_nvidia_a100/#retinanet","title":"Retinanet","text":""},{"location":"mlperf/inference/all/README_nvidia_a100/#quick-performance-test_2","title":"Quick performance test","text":"<pre><code>cmr \"run-mlperf inference _performance-only\"  \\\n--model=retinanet --implementation=nvidia-original \\\n--device=cuda --backend=tensorrt --category=edge \\\n--division=open --quiet --scenario=Offline --execution-mode=fast \\\n--target_qps=715 --rerun --gpu_name=a100 \\\n--adr.nvidia-harness.tags=_sxm \\\n--results_dir=$HOME/results_dir\n</code></pre>"},{"location":"mlperf/inference/all/README_nvidia_a100/#full-run_2","title":"Full run","text":"<p>This will do performance+accuracy+compliance for singlestream+offline scenarios. Please change the <code>target_qps</code> to the actual output value from the previous command.</p> <pre><code>cmr \"run-mlperf inference _submission _all-scenarios\"  \\\n--model=retinanet --implementation=nvidia-original \\\n--device=cuda --backend=tensorrt --category=edge \\\n--division=open --quiet --execution-mode=valid \\\n--target_qps=715 --rerun --gpu_name=a100 \\\n--adr.nvidia-harness.tags=_sxm \\\n--results_dir=$HOME/results_dir\n</code></pre>"},{"location":"mlperf/inference/all/README_nvidia_a100/#rnnt","title":"RNNT","text":""},{"location":"mlperf/inference/all/README_nvidia_a100/#quick-performance-test_3","title":"Quick performance test","text":"<pre><code>cmr \"run-mlperf inference _performance-only\"  \\\n--model=rnnt --implementation=nvidia-original \\\n--device=cuda --backend=tensorrt --category=edge \\\n--division=open --quiet --scenario=Offline --execution-mode=fast \\\n--target_qps=14000 --rerun --gpu_name=a100 \\\n--adr.nvidia-harness.tags=_sxm \\\n--results_dir=$HOME/results_dir\n</code></pre>"},{"location":"mlperf/inference/all/README_nvidia_a100/#full-run_3","title":"Full run","text":"<p>This will do performance+accuracy+compliance for singlestream+offline scenarios. Please change the <code>target_qps</code> to the actual output value from the previous command.</p> <pre><code>cmr \"run-mlperf inference _submission _all-scenarios\"  \\\n--model=rnnt --implementation=nvidia-original \\\n--device=cuda --backend=tensorrt --category=edge \\\n--division=open --quiet --execution-mode=valid \\\n--target_qps=14000 --rerun --gpu_name=a100 \\\n--adr.nvidia-harness.tags=_sxm \\\n--results_dir=$HOME/results_dir\n</code></pre>"},{"location":"mlperf/inference/all/README_nvidia_a100/#3d-unet","title":"3d-unet","text":""},{"location":"mlperf/inference/all/README_nvidia_a100/#quick-performance-test_4","title":"Quick performance test","text":"<pre><code>cmr \"run-mlperf inference _performance-only\"  \\\n--model=3d-unet-99 --implementation=nvidia-original \\\n--device=cuda --backend=tensorrt --category=edge \\\n--division=open --quiet --scenario=Offline --execution-mode=fast \\\n--target_qps=3.7 --rerun --gpu_name=a100 \\\n--adr.nvidia-harness.tags=_sxm \\\n--results_dir=$HOME/results_dir\n</code></pre>"},{"location":"mlperf/inference/all/README_nvidia_a100/#full-run_4","title":"Full run","text":"<p>This will do performance+accuracy+compliance for singlestream+offline scenarios. Please change the <code>target_qps</code> to the actual output value from the previous command.</p> <pre><code>cmr \"run-mlperf inference _submission _all-scenarios\"  \\\n--model=3d-unet-99 --implementation=nvidia-original \\\n--device=cuda --backend=tensorrt --category=edge \\\n--division=open --quiet --execution-mode=valid \\\n--target_qps=3.7 --rerun --gpu_name=a100 \\\n--adr.nvidia-harness.tags=_sxm \\\n--results_dir=$HOME/results_dir\n</code></pre> <p>Once all 5 model results are done, please follow Submission to generate the required submission. </p>"},{"location":"mlperf/inference/all/README_nvidia_a100/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/bert/","title":"Index","text":"<p>[ Back to MLPerf inference benchmarks index ]</p>"},{"location":"mlperf/inference/bert/#mlperf-inference-benchmark","title":"MLPerf inference benchmark","text":""},{"location":"mlperf/inference/bert/#language-processing-with-bert","title":"Language processing with BERT","text":""},{"location":"mlperf/inference/bert/#notes","title":"Notes","text":"<p>Bert has two variants - <code>bert-99</code> and <code>bert-99.9</code> where the <code>99</code> and <code>99.9</code> specifies the required accuracy constraint  with respect to the reference floating point model. <code>bert-99.9</code> model is applicable only on a datacenter system.</p> <p>In the edge category, bert-99 has Offline and SingleStream scenarios and in the datacenter category,  both <code>bert-99</code> and <code>bert-99.9</code> have Offline and Server scenarios.</p> <p>Please check MLPerf inference GitHub for more details.</p>"},{"location":"mlperf/inference/bert/#run-using-the-mlcommons-cm-framework","title":"Run using the MLCommons CM framework","text":"<p>From Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/bert/#a-few-ready-to-use-cm-commands","title":"A few ready-to-use CM commands","text":"<p>Install MLCommons CM automation framework with automation recipes for MLPerf as described here.</p> <p>The following guides explain how to run different implementations of this benchmark via CM:</p> <ul> <li>MLCommons reference implementation in Python (CPU &amp; GPU)</li> <li>NVIDIA optimized implementation (GPU)</li> <li>Intel optimized implementation (CPU)</li> <li>Qualcomm optimized implementation (QAIC)</li> <li>DeepSparse implementation (CPU: x64, Arm64)</li> </ul>"},{"location":"mlperf/inference/bert/#tutorials","title":"Tutorials","text":"<ul> <li>2023: Tutorial to run MLPerf inference benchmark with reference and deepsparse implementation and prepare submission</li> </ul>"},{"location":"mlperf/inference/bert/#a-few-example-scripts","title":"A few example scripts","text":"<ul> <li>Run custom ONNX models with MLPerf reference implementation</li> <li>Run multiple DeepSparse Zoo models via MLPerf</li> </ul>"},{"location":"mlperf/inference/bert/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/bert/README_deepsparse/","title":"README deepsparse","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/bert/README_deepsparse/#bert-large","title":"BERT-Large","text":""},{"location":"mlperf/inference/bert/README_deepsparse/#bert-99-obert-large-offline-deepsparse","title":"BERT-99%: oBERT-Large Offline - DeepSparse","text":"<pre><code>cm run script --tags=run,mlperf,inference,run-mlperf,_submission  \\\n   --adr.python.version_min=3.8 \\\n   --implementation=reference \\\n   --model=bert-99 \\\n   --precision=int8 \\\n   --backend=deepsparse \\\n   --device=cpu \\\n   --scenario=Offline \\\n   --mode=performance \\\n   --execution_mode=valid \\\n   --adr.mlperf-inference-implementation.max_batchsize=384 \\\n   --offline_target_qps=20 \\\n   --results_dir=$HOME/results_dir \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni\n</code></pre>"},{"location":"mlperf/inference/bert/README_deepsparse/#bert-99-mobilebert-offline","title":"BERT-99%: MobileBERT Offline","text":"<pre><code>cm run script --tags=run,mlperf,inference,run-mlperf,_submission  \\\n   --adr.python.name=mlperf \\\n   --adr.python.version_min=3.8 \\\n   --implementation=reference \\\n   --model=bert-99 \\\n   --precision=int8 \\\n   --backend=deepsparse \\\n   --device=cpu \\\n   --scenario=Offline \\\n   --mode=performance \\\n   --execution_mode=valid \\\n   --adr.mlperf-inference-implementation.max_batchsize=384 \\\n   --offline_target_qps=20 \\\n   --results_dir=$HOME/results_dir \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni \\\n   --env.DEEPSPARSE_SEQLENS=\"64,128,192,256,384\"\n</code></pre>"},{"location":"mlperf/inference/bert/README_deepsparse/#bert-999-mobilebert-offline-deepsparse","title":"BERT-99.9%: MobileBERT Offline - DeepSparse","text":"<pre><code>cm run script --tags=run,mlperf,inference,run-mlperf,_submission  \\\n   --adr.python.version_min=3.8 \\\n   --implementation=reference \\\n   --compliance=no \\\n   --model=bert-99 \\\n   --precision=int8 \\\n   --backend=deepsparse \\\n   --device=cpu \\\n   --scenario=Offline \\\n   --mode=performance \\\n   --execution_mode=valid \\\n   --adr.mlperf-inference-implementation.max_batchsize=384 \\\n   --offline_target_qps=20 \\\n   --results_dir=$HOME/results_dir \\\n   --env.DEEPSPARSE_SEQLENS=\"64,128,192,256,384\" \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"mlperf/inference/bert/README_deepsparse/#resnet50","title":"ResNet50","text":""},{"location":"mlperf/inference/bert/README_deepsparse/#resnet50-offline-deepsparse","title":"ResNet50 Offline - DeepSparse","text":"<pre><code>cm run script --tags=run,mlperf,inference,run-mlperf,_submission  \\\n   --adr.python.version_min=3.8 \\\n   --implementation=reference \\\n   --model=resnet50 \\\n   --precision=int8 \\\n   --backend=deepsparse \\\n   --device=cpu \\\n   --scenario=Offline \\\n   --mode=performance \\\n   --execution_mode=valid \\\n   --adr.imagenet-preprocessed.tags=_pytorch \\\n   --adr.mlperf-inference-implementation.dataset=imagenet_pytorch \\\n   --adr.mlperf-inference-implementation.model=zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned85_quant-none-vnni \\\n   --adr.mlperf-inference-implementation.max_batchsize=16 \\\n   --adr.mlperf-inference-implementation.num_threads=48 \\\n   --results_dir=$HOME/results_dir \\\n   --env.DEEPSPARSE_NUM_STREAMS=24 \\\n   --env.ENQUEUE_NUM_THREADS=2 \\\n   --offline_target_qps=204\n</code></pre>"},{"location":"mlperf/inference/bert/README_deepsparse/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/bert/README_deepsparse/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/bert/README_intel/","title":"README intel","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/bert/README_intel/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":"<p>We use <code>cm docker</code> to run the Intel implementation to avoid compilation problems with any host OS dependencies. </p>"},{"location":"mlperf/inference/bert/README_intel/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cm docker script --tags=run-mlperf,inference,_find-performance \\\n--scenario=Offline --model=bert-99 --implementation=intel-original --backend=pytorch \\\n--category=datacenter --division=open --quiet\n</code></pre> * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode)</p>"},{"location":"mlperf/inference/bert/README_intel/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cm docker script --tags=run-mlperf,inference,_submission,_all-scenarios \\\n--model=bert-99 --implementation=intel-original --backend=pytorch \\\n--category=datacenter --division=open --quiet\n</code></pre>"},{"location":"mlperf/inference/bert/README_intel/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/bert/README_intel/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/bert/README_nvidia/","title":"README nvidia","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/bert/README_nvidia/#build-nvidia-docker-container-from-31-inference-round","title":"Build Nvidia Docker Container (from 3.1 Inference round)","text":"<pre><code>cm docker script --tags=build,nvidia,inference,server\n</code></pre>"},{"location":"mlperf/inference/bert/README_nvidia/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/bert/README_nvidia/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cmr \"run-mlperf inference _find-performance\" --scenario=Offline \\\n--model=bert-99 --implementation=nvidia-original --device=cuda --backend=tensorrt \\\n--category=edge --division=open --quiet\n</code></pre> * Use <code>--model=bert-99.9</code> to run the high-accuracy model (only for datacenter) * Use <code>--rerun</code> to force a rerun even when result files (from a previous run) exist</p>"},{"location":"mlperf/inference/bert/README_nvidia/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cmr \"run-mlperf inference _submission _all-scenarios\" --model=bert-99 \\\n--device=cuda --implementation=nvidia-original --backend=tensorrt \\\n--execution-mode=valid --category=edge --division=open --quiet\n</code></pre> <ul> <li>Use <code>--category=datacenter</code> to run datacenter scenarios (only for bert-99.9)</li> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code>, and <code>--singlestream_target_latency</code> can be used to pass in the performance numbers</li> </ul>"},{"location":"mlperf/inference/bert/README_nvidia/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/bert/README_nvidia/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/bert/README_nvidia/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>CM automation for Nvidia's MLPerf inference implementation was developed by Arjun Suresh and Grigori Fursin.</li> <li>Nvidia's MLPerf inference implementation was developed by Zhihan Jiang, Ethan Cheng, Yiheng Zhang and Jinho Suh.</li> </ul>"},{"location":"mlperf/inference/bert/README_qualcomm/","title":"README qualcomm","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/bert/README_qualcomm/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/bert/README_qualcomm/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cmr \"run-mlperf inference _find-performance _all-scenarios\" \\\n--model=bert-99 --implementation=qualcomm --device=qaic --backend=glow \\\n--category=edge --division=open --quiet\n</code></pre> * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--category=datacenter</code> to run datacenter scenarios (only for bert-99.9) * Use <code>--model=bert-99.9</code> to run the high-accuracy model (only for datacenter) * Use <code>--rerun</code> to force a rerun even when result files (from a previous run) exist</p>"},{"location":"mlperf/inference/bert/README_qualcomm/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cmr \"run-mlperf inference _submission _all-scenarios\" --model=bert-99 \\\n--device=qaic --implementation=qualcomm --backend=qaic \\\n--execution-mode=valid --category=edge --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code>, and <code>--singlestream_target_latency</code> can be used to override the determined performance numbers</li> </ul>"},{"location":"mlperf/inference/bert/README_qualcomm/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/bert/README_qualcomm/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/bert/README_reference/","title":"README reference","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/bert/README_reference/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/bert/README_reference/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_all-scenarios \\\n--model=bert-99 --implementation=reference --device=cpu --backend=onnxruntime \\\n--category=edge --division=open --quiet --rerun\n</code></pre> * Use <code>--device=cuda</code> to run the inference on Nvidia GPU and <code>--device=rocm</code> to run on AMD GPUs * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--category=datacenter</code> to run datacenter scenarios * Use <code>--backend=pytorch</code> and <code>--backend=tf</code> to use the pytorch and tensorflow backends respectively. <code>--backend=deepsparse</code> will run the sparse int8 model using deepsparse backend (not allowed to be submitted under closed division). * Use <code>--model=bert-99.9</code> to run the high accuracy constraint bert-99 model. But since we are running the fp32 model, this is redundant and instead, we can reuse the results of bert-99 for bert-99.9</p>"},{"location":"mlperf/inference/bert/README_reference/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_submission,_all-scenarios --model=bert-99 \\\n--device=cpu --implementation=reference --backend=onnxruntime \\\n--execution-mode=valid --results_dir=$HOME/results_dir \\\n--category=edge --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code>, and <code>--singlestream_target_latency</code> can be used to override the determined performance numbers</li> <li><code>--rerun</code> flag can be used to force a rerun even when a valid result exists in the results_dir</li> </ul>"},{"location":"mlperf/inference/bert/README_reference/#populate-the-readme-files-describing-your-submission","title":"Populate the README files describing your submission","text":"<pre><code>cmr \"run-mlperf inference _populate-readme _all-scenarios\" \\\n--model=bert-99 --device=cpu --implementation=reference --backend=onnxruntime \\\n--execution-mode=valid --results_dir=$HOME/results_dir \\\n--category=edge --division=open --quiet\n</code></pre>"},{"location":"mlperf/inference/bert/README_reference/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/bert/README_reference/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/bert/tutorial/","title":"Tutorial","text":"<p>[ Back to index ]</p> <p>Note: from Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/bert/tutorial/#tutorial-modularizing-and-automating-mlperf-inference-language-processing-model-bert","title":"Tutorial: modularizing and automating MLPerf Inference Language Processing model bert","text":""},{"location":"mlperf/inference/bert/tutorial/#introduction","title":"Introduction","text":"<p>It should take less than an hour to complete this tutorial. In the end, you should obtain a tarball (<code>mlperf_submission.tar.gz</code>) with the MLPerf-compatible results.</p> <p>Note that both MLPerf and CM automation are evolving projects.  If you encounter issues or have questions, please submit them here  and feel free to join our public discord channel.</p>"},{"location":"mlperf/inference/bert/tutorial/#system-preparation","title":"System preparation","text":""},{"location":"mlperf/inference/bert/tutorial/#minimal-system-requirements","title":"Minimal system requirements","text":"<ul> <li>CPU: 1 node (x86-64 or Arm64)</li> <li>OS: we have tested this automation on Ubuntu 20.04, Ubuntu 22.04, Debian 10, Red Hat 9 and MacOS 13</li> <li>Disk space: ~ 10GB</li> <li>Python: 3.8+</li> <li>All other dependencies (artifacts and tools) will be installed by the CM meta-framework</li> </ul>"},{"location":"mlperf/inference/bert/tutorial/#system-requirements-to-run-mlperf-on-nvidia-gpu","title":"System requirements to run MLPerf on Nvidia GPU","text":"<ul> <li>GPU: any Nvidia GPU with 8GB+ or memory</li> <li>Disk space: ~ 30GB</li> </ul>"},{"location":"mlperf/inference/bert/tutorial/#mlcommons-cm-automation-meta-framework","title":"MLCommons CM automation meta-framework","text":"<p>The MLCommons is developing an open-source and technology-neutral  Collective Mind meta-framework (CM) to modularize ML Systems and automate their benchmarking, optimization  and design space exploration across continuously changing software, hardware and data.</p> <p>CM is the second generation of the MLCommons CK workflow automation framework  that was originally developed to make it easier to reproduce research papers at ML and Systems conferences. The goal is to help researchers unify and automate all the steps to prepare and run MLPerf and other benchmarks across diverse ML models, datasets, frameworks, compilers and hardware (see HPCA'22 presentation about our motivation).</p>"},{"location":"mlperf/inference/bert/tutorial/#cm-installation","title":"CM installation","text":"<p>Follow this guide to install the MLCommons CM automation language on your system.</p> <p>After the installation, you should be able to access the CM command line as follows:</p> <pre><code>cm\n</code></pre> <pre><code>cm {action} {automation} {artifact(s)} {--flags} @input.yaml @input.json\n</code></pre> <pre><code>cm --version\n</code></pre> <pre><code>1.5.3\n</code></pre> <p>Our goal is to keep CM language and scripts backward compatible.</p>"},{"location":"mlperf/inference/bert/tutorial/#pull-cm-repository-with-cross-platform-mlops-and-devops-scripts","title":"Pull CM repository with cross-platform MLOps and DevOps scripts","text":"<p>Pull stable MLCommons CM repository with cross-platform CM scripts for modular ML Systems:</p> <pre><code>cm pull repo mlcommons@ck\n</code></pre> <p>CM pulls all such repositories into the <code>$HOME/CM</code> directory to search for CM automations and artifacts. You can find the location of a pulled repository as follows:</p> <pre><code>cm find repo mlcommons@ck\n</code></pre> <pre><code>mlcommons@ck,a4705959af8e447a = /home/ubuntu/CM/repos/mlcommons@ck\n</code></pre>"},{"location":"mlperf/inference/bert/tutorial/#install-system-dependencies-for-your-platform","title":"Install system dependencies for your platform","text":"<p>First, you need to install various system dependencies required by the MLPerf inference benchmark.</p> <p>For this purpose, we have created a cross-platform CM script that will automatically install  such dependencies based on your OS (Ubuntu, Debian, Red Hat, MacOS ...). </p> <p>In this case, CM script serves simply as a wrapper with a unified and cross-platform interface for native scripts that you can find and extend here if some dependencies are missing on your machine - this is a collaborative way to make  CM scripts portable and interoperable.</p> <p>You can run this CM scripts as follows (note that you may be asked for a SUDO password on your platform):</p> <pre><code>cm run script \"get sys-utils-cm\" --quiet\n</code></pre> <p>If you think that you have all system dependencies installed, you can run this script without <code>--quiet</code> flag and type \"skip\" in the script prompt.</p>"},{"location":"mlperf/inference/bert/tutorial/#use-cm-to-detect-or-install-python-38","title":"Use CM to detect or install Python 3.8+","text":"<p>Since we use Python reference implementation of the MLPerf inference benchmark (unoptimized), we need to detect or install Python 3.8+ (MLPerf requirement). </p> <p>You need to detect it using the following CM script:</p> <pre><code>cm run script \"get python\" --version_min=3.8\n</code></pre> <p>Note, that all artifacts (including the above scripts) in MLCommons CM are organized as a database of interconnected components. They can be found either by their user friendly tags (such as <code>get,python</code>) or aliases (<code>get-python3</code>) and unique identifiers (<code>5b4e0237da074764</code>). You can find this information in a CM meta description of this script.</p> <p>If required Python is installed on your system, CM will detect it and cache related environment variables such as PATH, PYTHONPATH, etc. to be reused by other CM scripts. You can find an associated CM cache entry for your python as follows:</p> <pre><code>cm show cache --tags=get,python\n</code></pre> <p>You can see the environment variables produced by this CM script in the following JSON file: <pre><code>cat `cm find cache --tags=get,python`/cm-cached-state.json\n</code></pre></p> <p>If required Python is not detected, CM will automatically attempt to download and build it from sources  using another cross-platform CM script \"install-python-src\". In the end, CM will also cache new binaries and related environment variables such as PATH, PYTHONPATH, etc:</p> <pre><code>cm show cache\n</code></pre> <p>You can find installed binaries and reuse them in your own project with or without CM as follows: <pre><code>cm find cache --tags=install,python\n</code></pre></p> <p>Note that if you run the same script again, CM will automatically find and reuse the cached output: <pre><code>cm run script \"get python\" --version_min=3.8 --out=json\n</code></pre></p>"},{"location":"mlperf/inference/bert/tutorial/#setup-a-virtual-environment-for-python","title":"Setup a virtual environment for Python","text":"<pre><code>cm run script \"install python-venv\" --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"mlperf/inference/bert/tutorial/#pull-mlperf-inference-sources","title":"Pull MLPerf inference sources","text":"<p>You should now download and cache the MLPerf inference sources using the following command:</p> <pre><code>cm run script \"get mlperf inference src\"\n</code></pre>"},{"location":"mlperf/inference/bert/tutorial/#compile-mlperf-loadgen","title":"Compile MLPerf loadgen","text":"<p>You need to compile loadgen from the above inference sources:</p> <pre><code>cm run script \"get mlperf loadgen\"\n</code></pre>"},{"location":"mlperf/inference/bert/tutorial/#cm-automation-for-the-mlperf-benchmark","title":"CM automation for the MLPerf benchmark","text":""},{"location":"mlperf/inference/bert/tutorial/#mlperf-inference-python-bert-fp32-squad-v11-onnx-cpu-offline","title":"MLPerf inference - Python - Bert FP32 - SQUAD v1.1 - ONNX - CPU - Offline","text":""},{"location":"mlperf/inference/bert/tutorial/#download-the-squad-dataset","title":"Download the SQuAD dataset","text":"<pre><code>cm run script \"get dataset squad original\"\n</code></pre> <p>After installing this dataset via CM, you can reuse it in your own projects or other CM scripts (including MLPerf benchmarks). You can check the CM cache as follows (the unique ID of the CM cache entry will be different on your machine): <pre><code>cm show cache --tags=get,dataset,squad,original\n</code></pre></p> <pre><code>* Tags: dataset,get,language-processing,original,script-artifact-6651c119c3ae49b3,squad,validation,version-1.1\n  Path: /home/arjun/CM/repos/local/cache/e5ac8a524ba64d09\n  Version: 1.1\n</code></pre>"},{"location":"mlperf/inference/bert/tutorial/#install-onnx-runtime-for-cpu","title":"Install ONNX runtime for CPU","text":"<p>Now detect or install ONNX Python runtime (targeting CPU) your system using a generic CM script to install python package:</p> <pre><code>cm run script \"get generic-python-lib _onnxruntime\"\n</code></pre>"},{"location":"mlperf/inference/bert/tutorial/#download-bert-large-model-fp32-onnx-format","title":"Download Bert-large model (FP32, ONNX format)","text":"<p>Download and cache this reference model in the ONNX format (float32) using the following CM script:</p> <pre><code>cm run script \"get ml-model language-processing bert-large _onnx\"\n</code></pre> <p>It takes around ~1GB of disk space. You can find it in the CM cache as follows:</p> <pre><code>cm show cache --tags=get,ml-model,bert-large,_onnx\n</code></pre> <pre><code>*Tags: bert,bert-large,bert-squad,get,language,language-processing,ml-model,raw,script-artifact-5e865dbdc65949d2,_amazon-s3,_fp32,_onnx\n  Path: /home/arjun/CM/repos/local/cache/8727a38b72aa4b3f\n</code></pre>"},{"location":"mlperf/inference/bert/tutorial/#run-reference-mlperf-inference-benchmark-offline-accuracy","title":"Run reference MLPerf inference benchmark (offline, accuracy)","text":"<p>You are now ready to run the reference (unoptimized) Python implementation  of the MLPerf vision benchmark with ONNX backend.</p> <p>Normally, you would need to go through this README.md and prepare all the dependencies and environment variables manually.</p> <p>The CM \"app-mlperf-inference\" script allows you to run this benchmark as follows:</p> <pre><code>cm run script \"app mlperf inference generic _python _bert-99 _onnxruntime _cpu\" \\\n     --scenario=Offline \\\n     --mode=accuracy \\\n     --test_query_count=10 \\\n     --rerun\n</code></pre> <p>This CM script will automatically find or install all dependencies described in its CM meta description, aggregate all environment variables, preprocess all files and assemble the MLPerf benchmark CMD. After running the benchmark, it calls the MLPerf accuracy script to evaluate the accuracy of the results. </p> <p>It will take a few minutes to run it and you should see the following accuracy:</p> <pre><code>{\"exact_match\": 70.0, \"f1\": 70.0}\nReading examples...\nNo cached features at 'eval_features.pickle'... converting from examples...\nCreating tokenizer...\nConverting examples to features...\nCaching features at 'eval_features.pickle'...\nLoading LoadGen logs...\nPost-processing predictions..\n</code></pre> <p>Congratulations, you can now play with this benchmark using the unified CM commands!</p> <p>Note that even if did not install all the above dependencies manually, the below command will automatically install all the necessary dependencies.</p> <pre><code>cm run script \"app mlperf inference generic _python _bert-99 _onnxruntime _cpu\" \\\n     --adr.python.version_min=3.8 \\\n     --adr.compiler.tags=gcc \\\n     --scenario=Offline \\\n     --mode=accuracy \\\n     --test_query_count=10 \\\n     --quiet \\\n     --rerun\n</code></pre>"},{"location":"mlperf/inference/bert/tutorial/#run-mlperf-inference-benchmark-offline-performance","title":"Run MLPerf inference benchmark (offline, performance)","text":"<p>Let's run the MLPerf object detection while measuring performance:</p> <pre><code>cm run script \"app mlperf inference generic _python _bert-99 _onnxruntime _cpu\" \\\n     --scenario=Offline \\\n     --mode=performance \\\n     --rerun\n</code></pre> <p>It will run for a few seconds and you should see an output similar to the following one at the end (the QPS is the performance result of this benchmark that depends on the speed of your system):</p> <pre><code>================================================\nMLPerf Results Summary\n================================================\nSUT name : PySUT\nScenario : Offline\nMode     : PerformanceOnly\nSamples per second: 3.71597\nResult is : VALID\n  Min duration satisfied : Yes\n  Min queries satisfied : Yes\n  Early stopping satisfied: Yes\n\n================================================\nAdditional Stats\n================================================\nMin latency (ns)                : 267223684\nMax latency (ns)                : 2691085775\nMean latency (ns)               : 1478150052\n50.00 percentile latency (ns)   : 1612665856\n90.00 percentile latency (ns)   : 2691085775\n95.00 percentile latency (ns)   : 2691085775\n97.00 percentile latency (ns)   : 2691085775\n99.00 percentile latency (ns)   : 2691085775\n99.90 percentile latency (ns)   : 2691085775\n\n================================================\nTest Parameters Used\n================================================\nsamples_per_query : 10\ntarget_qps : 1\ntarget_latency (ns): 0\nmax_async_queries : 1\nmin_duration (ms): 0\nmax_duration (ms): 0\nmin_query_count : 1\nmax_query_count : 10\nqsl_rng_seed : 148687905518835231\nsample_index_rng_seed : 520418551913322573\nschedule_rng_seed : 811580660758947900\naccuracy_log_rng_seed : 0\naccuracy_log_probability : 0\naccuracy_log_sampling_target : 0\nprint_timestamps : 0\nperformance_issue_unique : 0\nperformance_issue_same : 0\nperformance_issue_same_index : 0\nperformance_sample_count : 10833\n\nNo warnings encountered during test.\n\nNo errors encountered during test.\n</code></pre> <p>Note that QPS is very low because we use an unoptimized reference implementation of this benchmark on CPU.</p>"},{"location":"mlperf/inference/bert/tutorial/#prepare-mlperf-submission","title":"Prepare MLPerf submission","text":"<p>You are now ready to generate the submission similar to the ones appearing on the official MLPerf inference dashboard.</p> <p>We have developed another script that runs the MLPerf inference benchmark in both accuracy and performance mode, runs the submission checker, unifies output for a dashboard and creates a valid MLPerf submission pack in <code>mlperf_submission.tar.gz</code>  with all required MLPerf logs and stats.</p> <p>You can run this script as follows:</p> <pre><code>cm run script --tags=run,mlperf,inference,run-mlperf,_submission,_short \\\n      --submitter=\"Community\" \\\n      --hw_name=default \\\n      --implementation=reference \\\n      --model=bert-99 \\\n      --backend=onnxruntime \\\n      --device=cpu \\\n      --scenario=Offline \\\n      --test_query_count=10 \\\n      --clean\n</code></pre> <p>It will take a few minutes to run and you should see the following output in the end:</p> <pre><code>[2023-09-26 19:20:42,245 submission_checker1.py:3308 INFO] Results open/Community/results/default-reference-cpu-onnxruntime-v1.15.1-default_config/bert-99/offline 3.72101\n[2023-09-26 19:20:42,245 submission_checker1.py:3310 INFO] ---\n[2023-09-26 19:20:42,245 submission_checker1.py:3395 INFO] ---\n[2023-09-26 19:20:42,245 submission_checker1.py:3396 INFO] Results=1, NoResults=0, Power Results=0\n[2023-09-26 19:20:42,245 submission_checker1.py:3403 INFO] ---\n[2023-09-26 19:20:42,245 submission_checker1.py:3404 INFO] Closed Results=0, Closed Power Results=0\n\n[2023-09-26 19:20:42,245 submission_checker1.py:3409 INFO] Open Results=1, Open Power Results=0\n\n[2023-09-26 19:20:42,245 submission_checker1.py:3414 INFO] Network Results=0, Network Power Results=0\n\n[2023-09-26 19:20:42,245 submission_checker1.py:3419 INFO] ---\n[2023-09-26 19:20:42,245 submission_checker1.py:3421 INFO] Systems=1, Power Systems=0\n[2023-09-26 19:20:42,245 submission_checker1.py:3422 INFO] Closed Systems=0, Closed Power Systems=0\n[2023-09-26 19:20:42,245 submission_checker1.py:3427 INFO] Open Systems=1, Open Power Systems=0\n[2023-09-26 19:20:42,245 submission_checker1.py:3432 INFO] Network Systems=0, Network Power Systems=0\n[2023-09-26 19:20:42,245 submission_checker1.py:3437 INFO] ---\n[2023-09-26 19:20:42,245 submission_checker1.py:3442 INFO] SUMMARY: submission looks OK\n/usr/bin/python3 /home/arjun/CM/repos/local/cache/0cfdde8a3bd64fb6/inference/tools/submission/generate_final_report.py --input summary.csv\n=========================================================\nSearching for summary.csv ...\nConverting to json ...\n\n                                                                           0\nOrganization                                                       Community\nAvailability                                                       available\nDivision                                                                open\nSystemType                                                              edge\nSystemName                                                           default\nPlatform                   default-reference-cpu-onnxruntime-v1.15.1-defa...\nModel                                                                bert-99\nMlperfModel                                                          bert-99\nScenario                                                             Offline\nResult                                                               3.72101\nAccuracy                                                                70.0\nnumber_of_nodes                                                            1\nhost_processor_model_name                AMD Ryzen 9 7950X 16-Core Processor\nhost_processors_per_node                                                   1\nhost_processor_core_count                                                 16\naccelerator_model_name                                                   NaN\naccelerators_per_node                                                      0\nLocation                   open/Community/results/default-reference-cpu-o...\nframework                                                onnxruntime v1.15.1\noperating_system             Ubuntu 22.04 (linux-6.2.0-32-generic-glibc2.35)\nnotes                      Powered by MLCommons Collective Mind framework...\ncompliance                                                                 1\nerrors                                                                     0\nversion                                                                 v3.1\ninferred                                                                   0\nhas_power                                                              False\nUnits                                                              Samples/s\n</code></pre> <p>Note that <code>--clean</code> flag cleans all previous runs of MLPerf benchmark to make sure that the MLPerf submission script picks up the latest results.</p> <p>You will also see the following 3 files in your current directory: <pre><code>ls -l\nmlperf_submission.tar.gz\nsummary.csv\nsummary.json\n</code></pre></p> <p>Note that by default, CM-MLPerf will store the raw results  in <code>$HOME/mlperf_submission</code> (with truncated accuracy logs) and in <code>$HOME/mlperf_submission_logs</code>  (with complete and very large accuracy logs).</p> <p>You can change this directory using the flag <code>--submission_dir={directory to store raw MLPerf results}</code> in the above script.</p>"},{"location":"mlperf/inference/bert/tutorial/#trying-deepsparse-backend","title":"Trying deepsparse backend","text":""},{"location":"mlperf/inference/bert/tutorial/#int8","title":"int8","text":"<pre><code>cm run script --tags=run,mlperf,inference,run-mlperf,_submission,_short  \\\n   --implementation=reference \\\n   --model=bert-99 \\\n   --backend=deepsparse \\\n   --device=cpu \\\n   --scenario=Offline \\\n   --test_query_count=1024 \\\n   --adr.mlperf-inference-implementation.max_batchsize=128 \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni \\\n   --clean \n</code></pre>"},{"location":"mlperf/inference/bert/tutorial/#fp32","title":"fp32","text":"<pre><code>cm run script --tags=run,mlperf,inference,run-mlperf,_submission,_short  \\\n   --adr.python.version_min=3.8 \\\n   --implementation=reference \\\n   --model=bert-99 \\\n   --backend=deepsparse \\\n   --device=cpu \\\n   --scenario=Offline \\\n   --test_query_count=1024 \\\n   --adr.mlperf-inference-implementation.max_batchsize=128 \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni \\\n   --clean \n</code></pre>"},{"location":"mlperf/inference/bert/tutorial/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/dlrm_v2/","title":"Index","text":"<p>[ Back to MLPerf inference benchmarks index ]</p>"},{"location":"mlperf/inference/dlrm_v2/#mlperf-inference-benchmark","title":"MLPerf inference benchmark","text":""},{"location":"mlperf/inference/dlrm_v2/#recommendation-with-dlrmv2","title":"Recommendation with DLRMv2","text":""},{"location":"mlperf/inference/dlrm_v2/#notes","title":"Notes","text":"<p>DLRMv2 has two variants - <code>dlrm_v2-99</code> and <code>dlrm_v2-99.9</code> where the <code>99</code> and <code>99.9</code> specify the required accuracy constraint  with respect to the reference fp32 model. DLRMv2 applies only to datacenter category and includes both Offline and Server scenarios.</p> <p>Please check MLPerf inference GitHub for more details.</p>"},{"location":"mlperf/inference/dlrm_v2/#run-using-the-mlcommons-cm-framework","title":"Run using the MLCommons CM framework","text":"<p>From Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/dlrm_v2/#a-few-ready-to-use-cm-commands","title":"A few ready-to-use CM commands","text":"<p>Install MLCommons CM automation framework with automation recipes for MLPerf as described here.</p> <p>The following guides explain how to run different implementations of this benchmark via CM:</p> <ul> <li>MLCommons Reference implementation in Python</li> <li>NVIDIA optimized implementation (GPU)</li> </ul>"},{"location":"mlperf/inference/dlrm_v2/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/dlrm_v2/README_nvidia/","title":"README nvidia","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/dlrm_v2/README_nvidia/#build-nvidia-docker-container-from-31-inference-round","title":"Build Nvidia Docker Container (from 3.1 Inference round)","text":"<pre><code>cm docker script --tags=build,nvidia,inference,server --dlrm_data_path=&lt;Path to dlrm data&gt;\n</code></pre>"},{"location":"mlperf/inference/dlrm_v2/README_nvidia/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/dlrm_v2/README_nvidia/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cmr \"run-mlperf inference _find-performance _all-scenarios\" \\\n--model=dlrm-v2-99 --implementation=nvidia-original --device=cuda --backend=tensorrt \\\n--category=datacenter --division=open --quiet\n</code></pre> * Use <code>--division=closed</code> to run all scenarios for the closed division. * Use <code>--model=dlrm-v2-99.9</code> to run the high-accuracy model * Use <code>--rerun</code> to force a rerun even when result files (from a previous run) exist</p>"},{"location":"mlperf/inference/dlrm_v2/README_nvidia/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cmr \"run-mlperf inference _submission _all-scenarios\" --model=dlrm-v2-99 \\\n--device=cuda --implementation=nvidia-original --backend=tensorrt \\\n--execution-mode=valid \\\n--category=datacenter --division=open --quiet --skip_submission_generation=yes\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division.</li> <li><code>--offline_target_qps</code> and <code>--server_target_qps</code> can be used to override the determined performance numbers</li> <li>Use <code>--model=dlrm-v2-99.9</code> to run the high-accuracy model</li> <li>Use <code>--rerun</code> to force a rerun even when result files (from a previous run) exist</li> </ul>"},{"location":"mlperf/inference/dlrm_v2/README_nvidia/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/dlrm_v2/README_nvidia/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/dlrm_v2/README_nvidia/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>CM automation for Nvidia's MLPerf inference implementation was developed by Arjun Suresh and Grigori Fursin.</li> <li>Nvidia's MLPerf inference implementation was developed by Zhihan Jiang, Ethan Cheng, Yiheng Zhang and Jinho Suh.</li> </ul>"},{"location":"mlperf/inference/dlrm_v2/README_reference/","title":"README reference","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/dlrm_v2/README_reference/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/dlrm_v2/README_reference/#detect-the-preprocessed-criteo-dataset","title":"Detect the preprocessed criteo dataset","text":"<pre><code>cm run script --tags=get,preprocessed,dataset,criteo --dir=&lt;path_to_multihot_preprocessed_dataset&gt;\n</code></pre>"},{"location":"mlperf/inference/dlrm_v2/README_reference/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_all-scenarios \\\n--model=dlrm-v2-99 --implementation=reference --device=cuda --backend=pytorch \\\n--category=datacenter --division=open --quiet \n</code></pre> * GPU needs a minimum of 92 GB memory for fp32 model.  * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--model=dlrm-v2-99.9</code> to run the high accuracy constraint dlrm-v2-99.9 model.</p>"},{"location":"mlperf/inference/dlrm_v2/README_reference/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_submission,_all-scenarios --model=dlrm-v2-99 \\\n--device=cuda --implementation=reference --backend=pytorch --execution-mode=valid \\\n--category=datacenter --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code> and <code>--server_target_qps</code> can be used to override the determined performance numbers</li> </ul>"},{"location":"mlperf/inference/dlrm_v2/README_reference/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/dlrm_v2/README_reference/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/gpt-j/","title":"Index","text":"<p>[ Back to MLPerf inference benchmarks index ]</p>"},{"location":"mlperf/inference/gpt-j/#mlperf-inference-benchmark","title":"MLPerf inference benchmark","text":""},{"location":"mlperf/inference/gpt-j/#language-processing-with-gpt-j","title":"Language processing with GPT-J","text":""},{"location":"mlperf/inference/gpt-j/#notes","title":"Notes","text":"<p>GPT-J has two variants - <code>gptj-99</code> and <code>gptj-99.9</code> where the <code>99</code> and <code>99.9</code> specifies the required accuracy constraint  with respect to the reference floating point model. <code>gptj-99.9</code> model is applicable only on a datacenter system.</p> <p>In the edge category, gptj-99 has Offline and SingleStream scenarios and in the datacenter category, both <code>gptj-99</code> and <code>gptj-99.9</code> have Offline and Server scenarios.</p> <p>Please check MLPerf inference GitHub for more details.</p>"},{"location":"mlperf/inference/gpt-j/#run-using-the-mlcommons-cm-framework","title":"Run using the MLCommons CM framework","text":"<p>From Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/gpt-j/#a-few-ready-to-use-cm-commands","title":"A few ready-to-use CM commands","text":"<p>Install MLCommons CM automation framework with automation recipes for MLPerf as described here.</p> <p>The following guides explain how to run different implementations of this benchmark via CM:</p> <ul> <li>MLCommons Reference implementation in Python</li> <li>NVIDIA optimized implementation (GPU)</li> </ul>"},{"location":"mlperf/inference/gpt-j/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/gpt-j/README_intel/","title":"README intel","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/gpt-j/README_intel/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":"<p>You can use <code>cm docker</code> instead of <code>cm run</code> to run the Intel implementation inside a docker container.  But for <code>gptj</code> we have found an issue of the code getting hanged when run on a 24 core machine but only when inside a docker container. </p>"},{"location":"mlperf/inference/gpt-j/README_intel/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance \\\n--scenario=Offline --model=gptj-99 --implementation=intel-original --backend=pytorch \\\n--category=datacenter --division=open --quiet\n</code></pre> * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Intel implementation currently supports only datacenter scenarios</p>"},{"location":"mlperf/inference/gpt-j/README_intel/#do-full-accuracy-and-performance-runs-for-all-the-offline-scenario","title":"Do full accuracy and performance runs for all the Offline scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_submission \\\n--scenario=Offline --model=gptj-99 --implementation=intel-original --backend=pytorch \\\n--category=datacenter --division=open --execution-mode=valid --quiet\n</code></pre>"},{"location":"mlperf/inference/gpt-j/README_intel/#do-full-accuracy-and-performance-runs-for-all-the-server-scenario","title":"Do full accuracy and performance runs for all the Server scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_submission \\\n--scenario=Server --model=gptj-99 --implementation=intel-original --backend=pytorch \\\n--category=datacenter --division=open --execution-mode=valid --server_target_qps=0.3 --quiet\n</code></pre> * <code>--server_target_qps</code> can be adjusted to the maximum as per the given system (which produces a valid result)</p>"},{"location":"mlperf/inference/gpt-j/README_intel/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/gpt-j/README_intel/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/gpt-j/README_intel/#extra-examples","title":"Extra Examples","text":""},{"location":"mlperf/inference/gpt-j/README_intel/#test-gpt-j-int4-using-intel-v31-code-drop-offline","title":"Test GPT-J int4 using Intel v3.1 code drop offline","text":"<pre><code>python3 -m pip install cmind -U\ncm pull repo mlcommons@ck\ncm run script --tags=run-mlperf,inference --implementation=intel --model=gptj-99 --scenario=Offline --precision=int4 --docker --sut=sapphire-rapids.112c --docker_cache=no\n</code></pre> <p>We now use <code>--docker</code> to run the command inside the docker while the preparation (model or big dataset download) happens on the host. </p> <p>In order to use rclone copy (and not rclone sync) on your submission system, please add <code>--env.CM_RCLONE_COPY_USING=copy</code> to the run above command. </p> <p>You can use <code>--docker_cache=no</code> to force CM to pick up the latest changes inside automatically-generated container (<code>cm pull repo</code>).</p> <p>Extra commands: <pre><code>cm run script --tags=run-mlperf,inference,_submission \\\n  --scenario=Offline --model=gptj-99 --implementation=intel-original --backend=pytorch \\\n  --category=datacenter --division=open --execution-mode=valid --quiet\n\ncm run script --tags=run-mlperf,inference,_submission \\\n  --scenario=Server --model=gptj-99 --implementation=intel-original --backend=pytorch \\\n  --category=datacenter --division=open --execution-mode=valid --server_target_qps=0.3 --quiet\n</code></pre></p>"},{"location":"mlperf/inference/gpt-j/README_nvidia/","title":"README nvidia","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/gpt-j/README_nvidia/#build-nvidia-docker-container-from-31-inference-round","title":"Build Nvidia Docker Container (from 3.1 Inference round)","text":"<pre><code>cm docker script --tags=build,nvidia,inference,server\n</code></pre>"},{"location":"mlperf/inference/gpt-j/README_nvidia/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/gpt-j/README_nvidia/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cmr \"run-mlperf inference _find-performance _all-scenarios\" \\\n--model=gptj-99 --implementation=nvidia-original --device=cuda --backend=tensorrt \\\n--category=edge --division=open --quiet\n</code></pre> * Use <code>--division=closed</code> to run all scenarios for the closed division. * Use <code>--category=datacenter</code> to run datacenter scenarios * Use <code>--model=gptj-99.9</code> to run the high-accuracy model * Use <code>--rerun</code> to force a rerun even when result files (from a previous run) exist</p>"},{"location":"mlperf/inference/gpt-j/README_nvidia/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cmr \"run-mlperf inference _submission _all-scenarios\" --model=gptj-99 \\\n--device=cuda --implementation=nvidia-original --backend=tensorrt \\\n--execution-mode=valid \\\n--category=edge --division=open --quiet --skip_submission_generation=yes\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division. No compliance runs are there for gptj. </li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code>, and <code>--singlestream_target_latency</code> can be used to override the determined performance numbers</li> <li>Use <code>--model=gptj-99.9</code> to run the high-accuracy model</li> <li>Use <code>--rerun</code> to force a rerun even when result files (from a previous run) exist</li> </ul>"},{"location":"mlperf/inference/gpt-j/README_nvidia/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/gpt-j/README_nvidia/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/gpt-j/README_nvidia/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>CM automation for Nvidia's MLPerf inference implementation was developed by Arjun Suresh and Grigori Fursin.</li> <li>Nvidia's MLPerf inference implementation was developed by Zhihan Jiang, Ethan Cheng, Yiheng Zhang and Jinho Suh.</li> </ul>"},{"location":"mlperf/inference/gpt-j/README_reference/","title":"README reference","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/gpt-j/README_reference/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/gpt-j/README_reference/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_all-scenarios \\\n--model=gpt-j-99 --implementation=reference --device=cuda --backend=pytorch \\\n--category=edge --division=open --quiet --precision=bfloat16 --env.GPTJ_BEAM_SIZE=4\n</code></pre> * GPU needs a minimum of 80 GB memory for fp32 model. For GPUs with shorter memory try <code>--env.GPTJ_BEAM_SIZE=2</code> and <code>--precision=float16</code>  * Use <code>--device=cpu</code> to run the inference on CPU (can be extremely slow) * <code>--precision=float16</code> can be tried on CPU and <code>--precision=fp32</code> on a 80 GB GPU  * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--category=datacenter</code> to run datacenter scenarios * Use <code>--model=gptj-99.9</code> to run the high accuracy constraint gptj-99.9 model.</p>"},{"location":"mlperf/inference/gpt-j/README_reference/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_submission,_all-scenarios --model=gpt-j-99 \\\n--device=cuda --implementation=reference --backend=pytorch \\\n--execution-mode=valid --results_dir=$HOME/results_dir \\\n--category=edge --division=open --quiet --precision=bfloat16 --env.GPTJ_BEAM_SIZE=4\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code>, and <code>--singlestream_target_latency</code> can be used to override the determined performance numbers</li> </ul>"},{"location":"mlperf/inference/gpt-j/README_reference/#populate-the-readme-files-describing-your-submission","title":"Populate the README files describing your submission","text":"<pre><code>cmr \"run-mlperf inference _populate-readme _all-scenarios\" \\\n--model=gpt-j-99 --device=cpu --implementation=reference --backend=pytorch \\\n--execution-mode=valid --results_dir=$HOME/results_dir \\\n--category=edge --division=open --quiet --precision=bfloat16 --env.GPTJ_BEAM_SIZE=4\n</code></pre>"},{"location":"mlperf/inference/gpt-j/README_reference/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/gpt-j/README_reference/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/llama2-70b/","title":"Index","text":"<p>[ Back to MLPerf inference benchmarks index ]</p>"},{"location":"mlperf/inference/llama2-70b/#mlperf-inference-benchmark","title":"MLPerf inference benchmark","text":""},{"location":"mlperf/inference/llama2-70b/#text-summarization-with-llama2-70b","title":"Text summarization with Llama2-70b","text":""},{"location":"mlperf/inference/llama2-70b/#notes","title":"Notes","text":"<p>Llama2-70b has two variants - <code>llama2-70b-99</code> and <code>llama2-70b-99.9</code> where the <code>99</code> and <code>99.9</code> specify the required accuracy constraint  with respect to the reference fp32 model. Llama2-70b applies only to datacenter category and includes both Offline and Server scenarios.</p> <p>Please check MLPerf inference GitHub for more details.</p>"},{"location":"mlperf/inference/llama2-70b/#run-using-the-mlcommons-cm-framework","title":"Run using the MLCommons CM framework","text":"<p>From Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/llama2-70b/#a-few-ready-to-use-cm-commands","title":"A few ready-to-use CM commands","text":"<p>Install MLCommons CM automation framework with automation recipes for MLPerf as described here.</p> <p>The following guides explain how to run different implementations of this benchmark via CM:</p> <ul> <li>MLCommons Reference implementation in Python</li> </ul>"},{"location":"mlperf/inference/llama2-70b/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/llama2-70b/README_reference/","title":"README reference","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/llama2-70b/README_reference/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/llama2-70b/README_reference/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_all-scenarios \\\n--model=llama2-70b-99 --implementation=reference --device=cuda --backend=pytorch \\\n--category=datacenter --division=open --precision=bfloat16--quiet \n</code></pre> * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--precision=float16</code> or <code>--precision=float32</code> to change the model precision * Use <code>--model=llama2-70b-99.9</code> to run the high accuracy constraint llama2-70b-99.9 model.</p>"},{"location":"mlperf/inference/llama2-70b/README_reference/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_submission,_all-scenarios --model=llama2-70b-99 \\\n--device=cuda --implementation=reference --backend=pytorch --precision=bfloat16 \\\n--execution-mode=valid --category=datacenter --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code> and <code>--server_target_qps</code> can be used to override the determined performance numbers</li> </ul>"},{"location":"mlperf/inference/llama2-70b/README_reference/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/llama2-70b/README_reference/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/resnet50/","title":"Index","text":"<p>[ Back to MLPerf inference benchmarks index ]</p>"},{"location":"mlperf/inference/resnet50/#mlperf-inference-benchmark","title":"MLPerf inference benchmark","text":""},{"location":"mlperf/inference/resnet50/#image-classification-with-resnet50","title":"Image classification with ResNet50","text":""},{"location":"mlperf/inference/resnet50/#notes","title":"Notes","text":"<p>In the edge category, ResNet50 has Offline, SingleStream, and MultiStream scenarios and in the datacenter category, it has Offline and Server scenarios. </p> <p>Please check MLPerf inference GitHub for more details.</p>"},{"location":"mlperf/inference/resnet50/#run-using-the-mlcommons-cm-framework","title":"Run using the MLCommons CM framework","text":"<p>From Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/resnet50/#get-imagenet-dataset","title":"Get Imagenet Dataset","text":"<p>We need to get full ImageNet dataset to make image-classification submissions for MLPerf inference.  Since this dataset is not publicly available via a URL please follow the instructions  given here  to download the dataset and register in CM.</p>"},{"location":"mlperf/inference/resnet50/#a-few-ready-to-use-cm-commands","title":"A few ready-to-use CM commands","text":"<p>Install MLCommons CM automation framework with automation recipes for MLPerf as described here.</p> <p>The following guides explain how to run different implementations of this benchmark via CM:</p> <ul> <li>MLCommons Reference implementation in Python</li> <li>NVIDIA optimized implementation (GPU)</li> <li>TFLite C++ implementation</li> </ul>"},{"location":"mlperf/inference/resnet50/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/resnet50/README_nvidia/","title":"README nvidia","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/resnet50/README_nvidia/#build-nvidia-docker-container-from-31-inference-round","title":"Build Nvidia Docker Container (from 3.1 Inference round)","text":"<pre><code>cm docker script --tags=build,nvidia,inference,server\n</code></pre>"},{"location":"mlperf/inference/resnet50/README_nvidia/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/resnet50/README_nvidia/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cmr \"run-mlperf inference _find-performance _all-scenarios\" \\\n--model=resnet50 --implementation=nvidia-original --device=cuda --backend=tensorrt \\\n--category=edge --division=open --quiet\n</code></pre> * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--category=datacenter</code> to run datacenter scenarios * Remove <code>_all-scenarios</code> and use <code>--scenario=Offline</code> to run the <code>Offline</code> scenario and similarly for <code>Server</code>, <code>SingleStream</code> and <code>MultiStream</code> scenarios.</p>"},{"location":"mlperf/inference/resnet50/README_nvidia/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cmr \"run-mlperf inference _submission _all-scenarios\" --model=resnet50 \\\n--device=cuda --implementation=nvidia-original --backend=tensorrt --execution-mode=valid \\\n--category=edge --division=open --quiet --skip_submission_generation=yes\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code>, <code>--singlestream_target_latency</code> and <code>multistream_target_latency</code> can be used to override the determined performance numbers</li> </ul>"},{"location":"mlperf/inference/resnet50/README_nvidia/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/resnet50/README_nvidia/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/resnet50/README_reference/","title":"README reference","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/resnet50/README_reference/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/resnet50/README_reference/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_all-scenarios \\\n--model=resnet50 --implementation=reference --device=cpu --backend=onnxruntime \\\n--category=edge --division=open --quiet\n</code></pre> * Use <code>--device=cuda</code> to run the inference on Nvidia GPU * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--category=datacenter</code> to run datacenter scenarios * Use <code>--backend=tf</code>, <code>--backend=ncnn</code> or <code>--backend=tvm-onnx</code> to use tensorflow, ncnn and tvm-onnx backends respectively * Remove <code>_all-scenarios</code> and use <code>--scenario=Offline</code> to run the <code>Offline</code> scenario and similarly for <code>Server</code>, <code>SingleStream</code> and <code>MultiStream</code> scenarios.</p>"},{"location":"mlperf/inference/resnet50/README_reference/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_submission,_all-scenarios --model=resnet50 \\\n--device=cpu --implementation=reference --backend=onnxruntime --execution-mode=valid \\\n--category=edge --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes --adr.mlperf-power-client.power_server=192.168.0.15 --adr.mlperf-power-client.port=4950</code> for measuring power. Please adjust the server IP (where MLPerf power server is installed) and Port (default is 4950). <code>power=yes</code> is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code>, <code>--singlestream_target_latency</code> and <code>multistream_target_latency</code> can be used to override the determined performance numbers</li> </ul>"},{"location":"mlperf/inference/resnet50/README_reference/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/resnet50/README_reference/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/resnet50/README_tflite/","title":"README tflite","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/resnet50/README_tflite/#running-a-mobilenet-model-using-tflite-backend","title":"Running a mobilenet model using TFLite backend","text":"<p>Please follow this guide  to run the entire set of MobileNet and EfficientNet models trained using ImageNet dataset (total 81 models) using TFLite backend.</p> <p>If you want to try an individual model run, you can proceed as follows:</p>"},{"location":"mlperf/inference/resnet50/README_tflite/#run-command","title":"Run Command","text":"<pre><code>cm run script --tags=run,mlperf,inference,run-mlperf,_submission,_short  \\\n   --adr.python.version_min=3.8 \\\n   --implementation=tflite-cpp \\\n   --model=efficientnet \\\n   --backend=tflite \\\n   --device=cpu \\\n   --scenario=SingleStream \\\n   --test_query_count=100 \\\n   --adr.tflite-model.tags=_lite0 \\\n   --adr.mlperf-inference-implementation.tags=_armnn,_use-neon \\\n   --clean\n</code></pre>"},{"location":"mlperf/inference/resnet50/README_tflite/#run-resnet50-tflite-via-cm","title":"Run ResNet50 TFLite via CM","text":""},{"location":"mlperf/inference/resnet50/README_tflite/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_all-scenarios \\\n--model=resnet50 --implementation=tflite-cpp --device=cpu --backend=tflite \\\n--category=edge --division=open --quiet\n</code></pre> * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Since only singlestream is implemented for tflite-cpp, datacenter submission is not possible * Use <code>--adr.mlperf-inference-implementation.tags=_armnn,_use-neon</code> to use ARMNN backend</p>"},{"location":"mlperf/inference/resnet50/README_tflite/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_submission,_all-scenarios --model=resnet50 \\\n--device=cpu --implementation=tflite-cpp --backend=tflite \\\n--execution-mode=valid --category=edge --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code>, <code>--singlestream_target_latency</code> and <code>multistream_target_latency</code> can be used to override the determined performance numbers</li> <li>Use <code>--adr.mlperf-inference-implementation.tags=_armnn,_use-neon</code> to use ARMNN backend</li> </ul>"},{"location":"mlperf/inference/resnet50/README_tflite/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/resnet50/README_tflite/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/retinanet/","title":"Index","text":"<p>[ Back to MLPerf inference benchmarks index ]</p>"},{"location":"mlperf/inference/retinanet/#mlperf-inference-benchmark","title":"MLPerf inference benchmark","text":""},{"location":"mlperf/inference/retinanet/#object-detection-with-retinanet","title":"Object detection with RetinaNet","text":""},{"location":"mlperf/inference/retinanet/#notes","title":"Notes","text":"<p>In the edge category, RetinaNet has Offline, SingleStream and MultiStream scenarios and in the datacenter category, it has Offline and Server scenarios. </p> <p>Please check MLPerf inference GitHub for more details.</p>"},{"location":"mlperf/inference/retinanet/#run-using-the-mlcommons-cm-framework","title":"Run using the MLCommons CM framework","text":"<p>From Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/retinanet/#a-few-ready-to-use-cm-commands","title":"A few ready-to-use CM commands","text":"<p>Install MLCommons CM automation framework with automation recipes for MLPerf as described here.</p> <p>The following guides explain how to run different implementations of this benchmark via CM:</p> <ul> <li>MLCommons Reference implementation in Python</li> <li>NVIDIA optimized implementation (GPU)</li> </ul>"},{"location":"mlperf/inference/retinanet/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/retinanet/README_nvidia/","title":"README nvidia","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/retinanet/README_nvidia/#build-nvidia-docker-container-from-31-inference-round","title":"Build Nvidia Docker Container (from 3.1 Inference round)","text":"<pre><code>cm docker script --tags=build,nvidia,inference,server\n</code></pre>"},{"location":"mlperf/inference/retinanet/README_nvidia/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/retinanet/README_nvidia/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cmr \"run-mlperf inference _find-performance _all-scenarios\" \\\n--model=retinanet --implementation=nvidia-original --device=cuda --backend=tensorrt \\\n--category=edge --division=open --quiet\n</code></pre> * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--category=datacenter</code> to run datacenter scenarios</p>"},{"location":"mlperf/inference/retinanet/README_nvidia/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cmr \"run-mlperf inference _submission _all-scenarios\" --model=retinanet \\\n--device=cuda --implementation=nvidia-original --backend=tensorrt \\\n--execution-mode=valid --category=edge --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code>, <code>--singlestream_target_latency</code> and <code>multistream_target_latency</code> can be used to override the determined performance numbers</li> </ul>"},{"location":"mlperf/inference/retinanet/README_nvidia/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/retinanet/README_nvidia/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/retinanet/README_nvidia/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>CM automation for Nvidia's MLPerf inference implementation was developed by Arjun Suresh and Grigori Fursin.</li> <li>Nvidia's MLPerf inference implementation was developed by Zhihan Jiang, Ethan Cheng, Yiheng Zhang and Jinho Suh.</li> </ul>"},{"location":"mlperf/inference/retinanet/README_reference/","title":"README reference","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/retinanet/README_reference/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/retinanet/README_reference/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cmr \"run-mlperf inference _find-performance _all-scenarios\" \\\n--model=retinanet --implementation=reference --device=cpu --backend=onnxruntime \\\n--category=edge --division=open --quiet\n</code></pre> * Use <code>--device=cuda</code> to run the inference on Nvidia GPU * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--category=datacenter</code> to run datacenter scenarios * Use <code>--backend=pytorch</code> to use pytorch backend</p>"},{"location":"mlperf/inference/retinanet/README_reference/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cmr \"run-mlperf inference _submission _all-scenarios\" --model=retinanet \\\n--device=cpu --implementation=reference --backend=onnxruntime \\\n--execution-mode=valid --results_dir=$HOME/results_dir \\\n--category=edge --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code>, <code>--singlestream_target_latency</code> and <code>multistream_target_latency</code> can be used to override the determined performance numbers</li> </ul>"},{"location":"mlperf/inference/retinanet/README_reference/#populate-the-readme-files","title":"Populate the README files","text":"<pre><code>cmr \"run-mlperf inference _populate-readme _all-scenarios\" \\\n--model=retinanet --device=cpu --implementation=reference --backend=onnxruntime \\\n--execution-mode=valid --results_dir=$HOME/results_dir \\\n--category=edge --division=open --quiet\n</code></pre>"},{"location":"mlperf/inference/retinanet/README_reference/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/retinanet/README_reference/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/rnnt/","title":"Index","text":"<p>[ Back to MLPerf inference benchmarks index ]</p>"},{"location":"mlperf/inference/rnnt/#mlperf-inference-benchmark","title":"MLPerf inference benchmark","text":""},{"location":"mlperf/inference/rnnt/#speech-recognition-with-rnnt","title":"Speech recognition with RNNT","text":""},{"location":"mlperf/inference/rnnt/#notes","title":"Notes","text":"<p>In the edge category, ResNet50 has Offline, SingleStream and MultiStream scenarios and in the datacenter category, it has Offline and Server scenarios.</p> <p>Please check MLPerf inference GitHub for more details.</p>"},{"location":"mlperf/inference/rnnt/#run-using-the-mlcommons-cm-framework","title":"Run using the MLCommons CM framework","text":"<p>From Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/rnnt/#a-few-ready-to-use-cm-commands","title":"A few ready-to-use CM commands","text":"<p>Install MLCommons CM automation framework with automation recipes for MLPerf as described here.</p> <p>The following guides explain how to run different implementations of this benchmark via CM:</p> <ul> <li>MLCommons Reference implementation in Python</li> <li>NVIDIA optimized implementation (GPU)</li> </ul>"},{"location":"mlperf/inference/rnnt/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/rnnt/README_nvidia/","title":"README nvidia","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/rnnt/README_nvidia/#build-nvidia-docker-container-from-31-inference-round","title":"Build Nvidia Docker Container (from 3.1 Inference round)","text":"<pre><code>cm docker script --tags=build,nvidia,inference,server\n</code></pre>"},{"location":"mlperf/inference/rnnt/README_nvidia/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/rnnt/README_nvidia/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cmr \"run-mlperf inference _find-performance\" --scenario=Offline \\\n--model=rnnt --implementation=nvidia-original --device=cuda --backend=tensorrt \\\n--category=edge --division=open --quiet\n</code></pre> * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--category=datacenter</code> to run datacenter scenarios</p>"},{"location":"mlperf/inference/rnnt/README_nvidia/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cmr \"run-mlperf inference _submission _all-scenarios\" --model=rnnt \\\n--device=cuda --implementation=nvidia-original --backend=tensorrt \\\n--execution-mode=valid --category=edge --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code>, and <code>--singlestream_target_latency</code> can be used to override the determined performance numbers</li> </ul>"},{"location":"mlperf/inference/rnnt/README_nvidia/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/rnnt/README_nvidia/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/rnnt/README_nvidia/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>CM automation for Nvidia's MLPerf inference implementation was developed by Arjun Suresh and Grigori Fursin.</li> <li>Nvidia's MLPerf inference implementation was developed by Zhihan Jiang, Ethan Cheng, Yiheng Zhang and Jinho Suh.</li> </ul>"},{"location":"mlperf/inference/rnnt/README_reference/","title":"README reference","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/rnnt/README_reference/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/rnnt/README_reference/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cmr \"run-mlperf inference _find-performance _all-scenarios\" \\\n--model=rnnt --implementation=reference --device=cpu --backend=pytorch \\\n--category=edge --division=open --quiet \n</code></pre> * Use <code>--device=cuda</code> to run the inference on Nvidia GPU * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--category=datacenter</code> to run datacenter scenarios</p>"},{"location":"mlperf/inference/rnnt/README_reference/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cmr \"run-mlperf inference _submission _all-scenarios\" --model=rnnt \\\n--device=cpu --implementation=reference --backend=pytorch \\\n--execution-mode=valid --category=edge --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code> and  <code>--singlestream_target_latency</code> can be used to override the determined performance numbers</li> </ul>"},{"location":"mlperf/inference/rnnt/README_reference/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/rnnt/README_reference/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/stable-diffusion-xl/","title":"Index","text":"<p>[ Back to MLPerf inference benchmarks index ]</p>"},{"location":"mlperf/inference/stable-diffusion-xl/#mlperf-inference-benchmark","title":"MLPerf inference benchmark","text":""},{"location":"mlperf/inference/stable-diffusion-xl/#text-to-image-with-stable-diffusion-xl","title":"Text to image with Stable-Diffusion-xl","text":""},{"location":"mlperf/inference/stable-diffusion-xl/#notes","title":"Notes","text":"<p>Stable-diffusion-xl has SingleStream and Offline scenarios in the edge category and Offline and Server scenarios in the datacenter category.</p> <p>Please check MLPerf inference GitHub for more details.</p>"},{"location":"mlperf/inference/stable-diffusion-xl/#run-using-the-mlcommons-cm-framework","title":"Run using the MLCommons CM framework","text":"<p>From Feb 2024, we suggest you to use this GUI  to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software  and hardware, and prepare your submissions.</p>"},{"location":"mlperf/inference/stable-diffusion-xl/#a-few-ready-to-use-cm-commands","title":"A few ready-to-use CM commands","text":"<p>Install MLCommons CM automation framework with automation recipes for MLPerf as described here.</p> <p>The following guides explain how to run different implementations of this benchmark via CM:</p> <ul> <li>MLCommons Reference implementation in Python</li> </ul>"},{"location":"mlperf/inference/stable-diffusion-xl/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/inference/stable-diffusion-xl/README_reference/","title":"README reference","text":"<p>[ Back to index ]</p>"},{"location":"mlperf/inference/stable-diffusion-xl/README_reference/#run-this-benchmark-via-cm","title":"Run this benchmark via CM","text":""},{"location":"mlperf/inference/stable-diffusion-xl/README_reference/#do-a-test-run-to-detect-and-record-the-system-performance","title":"Do a test run to detect and record the system performance","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_all-scenarios \\\n--model=sdxl --implementation=reference --device=cuda --backend=pytorch \\\n--category=edge --division=open --quiet \n</code></pre> * Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode) * Use <code>--category=datacenter</code> to run datacenter scenarios</p>"},{"location":"mlperf/inference/stable-diffusion-xl/README_reference/#do-full-accuracy-and-performance-runs-for-all-the-scenarios","title":"Do full accuracy and performance runs for all the scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_submission,_all-scenarios --model=sdxl \\\n--device=cuda --implementation=reference --backend=pytorch \\\n--execution-mode=valid --category=datacenter --division=open --quiet\n</code></pre> <ul> <li>Use <code>--power=yes</code> for measuring power. It is ignored for accuracy and compliance runs</li> <li>Use <code>--division=closed</code> to run all scenarios for the closed division including the compliance tests</li> <li><code>--offline_target_qps</code>, <code>--server_target_qps</code>, and <code>--singlestream_target_latency</code> can be used to override the determined performance numbers</li> <li>Use <code>--category=datacenter</code> to run datacenter scenarios</li> </ul>"},{"location":"mlperf/inference/stable-diffusion-xl/README_reference/#generate-and-upload-mlperf-submission","title":"Generate and upload MLPerf submission","text":"<p>Follow this guide to generate the submission tree and upload your results.</p>"},{"location":"mlperf/inference/stable-diffusion-xl/README_reference/#questions-suggestions","title":"Questions? Suggestions?","text":"<p>Check the MLCommons Task Force on Automation and Reproducibility  and get in touch via public Discord server.</p>"},{"location":"mlperf/setup/setup-aws-instance/","title":"Setup aws instance","text":"<p>[ Back to MLPerf benchmarks index ]</p>"},{"location":"mlperf/setup/setup-aws-instance/#setup-asw-instance-for-mlperf","title":"Setup ASW instance for MLPerf","text":"<p>The below instructions are for creating an AWS instance from the CLI. You can also create an instance via web and setup CM on it.</p>"},{"location":"mlperf/setup/setup-aws-instance/#prerequisites","title":"Prerequisites","text":"<ol> <li>AWS Key, secret and token</li> <li><code>*.pem</code> ssh key file to be used to create the instance (public key from here will be copied to the <code>$HOME/.ssh/authorized_keys</code> file in the created instance)</li> </ol>"},{"location":"mlperf/setup/setup-aws-instance/#run-commands","title":"Run Commands","text":"<p>We need to get imagenet full dataset to make image-classification submissions for MLPerf inference. Since this dataset is not publicly available via a URL please follow the instructions given here to download the dataset and register in CM.</p>"},{"location":"mlperf/setup/setup-aws-instance/#update-access-details","title":"Update Access Details","text":"<p><pre><code>cd $HOME/CM/repos/mlcommon@ck/cm-mlops/script/run-terraform/aws/\ncp credentials.example credentials.sh\n</code></pre> Update <code>credentials.sh</code> with your AWS Key, Secret and Token</p>"},{"location":"mlperf/setup/setup-aws-instance/#create-an-aws-instance","title":"Create an AWS Instance","text":"<pre><code>cm run script --tags=run,terraform,_m7g.xlarge,_storage_size.500,_ubuntu.2204,_us-west-2 \\\n--cminit --key_file=$HOME/cmuser.pem\n</code></pre> <p>The above command will output the IP of the created instance which will be having CM setup already done. </p> <p><code>_m7g.xlarge,_storage_size.500,_ubuntu.2204</code> variations can be changed to launch a different instance. Below are the variation combinations we used for MLPerf inference 3.0 submissions.</p> <ul> <li><code>_g4dn.xlarge</code></li> <li><code>_a1.2xlarge,_storage_size.130,_ubuntu.2204</code></li> <li><code>_c5.4xlarge,_storage_size.130,_ubuntu.2204</code></li> <li><code>_m7g.2xlarge,_storage_size.500,_ubuntu.2204</code></li> <li><code>_inf1.2xlarge,_storage_size.500,_amazon-linux-2-kernel.510</code></li> <li><code>_t2.medium,_storage_size.200,_rhel.9</code></li> </ul>"},{"location":"mlperf/setup/setup-aws-instance/#copy-the-needed-files-from-the-local-machine","title":"Copy the needed files from the local machine","text":"<p>Copy the imagenet dataset to the created instance. For example,</p> <p><pre><code>rsync -avz -e 'ssh -i $HOME/cmuser.pem' $HOME/imagenet-2012-val/ ubuntu@54.189.93.134:\n</code></pre> For using nvidia-original implementation tar files for cuDNN and TensorRT are needed to be downloaded locally from Nvidia website and copied to the AWS instance similar to the above command.</p> <p>Once all the required files are copied over, login to the instance and follow the individual benchmark instructions from the README files given here</p>"},{"location":"mlperf/setup/setup-gcp-instance/","title":"Setup gcp instance","text":"<p>[ Back to MLPerf benchmarks index ]</p>"},{"location":"mlperf/setup/setup-gcp-instance/#setup-gcp-instance-for-mlperf","title":"Setup GCP instance for MLPerf","text":"<p>The below instructions are for creating a Google Cloud instance from the CLI. You can also create an instance via web and setup CM on it.</p>"},{"location":"mlperf/setup/setup-gcp-instance/#prerequisites","title":"Prerequisites","text":"<p>Please follow the authentication instructions given here.</p>"},{"location":"mlperf/setup/setup-gcp-instance/#run-commands","title":"Run Commands","text":"<p>We need to get imagenet full dataset to make image-classification submissions for MLPerf inference. Since this dataset is not publicly available via a URL please follow the instructions given here to download the dataset and register in CM.</p>"},{"location":"mlperf/setup/setup-gcp-instance/#create-a-gcp-instance","title":"Create a GCP Instance","text":"<pre><code>cm run script --tags=run,terraform,_gcp,_n1-highmem.4,_gcp_project.mlperf-inference-tests --cminit\n</code></pre> <p>The above command will output the IP of the created instance which will be having CM setup already done. </p> <p><code>_n1-highmem.4</code> variation can be changed to launch a different instance. Below are the variation combinations we used for MLPerf inference 3.0 submissions.</p> <ul> <li><code>_n1-standard.4</code></li> </ul>"},{"location":"mlperf/setup/setup-gcp-instance/#copy-the-needed-files","title":"Copy the needed files","text":"<p>Copy the imagenet dataset to the created instance. For example,</p> <p><pre><code>rsync -avz -e 'ssh -i $HOME/cmuser.pem' $HOME/imagenet-2012-val/ ubuntu@54.189.93.134:\n</code></pre> For using nvidia-original implementation tar files for cuDNN and TensorRT are needed to be downloaded locally from Nvidia website and copied to the AWS instance similar to the above command.</p> <p>Once all the required files are copied over, login to the instance and follow the individual benchmark instructions from the README files given here</p>"},{"location":"mlperf/setup/setup-nvidia-jetson-orin/","title":"Setup nvidia jetson orin","text":"<p>[ Back to MLPerf benchmarks index ]</p>"},{"location":"mlperf/setup/setup-nvidia-jetson-orin/#setup","title":"Setup","text":"<p>We used Nvidia Jetson AGX Orin developer kit with 32GB RAM and 64GB eMMC. We also connected a 500GB SSD disk via USB and Wifi connection for internet connectivity.</p> <p>We used the out of the box developer kit image which was running Ubuntu 20.04 and JetPack 5.0.1 Developer Preview (L4T 34.1.1) with CUDA 11.4. We were also using the default 4k page size (Nvidia recommends 64k for MLPerf inference).</p> <p>cuDNN 8.6.0 and TensorRT 8.5.2.2 were downloaded as Debian packages on a host machine, copied over to Nvidia Jetson Orin and installed.</p> <p>We need to get imagenet full dataset to make image-classification submissions for MLPerf inference. Since this dataset is not publicly available via a URL please follow the instructions given here to download the dataset.</p>"},{"location":"mlperf/setup/setup-nvidia-jetson-orin/#copy-the-needed-files-from-a-host-machine","title":"Copy the needed files from a host machine","text":"<p>[Optional but can avoid download] Copy the imagenet dataset to the created instance. For example,</p> <pre><code>rsync -avz  $HOME/imagenet-2012-val/ user@192.168.0.27:\n</code></pre> <p>Login to Orin and register the imagenet dataset as <pre><code>cm run script --tags=get,imagenet,dataset,_2012,_full --input=$HOME/imagenet-2012-val\n</code></pre> Once all the required files are copied over, follow the individual benchmark instructions from the README files given here. All the required dependencies should be resolved by CM.</p>"},{"location":"mlperf/setup/setup-nvidia-jetson-orin/#power-measurement-setup","title":"Power Measurement Setup","text":"<p>We were measuring power in the peak performance mode (MaxN) except for one SUT where the energy efficiency mode was changed to Max15. Our aim was to showcase the out of the box performance of Nvidia Jetson AGX Orin including the power usage. </p>"},{"location":"mlperf/setup/setup-nvidia-jetson-orin/#reproducing-the-nvidia-jetson-agx-orin-submission","title":"Reproducing the Nvidia Jetson AGX Orin Submission","text":"<p>After our submission we followed the instructions from Nvidia in the inference v3.0 repository and tried to reproduce the numbers from Nvidia. For MaxN mode we were able to match the numbers by Nvidia using same versions of CUDA, cuDNN and TensorRT but outside of docker. For MaxQ mode, we could get the same performance as Nvidia but our power usage was about 5W higher.</p>"},{"location":"mlperf/setup/setup-nvidia-jetson-orin/#performance-results-maxn","title":"Performance results MaxN","text":"<p>The below table shows the performance comparison of our results under different settings and the Nvidia submission for MLPerf inference 3.0. We'll be updating our instructions for easier reproducibility of these numbers including CM scripts for flashing the L4T image and rebuilding the kernel for 64k pagesize.</p> Workload Results L4T PAGESIZE Power Mode FAN Dynamic Speed control Offline Accuracy Offline Performance SingleStream Accuracy SingleStream Performance MultiStream Accuracy MultiStream Performance ResNet50 Nvidia Submitted (docker) r35.3 64k MaxN active 75.934 6438.1 76.032 0.633479 76.032 2.187731 ResNet50 cTuning Submitted r34.1.1 4k MaxN active 75.934 4697 76.032 0.72 76.032 2.57 ResNet50 MLCommons taskforce on reproducibility r35.2.1 4k MaxN active 75.85 6172 76.056 0.644 76.056 2.074 ResNet50 MLCommons taskforce on reproducibility r35.3 64k MaxN active 75.85 6430 76.056 0.659 76.056 2.20 RetinaNet Nvidia Submitted (docker) r35.3 x MaxN active 37.372 92.4048 37.403 13.924457 37.519 104.680313 RetinaNet MLCommons taskforce on reproducibility r35.2.1 4k MaxN active 37.346 80.0854 (no DLA) 37.350 14,19 37.409 105.344828 RetinaNet MLCommons taskforce on reproducibility r35.3 64k MaxN active 37.345 94.6886 37.340 14.073 37.488 103.8 BERT Nvidia Submitted (docker) r35.3 x MaxN active 90.552 544.243 90.344 5.635431 NA NA BERT cTuning Submitted r34.1.1 4k MaxN active 90.552 449.96 90.344 7.8 NA NA BERT MLCommons taskforce on reproducibility r35.2.1 4k MaxN active 90.562 527 (128 batchsize) 90.311 6.636 NA NA BERT MLCommons taskforce on reproducibility r35.3 64k MaxN active 90.552 539 90.344 6.31 NA NA"},{"location":"mlperf/setup/setup-nvidia/","title":"Setup nvidia","text":"<p>[ Back to MLPerf benchmarks index ]</p>"},{"location":"mlperf/setup/setup-nvidia/#mlperf-inference-v31","title":"MLPerf inference v3.1","text":"<p>Nvidia implementation requires extra CM command to build and run Docker container</p> <pre><code>cm docker script --tags=build,nvidia,inference,server\n</code></pre> <p>You can then copy/paste CM commands generated by this GUI to run MLPerf benchmarks.</p>"},{"location":"specs/","title":"Index","text":"<p>[ Back to Specs ]</p>"},{"location":"specs/#cm-specification","title":"CM specification","text":"<ul> <li>CM architecture</li> <li>CM CLI</li> <li>CM Python API</li> <li>CM format for software projects (CM repository)</li> </ul>"},{"location":"specs/#cm-automation-specs","title":"CM automation specs","text":"<ul> <li>CM automations \"script\" and \"cache\"</li> </ul>"},{"location":"specs/cm-automation-script/","title":"Cm automation script","text":"<p>[ Back to Specs ]</p>"},{"location":"specs/cm-automation-script/#cm-scripts","title":"CM scripts","text":"<p>Please check the CM introduction to understand CM motivation and concepts.</p>"},{"location":"specs/cm-automation-script/#tags","title":"Tags","text":"<pre><code>cm (some actions) --tags={list of tags}\n</code></pre>"},{"location":"specs/cm-automation-script/#searching-artifacts","title":"Searching artifacts","text":"<ul> <li>\"-\" prefix is used to exclude artifacts with this tag</li> </ul>"},{"location":"specs/cm-automation-script/#differentiating-ml-artifacts","title":"Differentiating ML artifacts","text":"<ul> <li>\"app-\" is used to specify application script</li> <li>\"_\" is used to select variations</li> </ul>"},{"location":"specs/cm-automation-script/#environment-variables","title":"Environment variables","text":""},{"location":"specs/cm-automation-script/#converted-from-cli","title":"Converted from CLI","text":"<ul> <li>CM_TMP_QUIET</li> <li>CM_PATH</li> <li>CM_INPUT</li> <li>CM_OUTPUT</li> <li>CM_NAME</li> </ul>"},{"location":"specs/cm-automation-script/#local-removed-from-deps","title":"Local (removed from deps)","text":"<ul> <li>CM_VERSION</li> <li>CM_VERSION_MIN</li> <li>CM_VERSION_MAX</li> <li>CM_VERSION_MAX_DEFAULT</li> <li>CM_GIT_*</li> <li>CM_TMP_*</li> </ul>"},{"location":"specs/cm-automation-script/#automatically-generated","title":"Automatically generated","text":"<ul> <li>CM_TMP_CURRENT_PATH - script or cache</li> <li>CM_TMP_CURRENT_SCRIPT_PATH - script only</li> </ul>"},{"location":"specs/cm-cli/","title":"Cm cli","text":"<p>[ Back to Specs ]</p>"},{"location":"specs/cm-cli/#cm-command-line-interface","title":"CM command line interface","text":"<p>One of the main goals of the CM language is to provide a common, unified and human-readable CLI  to access all software repositories shared in the CM format.</p>"},{"location":"specs/cm-cli/#format","title":"Format","text":"<p>The idea is to unify all shared READMEs, containers and Jupyter notebooks  with CM commands to  to make it easier for the community to run software projects and reuse individual automations across continuously changing software, hardware and data.</p> <p>Here is format of a unified CM command line  to run any reusable automation action  from any software project on Linux, Windows and MacOS:</p> <pre><code>cm {action} {automation alias | UID | alias,UID} \n  ({artifact name(s) | sub-action | argument}) \n  (--flag1=value1) (--flag2.key2=value2) (--flag3,=value3,value4) (--flag4)\n  (@input.json | @input.yaml)\n  (-- extra CMD)\n</code></pre> <p>First, CM will parse CM CLI into a unified CM input dictionary:</p> <pre><code>{\n  \"action\":\"automation action\",\n  \"automation\":\"automation alias | UID | alias,UID\",\n\n  \"artifact\":{above artifact name or sub-action},\n\n  \"flag1\":\"value1\",\n  \"flag2\":{\"key2\":\"value2\"},\n  \"flag3\":[\"value3\",\"value4\"],\n  \"flag4\":True,\n  ...\n  \"unparsed_cmd\": [\n    list of strings in extra CMD\n   ]\n}\n</code></pre> <p>When a user specify one or more input files with @ prefix, they will be loaded  and merged with the CM input in the same order as in command line.</p> <p>CM will then call a unified CM Python \"access\" function  with this input dictionary to perform some automation action. </p> <p>It is equivalent to using CM Python API except that CM will be in interactive mode.  You can add a flag <code>--out=json</code> to print the output dictionary at the end of an automation action invoked via CLI.</p> <p>You can test the CM interface using the following automation action that simply prints the unified CM input dictionary: <pre><code>cm print-input automation artifact1 artifact2 --flag1=value1 --flag2 -- something\n</code></pre></p>"},{"location":"specs/cm-cli/#examples","title":"Examples","text":"<pre><code>cm \ncm help\ncm {common automation action}\n</code></pre> <pre><code>cm {common automation action} --help\n</code></pre> <pre><code>cm {automation action} {automation}\n</code></pre> <pre><code>cm {automation action} {automation} --help\n</code></pre> <pre><code>cm {automation action} {automation} {artifact}\n</code></pre> <pre><code>cm {automation action} {automation} {artifact} {artifact2} {artifact3} ...\n</code></pre> <pre><code>cm {automation action} {automation} {artifact} --test --meta.a=b @input.json @input.yaml\n</code></pre> <p>The command line arguments are converted into a unified CM dictionary using this function.</p> <p>Flags are converted to the dictionary keys and their argument to the string value.</p> <p>If a flag doesn't have an argument, CM will use boolean value \"true\".</p> <p>If a flag has \".\", it will be treated as dictionary with multiple subkeys separated by \".\".</p> <p>If a flag ends with \",\", tis argument will be treated as a list of values separated by \",\".</p> <p>The CM dictionary is then passed to the  unified CM \"access\" function similar to micro-services and REST API.</p>"},{"location":"specs/cm-cli/#understanding-cm-names","title":"Understanding CM names","text":"<p>The {artifact} has the following format:</p> <ul> <li>\"artifact alias\" (str): may change in the future</li> <li>\"artifact UID\" (16 lowercase hex digits): the same since the creation of the artifact</li> <li>\"alias,UID\":  in such case, UID is used to search for an artifact or automation while alias is used as a user-friendly reminder</li> </ul> <p>It is possible to reference an artifact in a specific CM repository as follows:</p> <ul> <li>\"repo alias:artifact\"</li> <li>\"repo UID:artifact\"</li> <li>\"repo alias,repo UID:artifact\"</li> </ul> <p>Note that automation has the same format as an artifact and is stored as a CM artifact  in the CM repositories in the \"automation\" directory.</p> <p>CM repository also has the same format as an artifact: alias | UID | alias,UID.</p>"},{"location":"specs/cm-cli/#using-cm-cli-inside-a-cm-repository","title":"Using CM CLI inside a CM repository","text":"<p>If you are inside a CM repository, you can use \".\" to tell CM to detect a repository, automation and artifact in the current directory to avoid writing the names explicitly in the command line.</p> <p>For example, you can add a new artifact in the current repository as following: <pre><code>cm add {some automation} .:{some artifact}\n</code></pre></p> <p>or if you are inside a directory with CM automations, you can use the following: <pre><code>cm add . {some artifact}\n</code></pre></p>"},{"location":"specs/cm-cli/#cm-automations","title":"CM automations","text":"<p>CM automations are kept as CM artifacts in \"automation\" directories  (see this example).</p> <p>You can add a new automation as follows: <pre><code>cm add automation {new automation name}\n</code></pre></p> <p>A related CM artifact will be created in the \"local\" CM repository. You can create it in the other CM repository as follows: <pre><code>cm add automation {target CM repository}:{new automation name}\n</code></pre></p> <p>You can also move the new automation from local repository to the existing one as follows: <pre><code>cm move automation local:{new automation name} {target CM repository}:\n</code></pre></p> <p>The automation artifact has a Python module.py that implements automation actions: <pre><code>ls `cm find automation {new automation name}`\n_cm.json\nmodule.py\n</code></pre></p> <p>By default, it has a \"test\" action that prints the input dictionary that is aggregated  from the CM CLI. It helps you understand the CM CLI and how it is converted into a unified input dictionary.</p> <p>You can add a new function \"new_action\" in a similar way as \"test\" to make it a new automation action that can be invoked from the command line as follows: <pre><code>cm new_action {new automation name} ...\n</code></pre>  or <pre><code>cm new-action {new automation name} ...\n</code></pre></p>"},{"location":"specs/cm-cli/#cm-common-automation-actions","title":"CM common automation actions","text":"<p>All CM automations inherit common database function from this  Automation class.</p>"},{"location":"specs/cm-cli/#add","title":"add","text":"<p>Add new artifact:</p> <pre><code>cm add automation (repo:)artifact\n</code></pre> <p>API: CM docs</p>"},{"location":"specs/cm-cli/#delete","title":"delete","text":"<p>Remove artifact:</p> <p><pre><code>cm delete automation (repo:)artifact\n</code></pre> or <pre><code>cm rm automation (repo:)artifact\n</code></pre> or <pre><code>cm remove automation (repo:)artifact\n</code></pre></p> <p>API: CM docs</p>"},{"location":"specs/cm-cli/#find","title":"find","text":"<p>Find artifact(s):</p> <p><pre><code>cm find automation (repo:)artifact(s) (--tags=tag1,tag2,...)\n</code></pre> or <pre><code>cm search automation (repo:)artifact(s)\n</code></pre> or <pre><code>cm ls automation (repo:)artifact(s)\n</code></pre></p> <p>You can use wildcards.</p> <p>You can also list all available artifacts in all repositories as follows: <pre><code>cm find * *\n</code></pre></p> <p>You can find all artifact in the current repository as follows: <pre><code>cm find .\n</code></pre></p> <p>API: CM docs</p>"},{"location":"specs/cm-cli/#load","title":"load","text":"<p>Load artifact meta description:</p> <p><pre><code>cm load automation (repo:)artifact\n</code></pre> API: CM docs</p>"},{"location":"specs/cm-cli/#update","title":"update","text":"<p>Load artifact meta description:</p> <pre><code>cm update automation (repo:)artifact --meta.{key}={value} (@input.json) (@input.yaml)\n</code></pre> <p>API: CM docs</p>"},{"location":"specs/cm-cli/#rename","title":"rename","text":"<p>Rename or move artifacts:</p> <pre><code>cm move automation src-artifact (new repo:)target-artifact\n</code></pre> <p>API: CM docs</p>"},{"location":"specs/cm-cli/#cm-internal-automations","title":"CM internal automations","text":""},{"location":"specs/cm-cli/#repo","title":"repo","text":"<p>This automation deals with CM repositories. You can list available actions in this automation as follows: <pre><code>cm help repo\n</code></pre></p> <p>You can get an API/CLI for a specific automation as follows: <pre><code>cm init repo --help\n</code></pre></p>"},{"location":"specs/cm-cli/#automation","title":"automation","text":"<p>This automation helps you to add new automations as follows: <pre><code>cm add automation {new-automation}\n</code></pre></p>"},{"location":"specs/cm-cli/#core","title":"core","text":"<p>This automation has some common actions for the CM core including \"test\": <pre><code>cm help core\n</code></pre></p> <p>For example, you can test the CM as follows: <pre><code>cm test core\n</code></pre></p>"},{"location":"specs/cm-python-interface/","title":"Cm python interface","text":"<p>[ Back to Specs ]</p>"},{"location":"specs/cm-python-interface/#cm-python-api","title":"CM Python API","text":"<p>All CM automations can be accessed in a unified way either via CLI as shown above or via Python API:</p> <pre><code>import cmind\n\ninput={\n  \"action\":\"automation action\",\n  \"automation\":\"automation alias | UID | alias,UID\",\n  ...\n}\n\noutput = cmind.access(input)\n\nif output['return']&gt;0:\n    cmind.error(output)\n\nprint (output)\n</code></pre> <p>The output CM dictionary always has an integer key <code>return</code>.</p> <p>If a given automation action succeeded, the <code>output['return']</code> is equal to zero  and the output dictionary contains the output of this action.</p> <p>Otherwise, <code>output['return'] &gt; 0</code> and <code>output['error']</code>  contains some text explaining CM automation error.</p>"},{"location":"specs/cm-repository/","title":"Cm repository","text":"<p>[ Back to Specs ]</p>"},{"location":"specs/cm-repository/#cm-repository-structure","title":"CM repository structure","text":"<p>By default, CM repositories are stored in <code>$HOME/CM/repos</code> on Linux and <code>%homepath%\\CM\\repos</code> on Windows</p> <p>They have the following structure:</p> <pre><code>{ROOT directory} / \n   _cmr.yaml\n   {CM automation alias | Unique ID} / \n      {CM artifact alias | Unique ID } / \n         _cm.yaml  |&amp;  _cm.json\n         {user scripts, files and directories}\n</code></pre> <p>Feel free to explore two main CM repositories being developed by the open MLCommons taskforce: * internal CM repository (shared inside CM PYPI package) * MLCommons CM-MLOps repository (shared via GitHub)</p>"},{"location":"specs/cm-repository/#root-directory","title":"Root directory","text":"<ul> <li>File cmr.yaml - CM repository description</li> </ul> <pre><code>alias (str): CM name to find this repository\nuid (str): unique ID to find this repository\n\n(desc) (str): user-friendly description\n(git) (bool): True, if it's a Git repository and not a local one\n(prefix) (str): sub-directory inside this repository to keep CM automations and artifacts\n                - useful to keep original software project repository intact\n</code></pre> <p>Example: mlcommons@ck description </p>"},{"location":"specs/cm-repository/#first-level-directories","title":"First level directories","text":"<ul> <li>CM automation aliases | Unique IDs</li> </ul> <p>Examples: mlcommons@ck repo</p>"},{"location":"specs/cm-repository/#second-level-directories","title":"Second level directories","text":"<ul> <li>CM artifact aliases | Unique IDs</li> </ul> <p>Examples: </p> <ul> <li>CM artifacts to wrap CM automations (including \"script\") in mlcommons@ck repo </li> <li>CM artifacts to wrap CM scripts in mlcommons@ck repo</li> </ul>"},{"location":"specs/cm-repository/#third-level-files","title":"Third level files","text":"<ul> <li>_cm.yaml |&amp; _cm.json - CM meta description of a given CM artifact wrapping native scripts, files and directories</li> </ul> <pre><code>{\n  alias (str): CM name to find this artifact\n  uid (str): unique ID to find this artifact\n\n  automation_alias (str): CM automation name for this artifact\n  automation_uid (str): unique ID for the automation for this artifact\n\n  (_base) (str): preload meta description from this base artifact in format \"{automation}::{artifact}\" \n                 and then merge the current meta description with the base.\n                 This mechanism enables simple inheritance of artifact meta descriptions.\n\n  tags (list): list of tags to characterize and find this artifact\n\n  Any other keys required for a related CM automation ...\n}\n</code></pre> <ul> <li>any user scripts, files and directories wrapped by this CM artifact</li> </ul>"},{"location":"specs/cm-repository/#examples","title":"Examples","text":"<ul> <li>CM script to install system dependencies (any Linux and macOS) </li> <li>CM script to install system dependencies (Windows) </li> <li>CM script to detect or install Python </li> <li>CM script to detect LLVM </li> <li>CM script to install prebuilt LLVM </li> <li>CM script to run image classification with ONNX</li> </ul>"},{"location":"specs/cm-tool-architecture/","title":"Cm tool architecture","text":"<p>[ Back to Specs ]</p>"},{"location":"specs/cm-tool-architecture/#cm-internal-architecture","title":"CM internal architecture","text":"<p>Here is a diagram of the main CM classes, functions and internal automations (v1.1.3+):</p> <p></p> <p>The original chart is available here.</p>"},{"location":"specs/cm-tool-architecture/#cm-internal-apis","title":"CM internal APIs","text":"<ul> <li>Docs in sphinx format</li> </ul>"},{"location":"specs/cm-tool-architecture/#contributing-to-cm","title":"Contributing to CM","text":"<ul> <li>Guidelines</li> </ul>"},{"location":"tutorials/","title":"Index","text":"<p>[ Back to index ]</p>"},{"location":"tutorials/#tutorials","title":"Tutorials","text":"<ul> <li>Running Hello World example in a unified way on Windows, Linux and MacOS</li> <li>Testing CM automation language to run image classification on any platform with CPU and GPU</li> <li>Reproducing experiments from the IPOL'22 journal article using CM</li> <li>MLPerf modularization, automation and reproducibility using the CM automation language:</li> <li>Running MLPerf RetinaNet inference benchmark on CPU via CM (Student Cluster Competition'22 tutorial)</li> <li>Running MLPerf BERT inference benchmark on CUDA GPU via CM (official submission)</li> <li>Running all MLPerf inference benchmarks out-of-the-box for MLPerf inference v3.1 community submission</li> <li>Customizing MLPerf inference benchmark and preparing submission</li> <li>Measuring power during MLPerf inference benchmarks</li> <li>Automating TinyMLPerf benchmark</li> <li>Reproducing/replicating Tiny MLPerf benchmark</li> <li>Reproducing/replicating MLPerf training benchmark</li> <li>Adding common CM interface to reproduce research projects and papers</li> <li>Understanding CM concepts</li> <li>Adding new CM scripts and automation pipelines/workflows</li> </ul>"},{"location":"tutorials/automate-mlperf-tiny/","title":"Automate mlperf tiny","text":"<p>[ Back to index ]</p> Click here to see the table of contents.  * [Tutorial: automate TinyMLPerf benchmark](#tutorial-automate-tinymlperf-benchmark)   * [Install CM automation language](#install-cm-automation-language)   * [Install MLCommons CK repository with CM automations](#install-mlcommons-ck-repository-with-cm-automations)   * [Install Python virtual environment](#install-python-virtual-environment)   * [Install EEMBC Energy Runner repository](#install-eembc-energy-runner-repository)   * [Install CIFAR10 for image classification](#install-cifar10-for-image-classification)   * [Setup boards](#setup-boards)     * [STMicroelectronics NUCLEO-L4R5ZI](#stmicroelectronics-nucleo-l4r5zi)   * [Download and run EEMBC Energy Runner](#download-and-run-eembc-energy-runner)   * [Build and run TinyMLPerf benchmarks](#build-and-run-tinymlperf-benchmarks)   * [Prepare submission](#prepare-submission)   * [Visualize and compare results](#visualize-and-compare-results)   * [Contact MLCommons task force on automation and reproducibility](#contact-mlcommons-task-force-on-automation-and-reproducibility)"},{"location":"tutorials/automate-mlperf-tiny/#tutorial-automate-tinymlperf-benchmark","title":"Tutorial: automate TinyMLPerf benchmark","text":"<p>The MLCommons task force on automation and reproducibility is developing an open-source Collective Knowledge platform to make it easier for the community to run, visualize and optimize MLPerf benchmarks  out of the box across diverse software, hardware, models and data.</p> <p>This tutorial demonstrates how to automate a common setup for the Tiny MLPerf benchmark and EEMBC Energy runner with the help  of the MLCommons CM automation language on Linux or Windows.</p> <p>If you have any questions about this tutorial, please get in touch via our public Discord server or open a GitHub issue here.</p>"},{"location":"tutorials/automate-mlperf-tiny/#install-cm-automation-language","title":"Install CM automation language","text":"<p>Follow this guide  to install the MLCommons CM automation language on your platform. </p> <p>We have tested this tutorial with Ubuntu 20.04 and Windows 10.</p>"},{"location":"tutorials/automate-mlperf-tiny/#install-mlcommons-ck-repository-with-cm-automations","title":"Install MLCommons CK repository with CM automations","text":"<pre><code>cm pull repo mlcommons@ck\n</code></pre> <p>If you have been using CM and would like to have a clean installation, you can clean CM cache as follows: <pre><code>cm rm cache -f\n</code></pre></p>"},{"location":"tutorials/automate-mlperf-tiny/#install-python-virtual-environment","title":"Install Python virtual environment","text":"<p>Since EEMBC Energy Runner and TinyMLPerf data sets require many specific Python dependencies, we suggest you to install Python virtual environment using CM as follows:</p> <pre><code>cm run script \"install python-venv\" --name=tiny --version_min=3.9\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=tiny\"\n</code></pre> <p>You can find its location in CM cache as follows: <pre><code>cm show cache --tags=python-venv\n</code></pre></p>"},{"location":"tutorials/automate-mlperf-tiny/#install-eembc-energy-runner-repository","title":"Install EEMBC Energy Runner repository","text":"<p>You can use CM to install the dependencies for the EEMBC Energy Runner  and prepare a required directory structure for all TinyMLPerf benchmarks using this CM script: <pre><code>cm run script \"get eembc energy-runner src\"\n</code></pre></p> <p>This CM script will download sources and create an <code>eembc</code> directory  in your $HOME directory on Linux, Windows or MacOs with partial datasets required by TinyMLPerf.</p>"},{"location":"tutorials/automate-mlperf-tiny/#install-cifar10-for-image-classification","title":"Install CIFAR10 for image classification","text":"<p>CIFAR10 data set is not included in EEMBC energy-runner GitHub repo and can be generated on your machine using this CM script: <pre><code>cm run script \"get dataset cifar10 _tiny\"\n</code></pre></p> <p>This script will download CIFAR10 data set in the Python (TensorFlow) format  together with the TinyMLPerf sources. It will then generate samples required for EEMBC Energy Runner in the following directory: <code>$HOME/eembc/runner/benchmarks/ulp-mlperf/datasets/ic01</code></p>"},{"location":"tutorials/automate-mlperf-tiny/#setup-boards","title":"Setup boards","text":""},{"location":"tutorials/automate-mlperf-tiny/#stmicroelectronics-nucleo-l4r5zi","title":"STMicroelectronics NUCLEO-L4R5ZI","text":"<p>If you run EEMBC Energy runner on Linux, please check that you have this rule  installed in <code>/usr/lib/udev/rules.d</code>. If not, please copy it there and unplug/replug the board! See related ticket for more details.</p> <p>Here is the list of CM automations scripts that can be reused in your experiments for this (and other) board:</p> <ul> <li>Get EEMBC Energy Runner repository</li> <li>Get TinyMLPerf repository</li> <li>Get CIFAR10 dataset</li> <li>Build Tiny Models</li> <li>Flash Tiny Models</li> <li>Get Zephyr</li> <li>Get Zephyr SDK</li> <li>Get MictoTVM</li> <li>Get CMSIS_5</li> </ul>"},{"location":"tutorials/automate-mlperf-tiny/#download-and-run-eembc-energy-runner","title":"Download and run EEMBC Energy Runner","text":"<p>Download EEMBC Energy Runner for your platform from this website and run it. Normally, you should be able to see and initialize the connected board as described  here.</p>"},{"location":"tutorials/automate-mlperf-tiny/#build-and-run-tinymlperf-benchmarks","title":"Build and run TinyMLPerf benchmarks","text":"<p>You can now follow this tutorial to build, flash and run image classification and keyword spotting benchmarks with MicroTVM, Zephyr and CMSIS on NUCLEO-L4R5ZI. It was prepare for the TinyMLPerf v1.1 submission round as a part of this MLCommons community challenge.</p> <p>You can then follow the official README to run benchmarks in performance, accuracy and energy modes.</p>"},{"location":"tutorials/automate-mlperf-tiny/#prepare-submission","title":"Prepare submission","text":"<p>We plan to automate TinyMLPerf submission for any hardware/software stack during the next submission round.</p>"},{"location":"tutorials/automate-mlperf-tiny/#visualize-and-compare-results","title":"Visualize and compare results","text":"<p>Please follow this README to import TinyMLPerf results (public or private) to the CM format to visualize and compare them on your local machine while adding derived metrics and providing constraints as shown in the following example:</p> <p></p> <p>We publish all public TinyMLPerf results in the MLCommons CK platform to help the community analyze, compare, reproduce, reuse and improve these results.</p> <p>The ultimate goal of our MLCommons task force and the free MLCommons CK platform is to help users automatically generate Pareto-efficient end-to-end applications using MLPerf results based on their requirements and constraints (performance, accuracy, energy, hardware/software stack, costs).</p>"},{"location":"tutorials/automate-mlperf-tiny/#contact-mlcommons-task-force-on-automation-and-reproducibility","title":"Contact MLCommons task force on automation and reproducibility","text":"<p>Please join the MLCommons task force on automation and reproducibility to get free help to automate and optimize MLPerf benchmarks for your software and hardware stack using the MLCommons CM automation language!</p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/","title":"Common interface to reproduce research projects","text":"<p>[ Back to index ]</p> Click here to see the table of contents.  * [Tutorial: Adding a common interface to reproduce research projects](#tutorial-adding-a-common-interface-to-reproduce-research-projects)   * [Motivation](#motivation)   * [Examples:](#examples)   * [Installing CM language](#installing-cm-language)   * [First approach: adding CM script to a common repository](#first-approach-adding-cm-script-to-a-common-repository)     * [Fork main repository](#fork-main-repository)     * [Copy demo script](#copy-demo-script)     * [Edit wrappers](#edit-wrappers)     * [Set up virtual environment before running experiments](#set-up-virtual-environment-before-running-experiments)     * [Run experiments](#run-experiments)   * [Second approach: adding CM interface to your research project](#second-approach-adding-cm-interface-to-your-research-project)     * [Local directory](#local-directory)     * [Git project](#git-project)   * [Adding CM script to prepare and run your experiment](#adding-cm-script-to-prepare-and-run-your-experiment)   * [Testing and extending CM script](#testing-and-extending-cm-script)   * [Adding extra Git repositories with artifacts to dependencies](#adding-extra-git-repositories-with-artifacts-to-dependencies)   * [Sharing this script with the community and artifact evaluators](#sharing-this-script-with-the-community-and-artifact-evaluators)   * [Automating experiments, autotuning and visualization](#automating-experiments-autotuning-and-visualization)   * [Participating in discussions and developments](#participating-in-discussions-and-developments)"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#tutorial-adding-a-common-interface-to-reproduce-research-projects","title":"Tutorial: Adding a common interface to reproduce research projects","text":""},{"location":"tutorials/common-interface-to-reproduce-research-projects/#motivation","title":"Motivation","text":"<p>While working with the community to reproduce and/or replicate 150+ research papers  during artifact evaluation, we have seen that reviewers spend most of their time at the kick-the-tires phase deciphering numerous ad-hoc READMEs and scripts to figure out how to prepare and run shared applications. </p> <p>That motivated us to develop a simple automation language (Collective Mind)  to provide the same common interface to prepare, run and visualize experiments from any paper or research project.</p> <p>The goal is to make it easier for the community and evaluators  to start reproducing/replicating research results  and even fully automate this process in the future.</p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#examples","title":"Examples:","text":"<ul> <li>CM tutorial to reproduce an IPOL journal paper.</li> <li>CM script to reproduce results from a MICRO paper.</li> </ul>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#installing-cm-language","title":"Installing CM language","text":"<p>CM requires minimal dependencies (Python 3+ and Git) and it should be straightforward to install it on Linux, MacOS, Windows and any other platform using this guide.</p> <p>If you encounter any issue, please don't hesitate to tell us via Discord server  and/or open a ticket here.</p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#first-approach-adding-cm-script-to-a-common-repository","title":"First approach: adding CM script to a common repository","text":""},{"location":"tutorials/common-interface-to-reproduce-research-projects/#fork-main-repository","title":"Fork main repository","text":"<p>You can simply add a new CM script to the MLCommons repository  to prepare, run and visualize your experiments without any changes required to your original repository.</p> <p>First, create your fork of this repository and pull it via CM: <pre><code>cm pull repo --url={fork of https://github.com/ctuning/mlcommons-ck}\n</code></pre></p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#copy-demo-script","title":"Copy demo script","text":"<p>You can then copy the CM script \"reproduce-micro-paper-2023-victima\" from a MICRO paper with Artifact Evaluation to a new script with your paper/artifact name as follows: <pre><code>cm copy script reproduce-micro-paper-2023-victima reproduce-micro-paper-2023-{new name}\n</code></pre></p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#edit-wrappers","title":"Edit wrappers","text":"<p>You can then find its directory and edit <code>_cm.yaml</code>, <code>install_deps.sh</code>, <code>run.sh</code> and <code>plot.sh</code> to pull your Git repository with your artifacts and call your scripts to install dependencies, run experiments and plot results from above wrappers: <pre><code>cm find script reproduce-micro-paper-2023-{new name}\n</code></pre></p> <p>You can now add this directory to your fork and create a PR.</p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#set-up-virtual-environment-before-running-experiments","title":"Set up virtual environment before running experiments","text":"<p>We suggest to use virtual environment installed via CM as follows: <pre><code>cm run script \"install python-venv\" --name=ae\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=ae\"\n</code></pre></p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#run-experiments","title":"Run experiments","text":"<p>Each paper should have a CM script with the following variations: <code>install_deps</code>, <code>run</code> and <code>plot</code></p> <p>It's possible to run them as follows: <pre><code>cm find script --tags=reproduce,paper\ncm run script {name from above list} --tags=_{variation}\n</code></pre></p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#second-approach-adding-cm-interface-to-your-research-project","title":"Second approach: adding CM interface to your research project","text":"<p>First, you also need to install the following CM repository with reusable CM scripts  shared by the MLCommons task force on automation and reproducibility to make it easier to run and reproduce ML and Systems applications and benchmarks across diverse software and hardware:</p> <pre><code>cm pull repo ctuning@mlcommons-ck\n</code></pre>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#local-directory","title":"Local directory","text":"<p>To initialize a CM repository/interface in a local project directory, you need to go there and run the following CM command while substituting <code>my-research-project</code> with a unique name:</p> <pre><code>cm init repo my-research-project --path=. --prefix=cmr\n</code></pre> <p>If this research project is associated with artifact evaluation, we suggest to use the name of the conference and your paper ID such as <code>micro-2023-038</code> for ACM MICRO.</p> <p>This command will create a <code>cmr.yaml</code> file with a description and unique ID of this repository, and will register it in the CM. Note that all CM automations and artifacts will be located in the <code>cmr</code> sub-directory to avoid contaminating your project. They can be deleted or moved to another project at any time.</p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#git-project","title":"Git project","text":"<p>If you use Git for your project, we suggest to initialize your repository as follows: <pre><code>cm pull repo my-research-project --url={Git URL}\n</code></pre></p> <p>In such case, CM will pull your Git repository, initialize CM interface and register it in the CM.</p> <p>You can see your registered CM repositories as follows: <pre><code>cm list repo\n</code></pre></p> <p>You can find the location of your Git repository in CM for further editing as follows: <pre><code>cm find repo my-research-project\n</code></pre></p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#adding-cm-script-to-prepare-and-run-your-experiment","title":"Adding CM script to prepare and run your experiment","text":"<p>You can now add CM interface (CM scripts)  to your CM-compatible research project to wrap your native scripts and run experiments as follows:</p> <pre><code>cm add script my-research-project:reproduce-paper-micro-2023-016 \\\n           --tags=reproduce,paper,micro,2023,016 \\\n           --script_name={name of your script} \\\n           --template=ae-python\n</code></pre> <p>Please add <code>--json</code> flag if you prefer to describe your dependencies and execution workflow in JSON instead of YAML format (default).</p> <p>You can now find and edit the newly created template script using its alias as follows: <pre><code>cm find script reproduce-paper-micro-2023-016\n</code></pre> You can also use tags to find your template script (preferred way since alias may change in the future): <pre><code>cm find script --tags=reproduce,paper,micro,2023,016\n</code></pre></p> <p>You must update the script after editing it's meta to cache it for fast search: <pre><code>cm update script reproduce-paper-micro-2023-016\n</code></pre></p> <p>If you use extra frameworks and libraries such as PyTorch and CUDA, you can create a new script with a template \"pytorch\" as follows:</p> <pre><code>cm add script my-research-project:reproduce-paper-micro-2023-016-2 \\\n           --tags=reproduce,paper,micro,2023,016-2\n           --template=pytorch\n</code></pre> <p>It will include all the necessary CM dependencies to detect and/or install Python, CUDA, PyTorch, etc.</p> <p>If you use Python, we suggest to use virtual environment installed via CM as follows: <pre><code>cm run script \"install python-venv\" --name=ae\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=ae\"\n</code></pre></p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#testing-and-extending-cm-script","title":"Testing and extending CM script","text":"<p>You can already run your native script via CM interface and pass environment variables as follows:</p> <pre><code>cm run script \"reproduce micro 2023 paper 016\" --env.KEY1=VAL1 --env.KEY2=VAL2 ...\n</code></pre> <p>Furthermore, you can run extra dummy scripts automatically generated for you via so-called CM variations (<code>install_deps, run, reproduce, plot, analyze,  validate</code>): <pre><code>cm run script \"reproduce micro 2023 paper 016 _{variation}\" --env.KEY1=VAL1 --env.KEY2=VAL2 ...\n</code></pre></p> <p>Each variation invokes an associated script <code>{variation}.sh</code> or <code>{variation}.bat</code> and can be extended by a user.</p> <p>From this moment, you can extend meta description of this script (<code>_cm.json</code> or <code>_cm.yaml</code>) to add dependencies on other CM scripts to detect and/or install tools, data sets, models and other artifacts, extend <code>customize.py</code> to update environment variables based on dependencies and run other native scripts before executing the main native script, post-process and unify output, etc.</p> <p>You can read about the CM script concept here and use the most related CM scripts as examples/templates  to extend your own script.</p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#adding-extra-git-repositories-with-artifacts-to-dependencies","title":"Adding extra Git repositories with artifacts to dependencies","text":"<p>If you artifact/experiment depends on various Git repositories, you can add them as dependencies to your script in <code>_cm.yaml</code> or <code>_cm.json</code>to be automatically downloaded and cached as shown in the following example:</p> <pre><code>deps:\n- tags: get,git,repo,_repo.https://github.com/CMU-SAFARI/Victima\n  env:\n    CM_GIT_ENV_KEY: 'CMU_SAFARI_VICTIMA'\n  extra_cache_tags: micro23,artifact,ae,cmu,safari,victima\n</code></pre> <p>The path to this cached GitHub repository will be available in your native scripts  via <code>CM_GIT_REPO_CMU_SAFARI_VICTIMA_CHECKOUT_PATH</code> environment variable.</p> <p>You can also find cached repo via CM CLI as follows: <pre><code>cm find cache --tags=get,git,cmu,safari,victima\n</code></pre></p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#sharing-this-script-with-the-community-and-artifact-evaluators","title":"Sharing this script with the community and artifact evaluators","text":"<p>You just need to share/commit <code>cmr</code> directory and <code>cmr.yaml</code> to your project. Another user or artifact evaluator will need to install CM and pull <code>mlcommons@ck</code> repository and your repository as follows: <pre><code>python3 -m pip install cmind -U\n\ncm pull repo mlcommons@ck\ncm pull repo --url={Git URL of your project}\n</code></pre></p> <p>It is now possible to run your CM script in the same way as on your platform: <pre><code>cm run script \"reproduce micro 2023 paper 016\" --env.KEY1=VAL1 --env.KEY2=VAL2 ...\n</code></pre></p> <p>CM will attempt to resolve all dependencies, install the missing ones and then run your native script while adapting to a new platform.</p> <p>If some issue is encountered, CM makes it easier for evaluators and users to work with your  to fix not only a given CM script for your paper but also improve and fix other shared CM scripts  thus helping the community gradually improve portability, reproducibility, replicability and reusability of all research projects  across different environments!</p> <p>It is also possible to use CM automation language in containers and README files  thus helping the community quickly understand how to prepare and run diverse projects.</p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#automating-experiments-autotuning-and-visualization","title":"Automating experiments, autotuning and visualization","text":"<p>We are developing CM \"experiment\" as a higher-level wrapper to CM scripts  to automate experimentation, autotuning, design space exploration, visualization and comparison of experiments.</p> <p>Please follow this tutorial  to learn about how to use the CM experiment automation.</p> <p>Fee free to check other tutorials to understand and apply CM automation  to your projects.</p>"},{"location":"tutorials/common-interface-to-reproduce-research-projects/#participating-in-discussions-and-developments","title":"Participating in discussions and developments","text":"<p>Note that this is an on-going and heavily evolving project - we are working with the community to add and improve CM automations, templates and tutorials.</p> <p>Please join the MLCommons task force on automation and reproducibility via this public Discord server to stay tuned, provide your feedback, and get help to add CM interface to your research projects, reuse and/or extend shared CM scripts,  add the new ones, and participate in our open benchmarking, optimization and reproducibility challenges.</p>"},{"location":"tutorials/concept/","title":"Concept","text":"<p>[ Back to index ]</p> Click here to see the table of contents.  * [Tutorial: understanding CM database concepts](#tutorial-understanding-cm-database-concepts)   * [Installing CM](#installing-cm)   * [Organizing your artifacts](#organizing-your-artifacts)     * [Without CM](#without-cm)     * [With CM](#with-cm)       * [Initializing CM-compatible repository in the directory](#initializing-cm-compatible-repository-in-the-directory)       * [Converting existing Git project to the CM repository](#converting-existing-git-project-to-the-cm-repository)       * [Creating CM artifacts](#creating-cm-artifacts)       * [Finding CM artifacts](#finding-cm-artifacts)       * [Renaming artifact](#renaming-artifact)       * [Moving artifacts to another CM repository](#moving-artifacts-to-another-cm-repository)       * [Deleting artifact](#deleting-artifact)       * [Copying artifacts](#copying-artifacts)       * [Viewing CM meta description](#viewing-cm-meta-description)       * [Creating other types of artifacts](#creating-other-types-of-artifacts)   * [Reusing others' artifacts in the CM format](#reusing-others'-artifacts-in-the-cm-format)     * [From command line](#from-command-line)     * [From Python and Jupyter notebooks](#from-python-and-jupyter-notebooks) * [List repositories](#list-repositories) * [Find an artifact](#find-an-artifact)     * [In Docker containers](#in-docker-containers) * [Adaptive container with the CM interface](#adaptive-container-with-the-cm-interface)     * [From zip file](#from-zip-file)   * [Adding reusable automations for related artifacts](#adding-reusable-automations-for-related-artifacts)   * [Adding reusable automation actions](#adding-reusable-automation-actions)   * [Extending meta descriptions of artifacts](#extending-meta-descriptions-of-artifacts)"},{"location":"tutorials/concept/#tutorial-understanding-cm-database-concepts","title":"Tutorial: understanding CM database concepts","text":"<p>Here we describe a few simple steps to let you try CM automation language  and help you understand the CM concepts.</p> <p>You will install CM v1.1.6+, transform your local directory into a database of reusable artifacts,  share it with others, implement some reusable automation actions to common artifacts, run CM automations from Python and Jupyter Notebooks, and convert any Git repository  into the CM format.</p>"},{"location":"tutorials/concept/#installing-cm","title":"Installing CM","text":"<p>CM language is implemented as a small Python library with a unified CLI and a simple API.</p> <p>It requires minimal dependencies (Python 3+, pip, pyyaml and a Git client)  and should work with any OS including Linux, MacOS, CentOS, Debian, RedHat and Windows.</p> <p>Please follow this guide to install CM on your system.</p>"},{"location":"tutorials/concept/#organizing-your-artifacts","title":"Organizing your artifacts","text":"<p>We use CM to provide a common structure to all software projects and organize all related artifacts  in such a way that it is possible to share, find, reuse, and extend them across different teams and projects with a minimal effort based on FAIR principles.</p>"},{"location":"tutorials/concept/#without-cm","title":"Without CM","text":"<p>Let's imagine that you want to share with your colleagues some machine learning model, an image of a cat,  and a JSON file with some experimental results including inference time and image classification via some GitHub repository.</p> <p>Based on our experience,  you will likely create some local directory \"my-cool-project\" in your $HOME directory to organize related artifacts:</p> <pre><code>mkdir my-cool-project\ncd my-cool-project\n</code></pre> <p>You will then create some ad-hoc directories to store your ML model, image and experimental data:</p> <pre><code>mkdir images\ncp cool-cat.jpeg images\n\nmkdir models\ncp my-cool-model.onnx models\n\nmkdir experiments\ncp my-cool-result-20220404.json experiments\n</code></pre> <p>You will then likely create a README.md describing the structure  and the content of your repository, and how someone can run your experiments.</p> <p>You will then pack this repository or push it to GitHub to share it with the community.</p> <p>Another person will need to read your README file to understand the structure of your repository, reproduce results, customize your code and reuse some artifacts in another project.</p> <p>However, since there thousands of incremental papers and projects published every months and most colleagues are very busy with their own projects, they will simply have no time  to read yet another ad-hoc ReadMe and figure out how to reproduce yet another ad-hoc project:</p> <p></p> <p>However, while reproducing 150+ research papers and validating them in the real world, we got feedback from researchers and practitioners that it would be beneficial to have a common format and a common interface/language to describe how to prepare, run and reproduce results from all papers across any software, hardware, models, and data.</p>"},{"location":"tutorials/concept/#with-cm","title":"With CM","text":""},{"location":"tutorials/concept/#initializing-cm-compatible-repository-in-the-directory","title":"Initializing CM-compatible repository in the directory","text":"<p>The feedback from the community motivated us to develop a simple, technology-agnostic and human-readable automation language to help the community convert their projects into a simple database of reusable automations and artifacts.</p> <p>The idea is to perform all common steps across all research projects in a unified and intuitive way with a cm prefix from the command line:</p> <p>For example, you can initialize a CM repository in your working directory as follows:</p> <pre><code>cm init repo\n</code></pre> <p>CM will create a cmr.yaml file with a global unique ID and will register  this location in the CM-compatible repository index $HOME/CM/repos.json. </p> <p>This is needed to let CM automatically search for reusable artifacts and automations in all CM-compatible directories on your machine and plug them into modular CM projects.</p> <p>If you forget the location, you can always find it using the following CM command: <pre><code>cm find repo my-cool-project\n</code></pre></p> <p>Note that CM will use the name of your current directory as an alias of this CM repository.  You can list already registered CM repositories as follows: <pre><code>cm ls repo\n</code></pre>  or <pre><code>cm ls repo | sort\n\nlocal = C:\\Users\\gfursin\\CM\\repos\\local\ninternal = C:\\!Progs\\Python39\\lib\\site-packages\\cmind-0.7.7-py3.9.egg\\cmind\\repo\nmy-cool-project = ...\n</code></pre></p> <p>You can also create a repository with a specific name in $HOME/CM/repos directory as follows: <pre><code>cm init repo another-cool-project\ncm find repo *cool*\n</code></pre></p>"},{"location":"tutorials/concept/#converting-existing-git-project-to-the-cm-repository","title":"Converting existing Git project to the CM repository","text":"<p>If you already have a Git repository you can pull it via CM and make it a CM-compatible repository in a non-intrusive way as follows:</p> <pre><code>cm pull repo my-cool-project --url={Git repo URL} \ncm find repo \n</code></pre> <p>CM will pull this repository to $HOME/CM/repos/my-cool-project,  will add cmr.yaml file with a global unique ID to let the community know that this repository is CM-compatible, and will register this location in the CM-compatible repository index $HOME/CM/repos.json. </p> <p>Note that you always have at least 2 CM-compatible repositories after you use CM for the first time:</p> <ul> <li> <p>internal is a CM repository with reusable artifacts and automations that were moved    inside the CM toolkit    to ensure their stability because they are frequently used by the community.</p> </li> <li> <p>local is a CM scratchpad repository where all new artifacts and automations    are created by default if a CM repository is not specified.</p> </li> </ul>"},{"location":"tutorials/concept/#creating-cm-artifacts","title":"Creating CM artifacts","text":"<p>You can now use CM to create a very similar structure as in your original Git repository but with some meta information in JSON and/or YAML format to describe your artifacts to make them findable, interoperable and reusable.</p> <p>The format of the CM to add artifacts is the following: <pre><code>cm add {common artifact automation name} {artifact name} \n</code></pre></p> <p>By default, CM will create new artifacts in the \"local\" CM repository (scratchpad). You can specify another CM repository as follows: <pre><code>cm add {common artifact automation name} {CM repo}:{artifact name}\n</code></pre></p> <p>You can also add some tags to describe a given artifact as follows: <pre><code>cm add {common artifact automation name} {CM repo}:{artifact name} --tags=tag1,tag2,tag3...\n</code></pre></p> <p>In our case, let's use \"images\" as our automation (artifact type).  Note that you can either use  any name to organize your artifacts or reuse an existing automation with some common automation actions shared by the community or within workgroups  as described later in this tutorial.</p> <pre><code>cm add images my-cool-project:cool-cat --tags=dataset,image,cool,cat\n</code></pre> <p>CM will create a directory images/cool-cat inside my-cool-project repository and added _cm.json with extensible meta description: <pre><code>{\n  \"alias\": \"cool-cat\",\n  \"automation_alias\": \"images\",\n  \"automation_uid\": \"\",\n  \"tags\": [\n    \"dataset\",\n    \"image\",\n    \"cool\",\n    \"cat\"\n  ],\n  \"uid\": \"780abfe6b8084327\"\n}\n</code></pre></p> <p>You will have a different UID on your system - you should use it instead of \"780abfe6b8084327\".</p> <p>Note that CM generated a unique ID for this artifact - that allows any  CM artifact to be findable and reusable in the world similar to a global database (Collective Mind) where another artifact with a similar name (alias) may already exist. In such case, we can use UID in our projects to make sure that we find and reuse a unique artifact.</p> <p>Also note that if you want to create another artifact in a CM repository, you can tell CM to use current CM repository and artifact type using \".\" instead of tying the full name:</p> <pre><code>cd automation\ncm add . cool-cat-v2 --tags=dataset,image,cool,cat-v2\n</code></pre> <p>CM will create cool-cat-v2 in the current CM repository rather than in the \"local\" repository.</p>"},{"location":"tutorials/concept/#finding-cm-artifacts","title":"Finding CM artifacts","text":"<p>Since CM keeps track of all CM-compatible repositories, it is now possible to find any artifact  using its name (alias), UID or tags:</p> <pre><code>cm find images cool-cat\ncm find images 780abfe6b8084327\ncm find images *cat*\ncm find images --tags=image,cat\n</code></pre> <p>Note that you can also reference your CM artifact by alias and UID at the same time: <pre><code>cm find images cool-cat,780abfe6b8084327\n</code></pre></p> <p>In such case, CM will ignore above alias and will search for an artifact by UID.  However, you can still see the original name of the artifact instead of a cryptic UID. If this name (alias) changes in the future, CM will still be able to find it using its UID!</p> <p>You can now use this CM artifact directory as a findable placeholder for your raw artifacts. For example, you can You can now copy your cool-cat.jpeg and any related files to this directory: <pre><code>cp cool-cat.jpeg `cm find images cool-cat`\n</code></pre></p> <p>Now, we will be able to find and reuse all generated or manually created artifacts  on our own machines or in a cloud even years later!</p> <p>Furthermore, we can use the same command line language to describe our repository in READMEs and containers thus providing a common artifact management language for projects.</p>"},{"location":"tutorials/concept/#renaming-artifact","title":"Renaming artifact","text":"<p>If needed, you can rename your artifact using CM to keep UID intact as follows: <pre><code>cm rename images cool-cat-v2 cool-cat-v3\n</code></pre></p>"},{"location":"tutorials/concept/#moving-artifacts-to-another-cm-repository","title":"Moving artifacts to another CM repository","text":"<p>You can move a given artifact to any CM repository using standard OS commands.</p> <p>However, you can also use CM CLI for your convenience:</p> <p><pre><code>cm move images cool-cat-v3 local:\n</code></pre> This command will move images::cool-cat-v3 artifact to \"local\" repository.</p>"},{"location":"tutorials/concept/#deleting-artifact","title":"Deleting artifact","text":"<p>You can also delete your artifacts using standard OS commands.</p> <p>However, you can also use CM CLI for your convenience:</p> <p><pre><code>cm rm images cool-*-v3\n</code></pre> This command will remove images::cool-cat-v3 artifact.</p>"},{"location":"tutorials/concept/#copying-artifacts","title":"Copying artifacts","text":"<p>CM allows you to use existing artifacts as templates for new artifacts. </p> <p>You can copy an artifact to a new one with a new alias  (new UID will be generated automatically) as follows:</p> <p><pre><code>cm copy images cool-cat-v3 .:cool-cat-v4\n</code></pre> This command will copy images::cool-cat-v4 artifact to  images::cool-cat-v4 in the same repository (specified by .*)</p>"},{"location":"tutorials/concept/#viewing-cm-meta-description","title":"Viewing CM meta description","text":"<p>You can use the following CM command to view the meta description of a given artifact:</p> <pre><code>cm load images cool-cat\n\n{\n  \"alias\": \"cool-cat\",\n  \"automation_alias\": \"images\",\n  \"automation_uid\": \"\",\n  \"tags\": [\n    \"dataset\",\n    \"image\",\n    \"cool\",\n    \"cat\"\n  ],\n  \"uid\": \"780abfe6b8084327\"\n}\n</code></pre> <p>or <pre><code>cm load images --tags=cool,cat\n</code></pre></p>"},{"location":"tutorials/concept/#creating-other-types-of-artifacts","title":"Creating other types of artifacts","text":"<p>Similarly, you can create CM artifacts for your ML model</p> <pre><code>cm add models my-cool-model --tags=model,ml,onnx,image-classification\n\ncm find models my-cool-project:*\n\ncp my-cool-model.onnx `cm find models my-cool-model`/model.onnx\n\nls `cm find models my-cool-model`\n\n_cm.json\nmodel.onnx\n</code></pre> <pre><code>cm add experiments cool-result --tags=experiment,inference,image-classification,cat,20220404\n\ncm ls experiments\n\ncp my-cool-result-20220404.json `cm find experiments cool-result`\n\nls `cm find experiments cool-result`\n\n _cm.json\n my-cool-result-20220404.json\n</code></pre> <p>You can now update the README.md of your repository to specify CM commands  and you can add the following badges to tell the community  that it is CM compatible:</p> <p> </p> <p>This will signal the community that they can now understand your README with the CM language, access your project via unified CM CLI or Python API, and even apply new automations.</p>"},{"location":"tutorials/concept/#reusing-others-artifacts-in-the-cm-format","title":"Reusing others' artifacts in the CM format","text":"<p>Whenever you see a CM-compatible repository, you can use CM language to manage it and reuse its automations and artifacts in your own project.</p> <p>You can also use MLCommons CK platform with a user-friendly GUI (under development) or integrate it with existing DevOps and MLOps platforms and tools.</p>"},{"location":"tutorials/concept/#from-command-line","title":"From command line","text":"<pre><code>cm pull repo my-cool-project --url={GitHub repo URL} \ncm find experiments\ncm load experiments cool-result\n</code></pre>"},{"location":"tutorials/concept/#from-python-and-jupyter-notebooks","title":"From Python and Jupyter notebooks","text":"<p>CM provides a simple and unified access function to all CM repositories similar to micro-services and ElasticSearch with an input and output as a unified CM dictionary:</p> <pre><code>import cmind\n\n# List repositories\n\nr=cmind.access({'action':'find', 'automation':'repo'})\nif r['return']&gt;0: cmind.error(r)\n\nprint (r)\n\n# Find an artifact \n\nr=cmind.access({'action':'load', 'automation':'images', 'artifact':'cool-cat'})\nif r['return']&gt;0: cmind.error(r)\n\nprint (r['path'])\n</code></pre> <pre><code>{ \n  'return': 0, \n  'path': 'C:\\\\Users\\\\gfursin\\\\CM\\\\repos\\\\my-cool-project\\\\images\\\\cool-cat', \n  'meta': {\n    'alias': 'cool-cat', \n    'automation_alias': 'images', \n    'automation_uid': '', \n    'tags': [], \n    'uid': 'f94970b1af7c49db'\n  }, \n  'artifact': &lt;cmind.artifact.Artifact object at 0x000002A1B499AE20&gt;\n}\n</code></pre> <p>You can see the Python class for a CM artifact here.</p> <p>Note that \"automation_uid\" is empty because CM doesn't know yet if your artifact types exists globally and thus can't add CM UID. We will explain how to reuse shared artifact types and automations later in this tutorial.</p>"},{"location":"tutorials/concept/#in-docker-containers","title":"In Docker containers","text":"<p>We can use CM commands to create modular containers:</p> <pre><code># Adaptive container with the CM interface\n\nFROM ubuntu:20.04\n\nLABEL maintainer=\"Grigori Fursin &lt;grigori.fursin@cTuning.org&gt;\"\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEBIAN_FRONTEND=noninteractive\n\nRUN apt update &amp;&amp; \\\n    apt install -y --no-install-recommends \\\n           apt-utils \\\n           git wget zip bzip2 libz-dev libbz2-dev cmake curl unzip \\\n           openssh-client vim mc tree \\\n           gcc g++ autoconf autogen libtool make libc6-dev build-essential patch \\\n           gfortran libblas-dev liblapack-dev \\\n           libsndfile1-dev libssl-dev libbz2-dev libxml2-dev libtinfo-dev libffi-dev \\\n           python3 python3-pip python3-dev \\\n           libtinfo-dev \\\n           python-is-python3 \\\n           libncurses-dev \\\n           sudo\n\nRUN python3 -m pip install cmind\n\nRUN cm pull repo mlcommons@ck\n\nRUN cm find automation\n\nRUN cm ...\n</code></pre>"},{"location":"tutorials/concept/#from-zip-file","title":"From zip file","text":"<p>You can pack your CM repository to a zip file as follows: <pre><code>cm pack repo my-cool-project\n\nPacking repo from C:\\Users\\gfursin\\CM\\repos\\my-cool-project to cm.zip ...\n</code></pre></p> <p>You can then share cm.zip with your colleagues who can unpack it  and install on their system using the following CM command: <pre><code>cm unpack repo\n\ncm find images\ncm find experiments\n</code></pre></p>"},{"location":"tutorials/concept/#adding-reusable-automations-for-related-artifacts","title":"Adding reusable automations for related artifacts","text":"<p>One of the goals of the CM language is to gradually systematize  all available artifacts and provide reusable automation actions  to similar artifact types.</p> <p>To be able to add automation actions to your artifact types and reuse them with others, you need to add a CM automation for your artifact type as follows:</p> <pre><code>cm add automation {artifact type}\n</code></pre> <p>For example, you can add the following automations for this tutorial: <pre><code>cm add automation images\ncm add automation experiments\ncm add automation models\n</code></pre></p> <p>Note that CM will add those automations to the \"local\" CM repository. You can add them to another public or private repository by using \":\" separator as follows: <pre><code>cm add automation my-cool-project:images\n</code></pre></p> <p>Or you can move your existing automation to another CM repository as follows: <pre><code>cm move automation local:images my-cool-project:\n</code></pre></p> <p>Now, whenever you add a new artifact with an associated automation, CM will find this automation and record \"automation_uid\" in the meta description  of the newly created artifact to be able to reuse common automation actions.</p>"},{"location":"tutorials/concept/#adding-reusable-automation-actions","title":"Adding reusable automation actions","text":"<p>We use Python as a simplified and portable DSL (domain specific language)  to implement reusable automation actions for similar artifact types. </p> <p>You can find a Python module for your automation as follows: <pre><code>cm find automation images\n</code></pre></p> <p>This directory will include a meta description of this automation in _cm.json and a module.py with the automation actions implemented as standard Python functions.</p> <p>This module inherits default \"CM database actions\" from the Automation class  in the CM package such as \"add\", \"rm\", \"find\", \"rename\", etc.</p> <p>It also includes a \"test\" automation action to help you understand the CM CLI: <pre><code>cm test images\n\n{\n  \"action\": \"test\",\n  \"automation\": \"images\",\n  \"out\": \"con\",\n  \"parsed_automation\": [\n    [\n      \"images\",\n      \"...\"\n    ]\n  ]\n}\n</code></pre></p> <p>You can add your own automation actions to this module that will be immediately accessible  from the command line: <pre><code>cm {my-new-automation-action} images\n...\n</code></pre> Note that all '-' characters in the automation action from the CLI will be converted into '_'.</p> <p>Please check the following examples of internal CM automations to understand how to write your own automation actions and apply them to artifacts: CM internal repo</p> <p>Now you can share your automation for a given artifact type in your private repository with your colleagues or in your public repository with the whole world.</p> <p>Others can pull your repository via cm pull repo ... and start reusing the common automations and artifacts in their own projects.</p> <p>Furthermore, everyone can now extend existing automation actions  or contribute the new ones instead of writing their own ad-hoc scripts  and artifact/project management frameworks.</p>"},{"location":"tutorials/concept/#extending-meta-descriptions-of-artifacts","title":"Extending meta descriptions of artifacts","text":"<p>Besides adding new common automation actions, the community can also gradually  extend JSON or YAML files of shared artifacts to find a better way to describe them when reusing them across different projects.</p> <p>We hope that such Wikipedia-style mechanisms will help the community to gradually decompose  all complex software and research projects into a collection of reusable artifacts and automation actions as we successfully did for MLPerf benchmarks.</p> <p>Feel free to join our MLCommons task force on automation and reproducibility if you have questions or would like to participate in further collaborative developments.</p>"},{"location":"tutorials/hello-world/","title":"Hello world","text":"<p>[ Back to index ]</p>"},{"location":"tutorials/hello-world/#tutorial-run-python-hello-world-app-on-linux-windows-and-macos","title":"Tutorial: run Python Hello World app on Linux, Windows and MacOS","text":"<pre><code>python3 -m pip install cmind\n</code></pre> <p>You may need to restart bash to add <code>cm</code> and <code>cmr</code> binaries to your PATH,</p> <pre><code>cm pull repo mlcommons@ck\ncm run script --tags=print,python,hello-world\ncmr \"print python hello-world\"\n</code></pre> <p>This CM script is a simple wrapper to native scripts and tools with a common CLI and API described by a simple declarative YAML configuration file that specifies wrapper inputs, environment variables and dependencies on other portable CM scripts:</p> <pre><code>alias: print-hello-world-py\nuid: d83274c7eb754d90\n\nautomation_alias: script\nautomation_uid: 5b4e0237da074764\n\ndeps:\n- tags: detect,os\n- tags: get,sys-utils-cm\n- names:\n  - python\n  tags: get,python3\n\ntags:\n- print\n- hello-world\n- python\n</code></pre>"},{"location":"tutorials/mlperf-inference-power-measurement/","title":"Mlperf inference power measurement","text":"<p>[ Back to index ]</p>"},{"location":"tutorials/mlperf-inference-power-measurement/#tutorial-automating-power-measurements-for-mlperf-inference-using-mlcommons-cm","title":"Tutorial: Automating Power Measurements for MLPerf inference using MLCommons CM","text":"<p>Prepared by the MLCommons taskforce on automation and reproducibility and OctoML.</p>"},{"location":"tutorials/mlperf-inference-power-measurement/#requirements","title":"Requirements","text":"<ol> <li> <p>Power analyzer (anyone certified by SPEC PTDaemon).     Yokogawa is the one that most submitters have submitted with and a new single-channel model like 310E can cost around 3000$.    The MLCommons taskforce on automation and reproducibility      is also using the Yokogawa 310E     to automate and simplify MLPerf submissions.</p> </li> <li> <p>SPEC PTDaemon (can be downloaded from here after signing the EULA which can be requested     by sending an email to <code>support@mlcommons.org</code>). Once you have GitHub access to the MLCommons power repository then the CM workflow     will automatically download and configure the SPEC PTDaemon tool.</p> </li> <li> <p>Access to the MLCommons power-dev repository     which has the <code>server.py</code> to be run on the director node and <code>client.py</code> to be run on the SUT node. This repository being public will be     automatically pulled by the CM workflow.</p> </li> </ol>"},{"location":"tutorials/mlperf-inference-power-measurement/#connecting-power-analyzer-to-the-computer","title":"Connecting power analyzer to the computer","text":"<p>We need to connect the power analyzer to a director machine via USB and the machine must be running Linux  (Ethernet mode is supported only on Windows which this workflow is not supporting yet).  The power supply to the SUT is done through the power analyzer (current in series and voltage in parallel).  An adapter like this can help avoid cutting the electrical wires. </p> <p>.</p> <p>The director machine runs the <code>server.py</code> script and loads a server process that communicates with the SPEC PTDaemon.  When a client connects to it (using <code>client.py</code>), it in turn connects to the PTDaemon and initiates a measurement run.  Once the measurement ends, the power log files are transferred to the client. </p>"},{"location":"tutorials/mlperf-inference-power-measurement/#ranging-mode-and-testing-mode","title":"Ranging mode and Testing mode","text":"<p>Power analyzers usually have different current and voltage ranges it supports and the exact ranges to be used  depends on a given SUT and this needs some empirical data. We can do a ranging run where the current and voltage ranges  are set to <code>Auto</code> and the power analyzer automatically figures out the correct ranges needed.  These determined ranges are then used for a proper testing mode run.  Using the 'auto' mode in a testing run is not allowed as it can mess up the measurements.</p>"},{"location":"tutorials/mlperf-inference-power-measurement/#setup-using-mlcommons-cm","title":"Setup using MLCommons CM","text":""},{"location":"tutorials/mlperf-inference-power-measurement/#install-cm","title":"Install CM","text":"<p>Please follow these instructions to install the MLCommons CM automation tool.</p>"},{"location":"tutorials/mlperf-inference-power-measurement/#start-power-server-power-analyzer-should-be-connected-to-this-computer-and-ptdaemon-runs-here","title":"Start Power Server (Power analyzer should be connected to this computer and PTDaemon runs here)","text":"<p>If you are having GitHub access to MLCommons power repository, PTDaemon should be automatically installed using the following CM command:</p> <p>PS: The below command will ask for <code>sudo</code> permission on Linux and should be run with administrator privilege on Windows (to do NTP time sync). <pre><code>cm run script --tags=mlperf,power,server --device_type=49 --device_port=/dev/usbtmc0\n</code></pre> * <code>`--interface_flag=\"-U\" and</code>--device_port=1<code>(can change as per the USB slot used for connecting) can be used on Windows for USB connection *</code>--device_type=49<code>corresponds to Yokogawa 310E and</code>ptd -h<code>should list the device_type for all supported devices. The location of</code>ptd<code>can be found using the below command *</code>--device_port=20<code>and</code>--interface_flag=\"-g\" can be used to connect to GPIB interface (currently supported only on Windows) with the serial address set to 20 <pre><code>cat `cm find cache --tags=get,spec,ptdaemon`/cm-cached-state.json\n</code></pre></p> <p>More configuration options can be found here.</p>"},{"location":"tutorials/mlperf-inference-power-measurement/#running-the-power-server-inside-a-docker-container","title":"Running the power server inside a docker container","text":"<p><pre><code>cm docker script --tags=run,mlperf,power,server --docker_gh_token=&lt;GITHUB AUTH_TOKEN&gt; \\\n--docker_os=ubuntu --docker_os_version=22.04 --device=/dev/usbtmc0\n</code></pre> * Device address may need to be changed depending on the USB port being used * The above command uses a host-container port mapping 4950:4950 which can be changed by using <code>--docker_port_maps,=4950:4950</code></p>"},{"location":"tutorials/mlperf-inference-power-measurement/#running-a-dummy-workload-with-power-on-host-machine","title":"Running a dummy workload with power (on host machine)","text":"<pre><code>cm run script --tags=mlperf,power,client --power_server=&lt;POWER_SERVER_IP&gt; \n</code></pre>"},{"location":"tutorials/mlperf-inference-power-measurement/#run-a-dummy-workload-with-power-inside-a-docker-container","title":"Run a dummy workload with power inside a docker container","text":"<pre><code>cm run script --tags=run,docker,container --cm_repo=ctuning@mlcommons-ck \\\n--docker_os=ubuntu --docker_os_version=22.04  \\\n--run_cmd=\"cm run script --tags==mlperf,power,client --power_server=&lt;POWER_SERVER_IP&gt;\"\n</code></pre>"},{"location":"tutorials/mlperf-inference-power-measurement/#running-mlperf-image-classification-with-power","title":"Running MLPerf Image Classification with power","text":"<pre><code>cm run script --tags=app,mlperf,inference,_reference,_power,_resnet50,_onnxruntime,_cpu --mode=performance --power_server=&lt;POWER_SERVER_IP&gt;\n</code></pre>"},{"location":"tutorials/mlperf-inference-power-measurement/#running-mlperf-image-classification-with-power-inside-a-docker-container","title":"Running MLPerf Image Classification with power inside a docker container","text":"<pre><code>cm run script --tags=run,docker,container --cm_repo=ctuning@mlcommons-ck  \\\n--docker_os=ubuntu --docker_os_version=22.04  \\\n--run_cmd=\"cm run script --tags=app,mlperf,inference,_reference,_power,_resnet50,_onnxruntime,_cpu --mode=performance --power_server=&lt;POWER_SERVER_IP&gt;\"\n</code></pre>"},{"location":"tutorials/mlperf-inference-power-measurement/#using-cm-gui-to-run-mlperf-inference-benchmarks-and-measure-power","title":"Using CM GUI to run MLPerf inference benchmarks and measure power","text":"<p>Link</p>"},{"location":"tutorials/mlperf-inference-power-measurement/#further-questions","title":"Further questions?","text":"<p>If you have further questions, are interested in our development roadmap, or need help to automate, optimize and validate your MLPerf submission, feel free to contact the MLCommons taskforce on automation and reproducibility.</p>"},{"location":"tutorials/mlperf-inference-submission/","title":"Mlperf inference submission","text":"<p>[ Back to index ]</p>"},{"location":"tutorials/mlperf-inference-submission/#tutorial-running-the-mlperf-inference-benchmark-and-preparing-the-submission","title":"Tutorial: running the MLPerf inference benchmark and preparing the submission","text":"Click here to see the table of contents.  * [Tutorial: running the MLPerf inference benchmark and preparing the submission](#tutorial-running-the-mlperf-inference-benchmark-and-preparing-the-submission) * [Introduction](#introduction) * [System preparation](#system-preparation)   * [Minimal system requirements](#minimal-system-requirements)   * [CM installation](#cm-installation)   * [Pull CM repository with cross-platform MLOps and DevOps scripts](#pull-cm-repository-with-cross-platform-mlops-and-devops-scripts)   * [Optional: update CM and repository to the latest version](#optional-update-cm-and-repository-to-the-latest-version)   * [Install system dependencies for your platform](#install-system-dependencies-for-your-platform)   * [Use CM to detect or install Python 3.8+](#use-cm-to-detect-or-install-python-38)   * [Install Python virtual environment with above Python](#install-python-virtual-environment-with-above-python)   * [Customize and run the MLPerf inference benchmark](#customize-and-run-the-mlperf-inference-benchmark)   * [Debug the MLPerf benchmark](#debug-the-mlperf-benchmark)   * [Customize MLPerf benchmark](#customize-mlperf-benchmark)     * [Implementations](#implementations)     * [Device](#device)       * [CPU](#cpu)       * [CUDA](#cuda)     * [Backend (ML framework)](#backend-ml-framework)       * [Deepsparse](#deepsparse)       * [ONNX runtime CPU](#onnx-runtime-cpu)       * [ONNX runtime CUDA](#onnx-runtime-cuda)       * [PyTorch CPU](#pytorch-cpu)       * [PyTorch CUDA](#pytorch-cuda)       * [TensorFlow (Python)](#tensorflow-python)       * [TensorFlow from source](#tensorflow-from-source)       * [TensorFlow Lite](#tensorflow-lite)       * [TensorRT](#tensorrt)       * [TVM ONNX (Python)](#tvm-onnx-python)     * [Datasets](#datasets)     * [Power measurements](#power-measurements)   * [Prepare submission](#prepare-submission) * [The next steps](#the-next-steps) * [Authors](#authors) * [Acknowledgments](#acknowledgments)"},{"location":"tutorials/mlperf-inference-submission/#introduction","title":"Introduction","text":"<p>This tutorial briefly explains how to run a modular version of the MLPerf inference benchmark using the cross-platform automation meta-framework (MLCommons CM aka CK2)  with a simple GUI and prepare your submission.</p> <p>Please follow this CM tutorial from the Student Cluster Competition for more details.</p> <p>If you have questions, encounter issues or have feature requests, please submit them here and feel free to join our open taskforce on automation and reproducibility and Discord discussions.*</p>"},{"location":"tutorials/mlperf-inference-submission/#system-preparation","title":"System preparation","text":""},{"location":"tutorials/mlperf-inference-submission/#minimal-system-requirements","title":"Minimal system requirements","text":"<ul> <li>Device: CPU (x86-64 or Arm64) or GPU (Nvidia)</li> <li>OS: we have tested CM automations on Ubuntu 20.04, Ubuntu 22.04, Debian 10, Red Hat 9 and MacOS 13</li> <li>Disk space: </li> <li>test runs: minimal preprocessed datasets &lt; ~5GB</li> <li>otherwise depends on a task and a dataset. Sometimes require 0.3 .. 3TB</li> <li>Python: 3.8+</li> <li>All other dependencies (artifacts and tools) will be installed by the CM meta-framework</li> </ul>"},{"location":"tutorials/mlperf-inference-submission/#cm-installation","title":"CM installation","text":"<p>Follow this guide to install the MLCommons CM framework (CK2) on your system.</p> <p>After the installation, you should be able to access the CM command line as follows:</p> <pre><code>$ cm\n\ncm {action} {automation} {artifact(s)} {--flags} @input.yaml @input.json\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#pull-cm-repository-with-cross-platform-mlops-and-devops-scripts","title":"Pull CM repository with cross-platform MLOps and DevOps scripts","text":"<p>Pull stable MLCommons CM repository with cross-platform CM scripts for modular ML Systems:</p> <pre><code>cm pull repo mlcommons@ck\n</code></pre> <p>CM pulls all such repositories into the <code>$HOME/CM</code> directory to search for CM automations and artifacts. You can find the location of a pulled repository as follows:</p> <pre><code>cm find repo mlcommons@ck\n</code></pre> <p>You can also pull a stable version of this CM repository using some checkout:</p> <pre><code>cm pull repo mlcommons@ck --checkout=...\n</code></pre> <p>You can now use the unified CM CLI/API of reusable and cross-platform CM scripts) to detect or install all artifacts (tools, models, datasets, libraries, etc)  required for a given software project (MLPerf inference benchmark in our case).</p> <p>Conceptually, these scripts take some environment variables and files as an input, perform a cross-platform action (detect artifact, download files, install tools), prepare new environment variables and cache output if needed.</p> <p>Note that CM can automatically detect or install all dependencies for a given benchmark and run it on a given platform in just one command using a simple JSON or YAML description of dependencies on all required CM scripts.</p> <p>However, since the goal of this tutorial is to explain you how we modularize MLPerf and any other benchmark,  we will show you all individual CM commands to prepare and run the MLPerf inference benchmark.  You can reuse these commands in your own projects thus providing a common interface for research projects.</p> <p>In the end, we will also show you how to run MLPerf benchmark in one command from scratch.</p>"},{"location":"tutorials/mlperf-inference-submission/#optional-update-cm-and-repository-to-the-latest-version","title":"Optional: update CM and repository to the latest version","text":"<p>Note that if you already have CM and mlcommons@ck repository installed on your system, you can update them to the latest version at any time and clean the CM cache as follows:</p> <pre><code>python3 -m pip install cmind -U\ncm pull repo mlcommons@ck --checkout=master\ncm rm cache -f\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#install-system-dependencies-for-your-platform","title":"Install system dependencies for your platform","text":"<p>We suggest you to install system dependencies required by the MLPerf inference benchmark using CM (requires SUDO access).</p> <p>For this purpose, we have created a cross-platform CM script that will automatically install  such dependencies based on your OS (Ubuntu, Debian, Red Hat, MacOS ...). </p> <p>In this case, CM script serves simply as a wrapper with a unified and cross-platform interface for native scripts that you can find and extend here if some dependencies are missing on your machine - this is a collaborative way to make  CM scripts portable and interoperable.</p> <p>You can run this CM scripts as follows (note that you may be asked for a SUDO password on your platform):</p> <pre><code>cm run script \"get sys-utils-cm\" --quiet\n</code></pre> <p>If you think that you have all system dependencies installed, you can run this script with a <code>--skip</code> flag: <pre><code>cm run script \"get sys-utils-cm\" --skip\n</code></pre></p>"},{"location":"tutorials/mlperf-inference-submission/#use-cm-to-detect-or-install-python-38","title":"Use CM to detect or install Python 3.8+","text":"<p>Since we use Python reference implementation of the MLPerf inference benchmark (unoptimized), we need to detect or install Python 3.8+ (MLPerf requirement). </p> <p>You need to detect it using the following CM script:</p> <pre><code>cm run script \"get python\" --version_min=3.8\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#install-python-virtual-environment-with-above-python","title":"Install Python virtual environment with above Python","text":"<pre><code>cm run script \"install python-venv\" --name=mlperf --version_min=3.8\n</code></pre> <p>You can change the name of your virtual Python environment using <code>--name</code> flag.</p>"},{"location":"tutorials/mlperf-inference-submission/#customize-and-run-the-mlperf-inference-benchmark","title":"Customize and run the MLPerf inference benchmark","text":"<p>You can use this online GUI to generate CM commands to customize and run the MLPerf inference benchmark. You can select different implementations, models, data sets, frameworks and parameters and then copy/paste the final commands to your shell to run MLPerf.</p> <p>Alternatively, you can use your own local GUI to run this benchmark as follows: <pre><code>cm run script --tags=gui \\\n     --script=\"app generic mlperf inference\" \\\n     --prefix=\"gnome-terminal --\"\n</code></pre></p> <p>You may just need to substitute <code>gnome-terminal --</code> with a command line that opens a new shell on your OS.</p> <p>CM will attempt to automatically detect or download and install the default versions of all required ML components.</p>"},{"location":"tutorials/mlperf-inference-submission/#debug-the-mlperf-benchmark","title":"Debug the MLPerf benchmark","text":"<p>You can add flag <code>--debug</code> to CM command to let CM stop just before running a given MLPerf benchmark, open a shell and let you run/customize benchmark manually from command line while reusing environment variables and tools prepared by CM.</p>"},{"location":"tutorials/mlperf-inference-submission/#customize-mlperf-benchmark","title":"Customize MLPerf benchmark","text":""},{"location":"tutorials/mlperf-inference-submission/#implementations","title":"Implementations","text":"<p>The community provided a unified CM API for the following implementations of the MLPerf inference benchmark: * Python reference implementation (CPU and CUDA)   * See the current coverage here      and please help us test different combinations of models, frameworks and platforms (i.e. collaborative design space exploration)! * Universal C++ implementation (CPU and CUDA)   * Check our community projects to extend this and other implementations. * TFLite C++ implementation (CPU) * Nvidia's implementation (CPU and CUDA)</p> <p>We are also working on a light-weight universal script  to benchmark performance of any ML model with MLPerf loadgen without accuracy.</p> <p>If you want to add your own implementation or backend, the simplest solution is to create a fork of the  MLPerf inference GitHub repo, specify this repo in the above GUI in the fields <code>Git URL for MLPerf inference sources to build LoadGen</code> and <code>Git URL for MLPerf inference sources to run benchmarks</code> and update the CM meta description of our MLPerf wrapper.</p> <p>Don't hesitate to get in touch with this taksforce to get free help from the community to add your implementation and prepare the submission.</p>"},{"location":"tutorials/mlperf-inference-submission/#device","title":"Device","text":""},{"location":"tutorials/mlperf-inference-submission/#cpu","title":"CPU","text":"<p>We have tested out-of-the-box CM automation for the MLPerf inference benchmark across diverse x86-64-based platforms (Intel and AMD) as well as Arm64-based machines from RPi4 to AWS Graviton.</p>"},{"location":"tutorials/mlperf-inference-submission/#cuda","title":"CUDA","text":"<p>As a minimum requirement, you should have CUDA installed. It can be detected using CM as follows: <pre><code>cm run script \"get cuda\"\n</code></pre></p> <p>We suggest you to install cuDNN and TensorRT too.</p> <p>If it's not installed, you can use CM scripts to install them as follows:</p> <pre><code>cm run script --tags=get,cudnn --tar_file=&lt;PATH_TO_CUDNN_TAR_FILE&gt;\n</code></pre> <pre><code>cm run script --tags=get,tensorrt --tar_file=&lt;PATH_TO_TENSORRT_TAR_FILE&gt;\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#backend-ml-framework","title":"Backend (ML framework)","text":"<p>You can install specific versions of various backends using CM as follows (optional):</p>"},{"location":"tutorials/mlperf-inference-submission/#deepsparse","title":"Deepsparse","text":"<p>See this PR prepared by the open taskforce  during the public hackathon to add Neural Magic's Deepsparse BERT backend for MLPerf to the CM automation.</p> <p>We currently support BERT large model int 8 targeting CPU only. CUDA may come soon...</p>"},{"location":"tutorials/mlperf-inference-submission/#onnx-runtime-cpu","title":"ONNX runtime CPU","text":"<pre><code>cm run script \"get generic-python-lib _onnxruntime\" (--version=...)\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#onnx-runtime-cuda","title":"ONNX runtime CUDA","text":"<pre><code>cm run script \"get generic-python-lib _onnxruntime_gpu\" (--version=...)\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#pytorch-cpu","title":"PyTorch CPU","text":"<pre><code>cm run script \"get generic-python-lib _torch\" (--version=...)\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#pytorch-cuda","title":"PyTorch CUDA","text":"<pre><code>cm run script \"get generic-python-lib _torch_cuda\" (--version=...)\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#tensorflow-python","title":"TensorFlow (Python)","text":"<pre><code>cm run script \"get generic-python-lib _tensorflow\" (--version=...)\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#tensorflow-from-source","title":"TensorFlow from source","text":"<pre><code>cm run script \"get tensorflow from-src\" (--version=...)\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#tensorflow-lite","title":"TensorFlow Lite","text":"<pre><code>cm run script \"get tensorflow from-src _tflite\" (--version=...)\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#tensorrt","title":"TensorRT","text":"<pre><code>cm run script --tags=get,tensorrt (--tar_file=&lt;PATH_TO_DOWNLOADED_TENSORRT_PACKAGE_FILE&gt;)\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#tvm-onnx-python","title":"TVM ONNX (Python)","text":"<pre><code>cm run script \"get generic-python-lib _apache-tvm\" (--version=...)\n</code></pre>"},{"location":"tutorials/mlperf-inference-submission/#datasets","title":"Datasets","text":"<ul> <li>ImageNet</li> <li>Open Images</li> <li>Squad</li> <li>Criteo</li> <li>Kits19</li> <li>Libris Speech</li> </ul>"},{"location":"tutorials/mlperf-inference-submission/#power-measurements","title":"Power measurements","text":"<p>Please follow this tutorial to run MLPerf with power measurements using CM.</p>"},{"location":"tutorials/mlperf-inference-submission/#prepare-submission","title":"Prepare submission","text":"<p>You can use this online GUI to generate CM commands to run the MLPerf inference benchmark, generate your submission and add your results to a temporal W&amp;B dashboard. </p> <p>Alternatively, you can use your own local GUI to run this benchmark as follows: <pre><code>cm run script --tags=gui \\\n     --script=\"run mlperf inference generate-run-cmds\" \\\n     --prefix=\"gnome-terminal --\"\n</code></pre></p>"},{"location":"tutorials/mlperf-inference-submission/#the-next-steps","title":"The next steps","text":"<p>You are welcome to join the open MLCommons taskforce on automation and reproducibility to contribute to this project and continue optimizing this benchmark and prepare an official submission  for MLPerf inference benchmarks with the free help of the community.</p> <p>See the development roadmap here.</p>"},{"location":"tutorials/mlperf-inference-submission/#authors","title":"Authors","text":"<ul> <li>Grigori Fursin (cTuning foundation and cKnowledge.org)</li> <li>Arjun Suresh (cTuning foundation and cKnowledge.org)</li> </ul>"},{"location":"tutorials/mlperf-inference-submission/#acknowledgments","title":"Acknowledgments","text":"<p>We thank  Hai Ah Nam, Steve Leak, Vijay Janappa Reddi, Tom Jablin, Ramesh N Chukka, Peter Mattson, David Kanter, Pablo Gonzalez Mesa, Thomas Zhu, Thomas Schmid and Gaurav Verma for their suggestions and contributions.</p>"},{"location":"tutorials/mlperf-language-processing/","title":"Mlperf language processing","text":"<p>[ Back to index ]</p> <p>Draft Stage</p>"},{"location":"tutorials/mlperf-language-processing/#run-commands","title":"Run Commands","text":""},{"location":"tutorials/mlperf-language-processing/#quick-submission-run-short-run","title":"Quick submission run (short run)","text":"<pre><code>cm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short --submitter=name \\\n--lang=reference --model=bert-99 --backend=deepsparse --device=cpu --scenario=Offline  --quantized \n--results_dir=/home/cmuser/tmp2  --submission_dir=/home/cmuser/submission --clean\n</code></pre>"},{"location":"tutorials/mlperf-language-processing/#customizations","title":"Customizations","text":"<ol> <li><code>_short</code> -&gt; <code>_valid</code>: For full submission run</li> <li><code>--scenario</code> -&gt; one of <code>SingleStream</code>, <code>Server</code> for changing the scenarios</li> <li><code>--backend</code> -&gt; one of <code>onnxruntime</code>, <code>tf</code>, <code>pytorch</code>, <code>deepsparse</code></li> <li><code>--quantized</code>: only works for <code>onnxruntime</code> and <code>deepsparse</code></li> <li><code>--device</code> -&gt; one of <code>cpu</code>, <code>cuda</code></li> <li><code>--submitter</code>: Name of the submitter excluding spaces</li> <li><code>--clean</code>: Clean everything and do not reuse previous run results</li> </ol>"},{"location":"tutorials/modular-image-classification/","title":"Modular image classification","text":"<p>[ Back to index ]</p> <p>An interactive version of this tutorial is also available   at this Google Colab page.</p>"},{"location":"tutorials/modular-image-classification/#trying-cm-modular-image-classification","title":"Trying CM: modular image classification","text":"<p>This example demonstrates our unified, technology-agnostic and human-readable CM automation language  to prepare and run image classification on any platform while automatically detecting or installing  all related artifacts and adapting them to your system and environment.</p> <p>This language is being developed by the open taskforce  to solve the dependency hell and make it easier for the community to run, customize and reuse  research (software) projects in a unified way.</p>"},{"location":"tutorials/modular-image-classification/#install-cm-automation-language","title":"Install CM automation language","text":"<p>Please follow this guide to install the MLCommons CM language on your platform.</p>"},{"location":"tutorials/modular-image-classification/#prepare-and-run-modular-image-classification-via-cm","title":"Prepare and run modular image classification via CM","text":"<p>Here is an example of a modular image classification assembled from  (portable and reusable CM scripts) using a human-readable YAML file. CM scripts simply wrap native scripts, tools, and artifacts while making them findable, accessible, portabl, interoperable, and reusable based on FAIR principles.</p> <p>CM will read this YAML (or JSON) description, go through all dependencies to run other CM scripts,  and attempt to automatically detect, download, install and build all related artifacts  and tools to adapt this example to any software and hardware.</p> <p>We have tested this tutorial on various Linux distributions, MacOS and Windows.</p> <p>Let's go through these steps manually to better understand how CM scripts work.</p> <p>First you need to install an MLCommons CM-MLOps repository  with portable and reusable scripts developed by the MLCommons taskforce on automation and reproducibility to unify benchmarking and optimization of ML/AI systems:</p> <pre><code>cm pull repo mlcommons@ck\n</code></pre> <p>You can then run a CM script implementing modular image classification example as follows:</p> <pre><code>cm run script --tags=app,image-classification,onnx,python --quiet\n</code></pre> <p>or</p> <pre><code>cm run script \"python app image-classification onnx\" --quiet\n</code></pre> <p>or for CM v1.4.1+</p> <pre><code>cmr \"python app image-classification onnx\" --quiet\n</code></pre> <p>Note that you can also access this CM script using just one unified function <code>cmind.access</code> from CM Python API similar to micro services:</p> <pre><code>import cmind\nr=cmind.access({'action':'run', \n                'automation':'script'\n                'tags':'app,image-classification,onnx,python',\n                'out':'con',\n                'quiet':True})\nprint (r)\n</code></pre> <p>It may take a few minutes to run this CM script for the first time and adapt it to your platform depending on your hardware and the Internet speed.</p> <p>Note that all the subsequent runs will be much faster because CM automatically caches the output of all portable CM scripts  to be quickly reused in this and other CM scripts.</p>"},{"location":"tutorials/modular-image-classification/#detect-or-install-individual-tools-and-artifacts-via-cm-interface","title":"Detect or install individual tools and artifacts via CM interface","text":"<p>You can also force to install specific versions of ML artifacts and tools (models, data sets, engines, libraries, run-times, etc)  using individual CM scripts to automatically plug them into the above ML application:</p> <pre><code>cmr \"detect os\" --out=json\ncmr \"get sys-utils-cm\" --quiet\ncmr \"get python\" --version_min=3.9.1\ncmr \"install python-venv\" --name=my-virtual-env\ncmr \"get ml-model resnet50 image-classification _onnx _fp32\" --const.CM_PACKAGE_URL=https://huggingface.co/ctuning/mlperf-inference-resnet50-onnx-fp32-imagenet2012-v1.0/resolve/main/resnet50_v1.onnx\ncmr \"get original imagenet dataset _2012-500\"\ncmr \"get generic-python-lib _onnxruntime\" --version=1.12.0\n\ncm show cache\ncm show cache --tags=python\ncm show cache --tags=ml-model\n\ncmr \"python app image-classification onnx\"\ncmr \"python app image-classification onnx\" --quiet --input=`cm find script app-image-classification-onnx-py,3d5e908e472b417e`/img/computer_mouse.jpg\n</code></pre> <p>CM scripts converts CLI flags into environment variables and generates some input files  in the <code>preprocess function</code> of <code>customize.py</code> module. They then run a Python function or some native script with these environment variables and input files,  and outputs new environment variables and files to the unified CM output dictionary. The output and files can be cached and reused by other CM scripts. </p> <p>Feel free to explore this CM script  with <code>_cm.yaml</code>, <code>run.sh</code>, <code>run.bat</code>, <code>src/onnx_classify.py</code> and other files required to run modular inference.</p>"},{"location":"tutorials/modular-image-classification/#run-this-script-with-cuda","title":"Run this script with CUDA","text":"<p>Here is another example to run the above image classification application with CUDA using the same CM interface:</p> <p>First detect or install CUDA:</p> <pre><code>cm run script \"get cuda\"\ncm run script \"get cuda-devices\"\n</code></pre> <p>Then run the same CM script with so-called variation <code>_cuda</code>: <pre><code>cm run script \"python app image-classification onnx _cuda\"\n</code></pre></p> <p>Note that variations are different from script tags because they simply update environment variables and dependencies  in a given CM script found using tags.</p> <p>If you have some image, you can classify it using this CM script as follows:</p> <pre><code>cm run script \"python app image-classification onnx _cuda\" --input=my-image.jpg\n</code></pre> <p>The variation _cuda  will set a specific environment variables such as <code>USE_CUDA=\"yes\"</code>,  that will simply turn on and off  some dependencies on other CM scripts. </p> <p>For example, this environment variable will be used to automatically detect or install ONNX run-time with CUDA support instead of the CPU version.</p>"},{"location":"tutorials/reproduce-mlperf-tiny/","title":"Reproduce mlperf tiny","text":"<p>[ Back to index ]</p> Click here to see the table of contents.  * [Tutorial: reproducibility study for TinyMLPerf submission with MicroTVM and NUCLEO-L4R5ZI board from STMicroelecronics](#tutorial-reproducibility-study-for-tinymlperf-submission-with-microtvm-and-nucleo-l4r5zi-board-from-stmicroelecronics)   * [Install software and setup hardware](#install-software-and-setup-hardware)   * [Build all benchmarks from OctoML's v1.0 submission](#build-all-benchmarks-from-octoml's-v10-submission)   * [Flash](#flash)   * [Generate submission](#generate-submission)   * [Run submission checker and prepare report](#run-submission-checker-and-prepare-report)   * [Import results to the CK platform](#import-results-to-the-ck-platform)   * [Visualize and compare TinyMLPerf](#visualize-and-compare-tinymlperf)   * [The next steps](#the-next-steps)   * [Contact MLCommons task force on automation and reproducibility](#contact-mlcommons-task-force-on-automation-and-reproducibility)"},{"location":"tutorials/reproduce-mlperf-tiny/#tutorial-reproducibility-study-for-tinymlperf-submission-with-microtvm-and-nucleo-l4r5zi-board-from-stmicroelecronics","title":"Tutorial: reproducibility study for TinyMLPerf submission with MicroTVM and NUCLEO-L4R5ZI board from STMicroelecronics","text":"<p>The MLCommons task force on automation and reproducibility, cTuning foundation and cKnowledge Ltd organize public challenges to let the community run, visualize and optimize MLPerf benchmarks  out of the box across diverse software, hardware, models and data.</p> <p>This tutorial demonstrates how to run and/or reproduce Tiny MLPerf benchmark (OctoML v1.0 submission) with the help of the MLCommons CM automation language.</p> <p>You will build, flash and run image classification and keyword spotting applications using microTVM compiler on the NUCLEO-L4R5ZI board from STMicroelectronics.</p> <p>You will need ~12GB of disk space and it will take ~20..30 minutes to download all dependencies  and build TinyMLPerf benchmarks depending on your Internet and host platform speed.</p> <p>Benchmark compilation and device flashing can be done on any Linux-based platform while running benchmark using EEMBC GUI can be done on Linux and Windows.</p> <p>If you have any questions about this tutorial, please get in touch via our public Discord server or open a GitHub issue here.</p>"},{"location":"tutorials/reproduce-mlperf-tiny/#install-software-and-setup-hardware","title":"Install software and setup hardware","text":"<p>Please follow this tutorial to install the MLCommons CM automation language, EEMBC Energy Runner and other software dependencies for your host platform, and setup NUCLEO-L4R5ZI board from STMicroelecronics.</p> <p>We reproduced/replicated OctoML's v1.0 submission using a host machine with Ubuntu 20.04 and Python 3.8.10.</p>"},{"location":"tutorials/reproduce-mlperf-tiny/#build-all-benchmarks-from-octomls-v10-submission","title":"Build all benchmarks from OctoML's v1.0 submission","text":"<p>You can use CM script to automatically build all benchmarks in all variants to reproduce OctoML's v1.0 submission:</p> <pre><code>cm run script --tags=generate,tiny,mlperf,octoml,submission\n</code></pre> <p>The main CM scripts which automatically gets called from the above command are given below.</p> <ol> <li>Build Tiny Models</li> <li>Flash Tiny Models</li> <li>Get Zephyr</li> <li>Get Zephyr SDK</li> <li>Get MictoTVM</li> <li>GET CMSIS_5</li> </ol> <p>The above command should produce five elf binaries which can be located inside the respective cache entries given by the below command <pre><code>cm show cache --tags=reproduce,tiny,octoml,mlperf\n</code></pre></p>"},{"location":"tutorials/reproduce-mlperf-tiny/#flash","title":"Flash","text":"<p>To flash each benchmark, follow the command bellow. Make sure to replace <code>VARIANT</code> by either <code>cmsis_nn</code> or <code>native</code>.  You need to specify the model by replacing <code>MODEL</code> with a value from (<code>ad</code>, <code>kws</code>, <code>ic</code>, <code>vww</code>).  Finally, you need to choose <code>_NUCLEO</code> or <code>_NRF</code> to specify the target board to flash.</p> <pre><code>cm run script --tags=flash,tiny,_VARIANT,_MODEL,_BOARD\n</code></pre> <p>We have tested the following combinations:</p> <pre><code>cm run script --tags=flash,tiny,_cmsis_nn,_ic,_NUCLEO\ncm run script --tags=flash,tiny,_native,_ic,_NUCLEO\ncm run script --tags=flash,tiny,_cmsis_nn,_kws,_NUCLEO\ncm run script --tags=flash,tiny,_native,_kws,_NUCLEO\n</code></pre> <p>After each flashing, follow the EEMBC Runner guide to run benchmark in performance and accuracy modes.</p> <p>You can find the logs after each run in the following directory on your host machine: <code>$HOME/eembc/runner/sessions</code>.</p>"},{"location":"tutorials/reproduce-mlperf-tiny/#generate-submission","title":"Generate submission","text":"<p>Under development.</p>"},{"location":"tutorials/reproduce-mlperf-tiny/#run-submission-checker-and-prepare-report","title":"Run submission checker and prepare report","text":"<p>Follow this guide.</p>"},{"location":"tutorials/reproduce-mlperf-tiny/#import-results-to-the-ck-platform","title":"Import results to the CK platform","text":"<p>Follow this guide</p>"},{"location":"tutorials/reproduce-mlperf-tiny/#visualize-and-compare-tinymlperf","title":"Visualize and compare TinyMLPerf","text":"<p>You can visualize and compare TinyMLPerf results here. You can use this collaborative platform inside your organization to reproduce and optimize benchmarks and applications of your interest.</p>"},{"location":"tutorials/reproduce-mlperf-tiny/#the-next-steps","title":"The next steps","text":"<p>Please follow the rest of this tutorial  to see how to visualize and compare your results, and learn more about our future automation plans.</p>"},{"location":"tutorials/reproduce-mlperf-tiny/#contact-mlcommons-task-force-on-automation-and-reproducibility","title":"Contact MLCommons task force on automation and reproducibility","text":"<p>Please join the MLCommons task force on automation and reproducibility to get free help to automate and optimize MLPerf benchmarks for your software and hardware stack using the MLCommons CM automation language!</p>"},{"location":"tutorials/reproduce-mlperf-training/","title":"Reproduce mlperf training","text":"<p>[ Back to index ]</p> Click here to see the table of contents.  * [Tutorial: automate, visualize and reproduce MLPerf training submissions](#tutorial-automate-visualize-and-reproduce-mlperf-training-submissions)   * [Install CM](#install-cm)   * [Import results to the CK platform](#import-results-to-the-ck-platform)   * [Visualize and compare MLPerf training results](#visualize-and-compare-mlperf-training-results)   * [Contact MLCommons task force on automation and reproducibility](#contact-mlcommons-task-force-on-automation-and-reproducibility)"},{"location":"tutorials/reproduce-mlperf-training/#tutorial-automate-visualize-and-reproduce-mlperf-training-submissions","title":"Tutorial: automate, visualize and reproduce MLPerf training submissions","text":"<p>The MLCommons task force on automation and reproducibility is developing an open-source Collective Knowledge platform to make it easier for the community to run, visualize and optimize MLPerf benchmarks  out of the box across diverse software, hardware, models and data.</p> <p>This tutorial demonstrates how to run and/or reproduce MLPerf training benchmark with the help of the MLCommons CM automation language.</p> <p>If you have any questions about this tutorial, please get in touch via our public Discord server or open a GitHub issue here.</p>"},{"location":"tutorials/reproduce-mlperf-training/#install-cm","title":"Install CM","text":"<p>Follow this guide  to install the MLCommons CM automation language on your platfom. </p> <p>We have tested this tutorial with Ubuntu 20.04 and Windows 10.</p> <p>To be continued ...</p>"},{"location":"tutorials/reproduce-mlperf-training/#import-results-to-the-ck-platform","title":"Import results to the CK platform","text":"<p>Follow this guide</p>"},{"location":"tutorials/reproduce-mlperf-training/#visualize-and-compare-mlperf-training-results","title":"Visualize and compare MLPerf training results","text":"<p>You can visualize and compare MLPerf results here. You can use this collaborative platform inside your organization to reproduce and optimize benchmarks and applications of your interest.</p>"},{"location":"tutorials/reproduce-mlperf-training/#contact-mlcommons-task-force-on-automation-and-reproducibility","title":"Contact MLCommons task force on automation and reproducibility","text":"<p>Please join the MLCommons task force on automation and reproducibility to get free help to automate and optimize MLPerf benchmarks for your software and hardware stack using the MLCommons CM automation language!</p>"},{"location":"tutorials/reproduce-research-paper-ipol/","title":"Reproduce research paper ipol","text":"<p>[ Back to index ]</p>"},{"location":"tutorials/reproduce-research-paper-ipol/#tutorial-reproduce-research-paper-ipol22-example","title":"Tutorial: Reproduce research paper (IPOL'22 example)","text":"<p>This tutorial was prepared during a public Collective Knowledge challenge to reproduce results from the IPOL'22 439 journal article using the MLCommons CM automation language.</p> <p>Please check the CM introduction to understand CM motivation and concepts.</p>"},{"location":"tutorials/reproduce-research-paper-ipol/#organizers","title":"Organizers","text":"<ul> <li>Jose Hernandez</li> <li>Miguel Colom</li> <li>Grigori Fursin</li> <li>cTuning foundation</li> <li>cKnowledge Ltd</li> </ul>"},{"location":"tutorials/reproduce-research-paper-ipol/#initial-discussion-and-materials","title":"Initial discussion and materials","text":"<ul> <li>https://github.com/mlcommons/ck/issues/617</li> <li>https://access.cknowledge.org/playground/?action=challenges&amp;name=reproduce-and-automate-ipol-paper</li> </ul>"},{"location":"tutorials/reproduce-research-paper-ipol/#implementation","title":"Implementation","text":"<p>We have implemented two portable CM scripts to automate reproducibility on any platform:</p> <ul> <li>Download IPOL paper sources and cache them in CM</li> <li>Run IPOL 2022 439 paper demo using above script and PyTorch</li> </ul>"},{"location":"tutorials/reproduce-research-paper-ipol/#reproducibility-study","title":"Reproducibility study","text":"<ol> <li> <p>Install MLCommons CM(CK2) automation framework as described here.</p> </li> <li> <p>Install the latest MLCommons repository with reusable CM scripts:</p> </li> </ol> <pre><code>cm pull repo mlcommons@ck\n</code></pre> <ol> <li>Install author sources from IPOL 2022 439 paper using CM:</li> </ol> <pre><code>cm run script \"get ipol src\" --year=2022 --number=439\n\ncm show cache --tags=ipol,src\n</code></pre> <ol> <li>Run script to install dependencies and reproduce results <pre><code>cm run script \"reproduce ipol 2022-439\"\n</code></pre></li> </ol> <p>This script will use these sample images and should produce a diff.png file in the current directory.</p>"},{"location":"tutorials/reproduce-research-paper-ipol/#using-other-data","title":"Using other data","text":"<p>You can use any other 2 images by specifying their full path as follows: <pre><code>cm run script \"reproduce ipol 2022-439\" \\\n       --image1={full path to png image 1} \\\n       --image2={full path to png image 2}\n</code></pre></p>"},{"location":"tutorials/sc22-scc-mlperf-part2/","title":"Sc22 scc mlperf part2","text":"<p>[ Back to index ]</p>"},{"location":"tutorials/sc22-scc-mlperf-part2/#tutorial-modularizing-and-automating-mlperf-part-2","title":"Tutorial: modularizing and automating MLPerf (part 2)","text":"Click here to see the table of contents.  * [Introduction](#introduction) * [Update CM framework and automation repository](#update-cm-framework-and-automation-repository) * [CM automation for the MLPerf benchmark](#cm-automation-for-the-mlperf-benchmark)   * [MLPerf inference - C++ - RetinaNet FP32 - Open Images - ONNX - CPU - Offline](#mlperf-inference---c---retinanet-fp32---open-images---onnx---cpu---offline)     * [Summary](#summary)   * [MLPerf inference - Python - RetinaNet FP32 - Open Images - ONNX - GPU - Offline](#mlperf-inference---python---retinanet-fp32---open-images---onnx---gpu---offline)     * [Prepare CUDA](#prepare-cuda)     * [Prepare Python with virtual environment](#prepare-python-with-virtual-environment)     * [Run MLPerf inference benchmark (offline, accuracy)](#run-mlperf-inference-benchmark-offline-accuracy)     * [Run MLPerf inference benchmark (offline, performance)](#run-mlperf-inference-benchmark-offline-performance)     * [Summary](#summary)   * [MLPerf inference - C++ - RetinaNet FP32 - Open Images - ONNX - GPU - Offline](#mlperf-inference---c---retinanet-fp32---open-images---onnx---gpu---offline)     * [Summary](#summary)   * [MLPerf inference - Python - RetinaNet FP32 - Open Images - PyTorch - CPU - Offline](#mlperf-inference---python---retinanet-fp32---open-images---pytorch---cpu---offline)     * [Summary](#summary) * [The next steps](#the-next-steps) * [Authors](#authors) * [Acknowledgments](#acknowledgments)"},{"location":"tutorials/sc22-scc-mlperf-part2/#introduction","title":"Introduction","text":"<p>We expect that you have completed the 1st part of this tutorial  and managed to run the MLPerf inference benchmark for object detection with RetinaNet FP32, Open Images and ONNX runtime on a CPU target.</p> <p>This tutorial shows you how to customize the MLPerf inference benchmark and run it with a C++ implementation, CUDA and PyTorch.</p>"},{"location":"tutorials/sc22-scc-mlperf-part2/#update-cm-framework-and-automation-repository","title":"Update CM framework and automation repository","text":"<p>Note that the CM automation meta-framework  and the repository with automation scripts  are being continuously updated by the community to improve the portability and interoperability of  all reusable components for MLOps and DevOps.</p> <p>You can get the latest version of the CM framework and automation repository as follows (though be careful since CM CLI and APIs may change):</p> <pre><code>python3 -m pip install cmind -U\ncm pull repo mlcommons@ck --checkout=master\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf-part2/#cm-automation-for-the-mlperf-benchmark","title":"CM automation for the MLPerf benchmark","text":""},{"location":"tutorials/sc22-scc-mlperf-part2/#mlperf-inference-c-retinanet-fp32-open-images-onnx-cpu-offline","title":"MLPerf inference - C++ - RetinaNet FP32 - Open Images - ONNX - CPU - Offline","text":"<p>Let's now run a universal and modular C++ implementation of the MLPerf inference benchmark  (developed by Thomas Zhu during his internship at OctoML).</p> <p>Note that CM will reuse already installed and preprocessed Open Images dataset, model and tools from the CM cache installed during the 1st part of this tutorial while installing the ONNX runtime library with C++ bindings for your system.</p> <p>If you want to reinstall all dependencies, you can clean the CM cache again and restart the above command: <pre><code>cm rm cache -f\n</code></pre></p> <p>You can run C++ implementation by simply changing <code>_python</code> variation to <code>_cpp</code> variation in our high-level CM MLPerf script that will then set up the correct dependencies and will run the C++ implementation of this script</p> <pre><code>cm run script \"app mlperf inference generic _cpp _retinanet _onnxruntime _cpu\" \\\n     --adr.python.version_min=3.8 \\\n     --adr.compiler.tags=gcc \\\n     --adr.openimages-preprocessed.tags=_500 \\\n     --scenario=Offline \\\n     --mode=accuracy \\\n     --test_query_count=10 \\\n     --rerun\n</code></pre> <p>CM will download the ONNX binaries for your system, compile our C++ implementation with the ONNX backend and will run the MLPerf inference benchmark. You should normally see the following output: <pre><code>...\n\nloading annotations into memory...\nDone (t=0.02s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.01s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nDONE (t=0.10s).\nAccumulating evaluation results...\nDONE (t=0.12s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.548\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.787\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.714\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.304\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.631\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.433\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.648\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.663\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.731\nmAP=54.814%\n\n    - running time of script \"run,mlperf,mlcommons,accuracy,mlc,process-accuracy\": 1.18 sec.\n  - running time of script \"app,vision,language,mlcommons,mlperf,inference,reference,generic,ref\": 53.81 sec.\n</code></pre></p> <p>You can then obtain performance using the C++ implemnentation of the MLPerf inference benchmark as follows: <pre><code>cm run script \"app mlperf inference generic _cpp _retinanet _onnxruntime _cpu\" \\\n     --adr.python.version_min=3.8 \\\n     --adr.compiler.tags=gcc \\\n     --adr.openimages-preprocessed.tags=_500 \\\n     --scenario=Offline \\\n     --mode=performance \\\n     --test_query_count=10 \\\n     --rerun\n</code></pre></p> <p>You should get the following output (QPS will depend on the speed of your machine): <pre><code>================================================\nMLPerf Results Summary\n================================================\nSUT name : QueueSUT\nScenario : Offline\nMode     : PerformanceOnly\nSamples per second: 0.631832\nResult is : VALID\n  Min duration satisfied : Yes\n  Min queries satisfied : Yes\n  Early stopping satisfied: Yes\n\n================================================\nAdditional Stats\n================================================\nMin latency (ns)                : 14547257820\nMax latency (ns)                : 15826999233\nMean latency (ns)               : 15129106642\n50.00 percentile latency (ns)   : 15045448544\n90.00 percentile latency (ns)   : 15826999233\n95.00 percentile latency (ns)   : 15826999233\n97.00 percentile latency (ns)   : 15826999233\n99.00 percentile latency (ns)   : 15826999233\n99.90 percentile latency (ns)   : 15826999233\n\n================================================\nTest Parameters Used\n================================================\nsamples_per_query : 10\ntarget_qps : 1\ntarget_latency (ns): 0\nmax_async_queries : 1\nmin_duration (ms): 0\nmax_duration (ms): 0\nmin_query_count : 1\nmax_query_count : 10\nqsl_rng_seed : 14284205019438841327\nsample_index_rng_seed : 4163916728725999944\nschedule_rng_seed : 299063814864929621\naccuracy_log_rng_seed : 0\naccuracy_log_probability : 0\naccuracy_log_sampling_target : 0\nprint_timestamps : 0\nperformance_issue_unique : 0\nperformance_issue_same : 0\nperformance_issue_same_index : 0\nperformance_sample_count : 64\n\nNo warnings encountered during test.\n\nNo errors encountered during test.\n\n  - running time of script \"app,vision,language,mlcommons,mlperf,inference,reference,generic,ref\": 50.24 sec.\nrsa-key-fgg-universal@mlperf-tests-e2-x86-16-64-ubuntu\n</code></pre></p> <p>We plan to continue optimizing this implementation of the MLPerf inference benchmark  together with the community across different ML engines, models, data sets and systems.</p>"},{"location":"tutorials/sc22-scc-mlperf-part2/#summary","title":"Summary","text":"<p>You can now test the end-to-end benchmarking and submission with the C++ implementation and ONNX on CPU using Python virtual environment as follows (just substitute \"Community\" with your name or organization or anything else): </p> <pre><code>cm pull repo mlcommons@ck\n\ncm run script \"get sys-utils-cm\" --quiet\n\ncm run script \"install python-venv\" --version=3.10.8 --name=mlperf\n\ncm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short,_dashboard \\\n      --adr.python.name=mlperf \\\n      --adr.python.version_min=3.8 \\\n      --adr.compiler.tags=gcc \\\n      --adr.openimages-preprocessed.tags=_500 \\\n      --submitter=\"Community\" \\\n      --implementation=cpp \\\n      --hw_name=default \\\n      --model=retinanet \\\n      --backend=onnxruntime \\\n      --device=cpu \\\n      --scenario=Offline \\\n      --test_query_count=10 \\\n      --clean\n</code></pre> <p>In case of a successfull run, you should see your crowd-testing results at this live W&amp;B dashboard.</p>"},{"location":"tutorials/sc22-scc-mlperf-part2/#mlperf-inference-python-retinanet-fp32-open-images-onnx-gpu-offline","title":"MLPerf inference - Python - RetinaNet FP32 - Open Images - ONNX - GPU - Offline","text":""},{"location":"tutorials/sc22-scc-mlperf-part2/#prepare-cuda","title":"Prepare CUDA","text":"<p>If your system has an Nvidia GPU, you can run the MLPerf inference benchmark on this GPU using the CM automation.</p> <p>First you need to detect CUDA and cuDNN installation using CM as follows:</p> <pre><code>cm run script \"get cuda\" --out=json\n</code></pre> <p>You should see the output similar to the following one (for CUDA 11.3): <pre><code>{\n  \"deps\": [],\n  \"env\": {\n    \"+CPLUS_INCLUDE_PATH\": [\n      \"/usr/local/cuda-11.3/include\"\n    ],\n    \"+C_INCLUDE_PATH\": [\n      \"/usr/local/cuda-11.3/include\"\n    ],\n    \"+DYLD_FALLBACK_LIBRARY_PATH\": [],\n    \"+LD_LIBRARY_PATH\": [],\n    \"+PATH\": [\n      \"/usr/local/cuda-11.3/bin\"\n    ],\n    \"CM_CUDA_CACHE_TAGS\": \"version-11.3\",\n    \"CM_CUDA_INSTALLED_PATH\": \"/usr/local/cuda-11.3\",\n    \"CM_CUDA_PATH_BIN\": \"/usr/local/cuda-11.3/bin\",\n    \"CM_CUDA_PATH_INCLUDE\": \"/usr/local/cuda-11.3/include\",\n    \"CM_CUDA_PATH_LIB\": \"/usr/local/cuda-11.3/lib64\",\n    \"CM_CUDA_PATH_LIB_CUDNN\": \"/usr/local/cuda-11.3/lib64/libcudnn.so\",\n    \"CM_CUDA_PATH_LIB_CUDNN_EXISTS\": \"yes\",\n    \"CM_CUDA_VERSION\": \"11.3\",\n    \"CM_NVCC_BIN\": \"nvcc\",\n    \"CM_NVCC_BIN_WITH_PATH\": \"/usr/local/cuda-11.3/bin/nvcc\"\n  },\n  \"new_env\": {\n    \"+CPLUS_INCLUDE_PATH\": [\n      \"/usr/local/cuda-11.3/include\"\n    ],\n    \"+C_INCLUDE_PATH\": [\n      \"/usr/local/cuda-11.3/include\"\n    ],\n    \"+DYLD_FALLBACK_LIBRARY_PATH\": [],\n    \"+LD_LIBRARY_PATH\": [],\n    \"+PATH\": [\n      \"/usr/local/cuda-11.3/bin\"\n    ],\n    \"CM_CUDA_CACHE_TAGS\": \"version-11.3\",\n    \"CM_CUDA_INSTALLED_PATH\": \"/usr/local/cuda-11.3\",\n    \"CM_CUDA_PATH_BIN\": \"/usr/local/cuda-11.3/bin\",\n    \"CM_CUDA_PATH_INCLUDE\": \"/usr/local/cuda-11.3/include\",\n    \"CM_CUDA_PATH_LIB\": \"/usr/local/cuda-11.3/lib64\",\n    \"CM_CUDA_PATH_LIB_CUDNN\": \"/usr/local/cuda-11.3/lib64/libcudnn.so\",\n    \"CM_CUDA_PATH_LIB_CUDNN_EXISTS\": \"yes\",\n    \"CM_CUDA_VERSION\": \"11.3\",\n    \"CM_NVCC_BIN\": \"nvcc\",\n    \"CM_NVCC_BIN_WITH_PATH\": \"/usr/local/cuda-11.3/bin/nvcc\"\n  },\n  \"new_state\": {},\n  \"return\": 0,\n  \"state\": {}\n}\n</code></pre></p> <p>You can obtain the information about your GPU using CM as follows: <pre><code>cm run script \"get cuda-devices\"\n</code></pre></p>"},{"location":"tutorials/sc22-scc-mlperf-part2/#prepare-python-with-virtual-environment","title":"Prepare Python with virtual environment","text":"<p>We suggest you to install Python virtual environment to avoid mixing up your local Python: <pre><code>cm run script \"get sys-utils-cm\" --quiet\n\ncm run script \"install python-venv\" --version=3.10.8 --name=mlperf-cuda\n</code></pre></p>"},{"location":"tutorials/sc22-scc-mlperf-part2/#run-mlperf-inference-benchmark-offline-accuracy","title":"Run MLPerf inference benchmark (offline, accuracy)","text":"<p>You are now ready to run the MLPerf object detection benchmark on GPU with Python virtual environment as folllows:</p> <pre><code>cm run script \"app mlperf inference generic _python _retinanet _onnxruntime _cuda\" \\\n     --adr.python.name=mlperf-cuda \\\n     --scenario=Offline \\\n     --mode=accuracy \\\n     --test_query_count=10 \\\n     --clean\n</code></pre> <p>This CM script will automatically find or install all dependencies described in its CM meta description, aggregate all environment variables, preprocess all files and assemble the MLPerf benchmark CMD.</p> <p>It will take a few minutes to run it and you should see the following accuracy:</p> <pre><code>loading annotations into memory...\nDone (t=0.02s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.02s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nDONE (t=0.09s).\nAccumulating evaluation results...\nDONE (t=0.11s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.548\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.787\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.714\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.304\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.631\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.433\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.648\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.663\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.731\n\nmAP=54.814%\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf-part2/#run-mlperf-inference-benchmark-offline-performance","title":"Run MLPerf inference benchmark (offline, performance)","text":"<p>Let's run the MLPerf object detection on GPU while measuring performance:</p> <pre><code>cm run script \"app mlperf inference generic _python _retinanet _onnxruntime _cuda\" \\\n     --adr.python.name=mlperf-cuda \\\n     --scenario=Offline \\\n     --mode=performance \\\n     --clean\n</code></pre> <p>It will run for 2-5 minutes and you should see the output similar to the following one in the end (the QPS is the performance result of this benchmark that depends on the speed of your system):</p> <pre><code>TestScenario.Offline qps=8.44, mean=4.7238, time=78.230, queries=660, tiles=50.0:4.8531,80.0:5.0225,90.0:5.1124,95.0:5.1658,99.0:5.2730,99.9:5.3445\n\n\n================================================\nMLPerf Results Summary\n================================================\n...\n\nNo warnings encountered during test.\n\nNo errors encountered during test.\n\n  - running time of script \"app,vision,language,mlcommons,mlperf,inference,reference,generic,ref\": 86.90 sec.\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf-part2/#summary_1","title":"Summary","text":"<p>You can now run MLPerf in the submission mode (accuracy and performance) on GPU using the following CM command with Python virtual env (just substitute \"Community\" with your organization or any other identifier):</p> <pre><code>cm pull repo mlcommons@ck\n\ncm run script \"get sys-utils-cm\" --quiet\n\ncm run script \"install python-venv\" --version=3.10.8 --name=mlperf-cuda\n\ncm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short,_dashboard \\\n      --adr.python.name=mlperf-cuda \\\n      --adr.python.version_min=3.8 \\\n      --adr.compiler.tags=gcc \\\n      --adr.openimages-preprocessed.tags=_500 \\\n      --submitter=\"Community\" \\\n      --implementation=python \\\n      --hw_name=default \\\n      --model=retinanet \\\n      --backend=onnxruntime \\\n      --device=gpu \\\n      --scenario=Offline \\\n      --test_query_count=10 \\\n      --clean\n</code></pre> <p>In case of a successfull run, you should see your crowd-testing results at this live W&amp;B dashboard.</p>"},{"location":"tutorials/sc22-scc-mlperf-part2/#mlperf-inference-c-retinanet-fp32-open-images-onnx-gpu-offline","title":"MLPerf inference - C++ - RetinaNet FP32 - Open Images - ONNX - GPU - Offline","text":""},{"location":"tutorials/sc22-scc-mlperf-part2/#summary_2","title":"Summary","text":"<p>After installing and detecting CUDA using CM in the previous section, you can also  run the C++ implementation of the MLPerf vision benchmark with CUDA as follows (just substitute \"Community\" with your organization or any other identifier):</p> <pre><code>cm pull repo mlcommons@ck\n\ncm run script \"get sys-utils-cm\" --quiet\n\ncm run script \"install python-venv\" --version=3.10.8 --name=mlperf-cuda\n\ncm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short,_dashboard \\\n      --adr.python.name=mlperf-cuda \\\n      --adr.python.version_min=3.8 \\\n      --adr.compiler.tags=gcc \\\n      --adr.openimages-preprocessed.tags=_500 \\\n      --submitter=\"Community\" \\\n      --implementation=cpp \\\n      --hw_name=default \\\n      --model=retinanet \\\n      --backend=onnxruntime \\\n      --device=gpu \\\n      --scenario=Offline \\\n      --test_query_count=10 \\\n      --clean\n</code></pre> <p>In case of a successfull run, you should see your crowd-testing results at this live W&amp;B dashboard.</p>"},{"location":"tutorials/sc22-scc-mlperf-part2/#mlperf-inference-python-retinanet-fp32-open-images-pytorch-cpu-offline","title":"MLPerf inference - Python - RetinaNet FP32 - Open Images - PyTorch - CPU - Offline","text":""},{"location":"tutorials/sc22-scc-mlperf-part2/#summary_3","title":"Summary","text":"<p>You can now try to use PyTorch instead of ONNX as follows:</p> <pre><code>cm pull repo mlcommons@ck\n\ncm run script \"get sys-utils-cm\" --quiet\n\ncm run script \"install python-venv\" --version=3.10.8 --name=mlperf\n\ncm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short,_dashboard \\\n      --adr.python.name=mlperf \\\n      --adr.python.version_min=3.8 \\\n      --adr.compiler.tags=gcc \\\n      --adr.ml-engine-torchvision.version_max=0.12.1 \\\n      --adr.openimages-preprocessed.tags=_500 \\\n      --submitter=\"Community\" \\\n      --implementation=python \\\n      --hw_name=default \\\n      --model=retinanet \\\n      --backend=onnxruntime \\\n      --device=cpu \\\n      --scenario=Offline \\\n      --test_query_count=10 \\\n      --num_threads=1 \\\n      --clean\n</code></pre> <p>CM will install PyTorch and PyTorch Vision &lt;= 0.12.1 (we need that because current MLPerf inference implementation  fails with other PyTorch Vision version - this will be fixed by the MLCommons inference WG) and will run this benchmark with 1 thread (this is needed because the current PyTorch implementation  sometimes fail with a high number of threads - this will be fixed by the MLCommons inference WG)</p> <p>In case of a successfull run, you should see your crowd-testing results at this live W&amp;B dashboard.</p>"},{"location":"tutorials/sc22-scc-mlperf-part2/#the-next-steps","title":"The next steps","text":"<p>Please check other parts of this tutorial to learn how to  customize and optimize MLPerf inference benchmark using MLCommons CM (under preparation):</p> <ul> <li>1st part: customize MLPerf inference (Python ref implementation, Open images, ONNX, CPU)</li> <li>3rd part: customize MLPerf inference (ResNet50 Int8, ImageNet, TVM)</li> <li>To be continued</li> </ul> <p>You are welcome to join the open MLCommons taskforce on automation and reproducibility to contribute to this project and continue optimizing this benchmark and prepare an official submission  for MLPerf inference v3.0 (March 2023) with the help of the community.</p> <p>See the development roadmap here.</p>"},{"location":"tutorials/sc22-scc-mlperf-part2/#authors","title":"Authors","text":"<ul> <li>Grigori Fursin (cTuning foundation and cKnowledge.org)</li> <li>Arjun Suresh (cTuning foundation and cKnowledge.org)</li> </ul>"},{"location":"tutorials/sc22-scc-mlperf-part2/#acknowledgments","title":"Acknowledgments","text":"<p>We thank  Hai Ah Nam, Steve Leak, Vijay Janappa Reddi, Tom Jablin, Ramesh N Chukka, Peter Mattson, David Kanter, Pablo Gonzalez Mesa, Thomas Zhu, Thomas Schmid and Gaurav Verma for their suggestions and contributions.</p>"},{"location":"tutorials/sc22-scc-mlperf-part3/","title":"Sc22 scc mlperf part3","text":"<p>[ Back to index ]</p>"},{"location":"tutorials/sc22-scc-mlperf-part3/#tutorial-modularizing-and-automating-mlperf-part-3","title":"Tutorial: modularizing and automating MLPerf (part 3)","text":"Click here to see the table of contents.  * [Introduction](#introduction) * [System preparation](#system-preparation)   * [Update CM framework and automation repository](#update-cm-framework-and-automation-repository)   * [Install virtual environment](#install-virtual-environment) * [CM automation for the MLPerf benchmark](#cm-automation-for-the-mlperf-benchmark)   * [MLPerf inference - Python - ResNet50 FP32 - ImageNet - ONNX - CPU - Offline](#mlperf-inference---python---resnet50-fp32---imagenet---onnx---cpu---offline)     * [Accuracy mode](#accuracy-mode)     * [Performance mode](#performance-mode)     * [Submission mode](#submission-mode)     * [Summary](#summary)   * [MLPerf inference - Python - ResNet50 FP32 - ImageNet - TVM - CPU - Offline](#mlperf-inference---python---resnet50-fp32---imagenet---tvm---cpu---offline)     * [Accuracy](#accuracy)     * [Submission mode](#submission-mode)     * [Summary](#summary) * [The next steps](#the-next-steps) * [Authors](#authors) * [Acknowledgments](#acknowledgments)"},{"location":"tutorials/sc22-scc-mlperf-part3/#introduction","title":"Introduction","text":"<p>We expect that you have completed the 1st part of this tutorial  and managed to run the MLPerf inference benchmark for object detection with RetinaNet FP32, Open Images and ONNX runtime on a CPU target.</p> <p>This tutorial shows you how to customize the MLPerf inference benchmark and run it with the reference Python implementation of image classification,  ImageNet, ONNX runtime and TVM on CPU.</p> <p>Note that this tutorial is under preparation and is gradually extended  by the MLCommons taskforce on automation and reproducibility.</p>"},{"location":"tutorials/sc22-scc-mlperf-part3/#system-preparation","title":"System preparation","text":""},{"location":"tutorials/sc22-scc-mlperf-part3/#update-cm-framework-and-automation-repository","title":"Update CM framework and automation repository","text":"<p>Note that the CM automation meta-framework  and the repository with automation scripts  are being continuously updated by the community to improve the portability and interoperability of  all reusable components for MLOps and DevOps.</p> <p>You can get the latest version of the CM framework and automation repository as follows (though be careful since CM CLI and APIs may change):</p> <pre><code>python3 -m pip install cmind -U\ncm pull repo mlcommons@ck --checkout=master\n\ncm run script \"get sys-utils-cm\" --quiet\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf-part3/#install-virtual-environment","title":"Install virtual environment","text":"<p>We suggest you to use Python virtual environment to avoid mixing up your native installation with MLPerf dependencies. You can use your own one or install Python virtual environment using CM automation as follows:`</p> <pre><code>cm run script \"install python-venv\" --version=3.10.8 --name=mlperf\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf-part3/#cm-automation-for-the-mlperf-benchmark","title":"CM automation for the MLPerf benchmark","text":""},{"location":"tutorials/sc22-scc-mlperf-part3/#mlperf-inference-python-resnet50-fp32-imagenet-onnx-cpu-offline","title":"MLPerf inference - Python - ResNet50 FP32 - ImageNet - ONNX - CPU - Offline","text":""},{"location":"tutorials/sc22-scc-mlperf-part3/#accuracy-mode","title":"Accuracy mode","text":"<p>You can run MLPerf image classification by customizing the CLI  of our universal CM wrapper  for MLPerf inference.</p> <p>You just need to update flag <code>--model=resnet50</code>:</p> <pre><code>cm run script --tags=run,mlperf,inference,generate-run-cmds \\\n         --adr.python.name=mlperf \\\n         --adr.python.version_min=3.8 \\\n         --submitter=\"Community\" \\\n         --implementation=python \\\n         --hw_name=default \\\n         --model=resnet50 \\\n         --backend=onnxruntime \\\n         --device=cpu \\\n         --scenario=Offline \\\n         --mode=accuracy \\\n         --test_query_count=5 \\\n         --quiet \\\n         --clean\n</code></pre> <p>In case of a successful run, you should see the following output: <pre><code>...\n\naccuracy=80.000%, good=4, total=5\n\n...\n</code></pre></p> <p>This CM script will install a small version of the ImageNet dataset (500 images) for testing and will automatically preprocess it with NCHW shape:</p> <pre><code>cm show cache --tags=get,dataset,imagenet\n</code></pre> <pre><code>* cache::242d289d79f54978\n    Tags: ['ILSVRC', 'dataset', 'get', 'image-classification', 'imagenet', 'original', 'script-artifact-7afd58d287fe4f11', '_2012-500']\n    Path: /home/fursin/CM/repos/local/cache/242d289d79f54978\n\n* cache::9e1013fd58724e2f\n    Tags: ['ILSVRC', 'dataset', 'get', 'image-classification', 'imagenet', 'preprocessed', 'script-artifact-f259d490bbaf45f5', '_NCHW']\n    Path: /home/fursin/CM/repos/local/cache/9e1013fd58724e2f\n</code></pre> <p>Check this CM script if you want to detect a full ImageNet validation dataset for submission.</p> <p>CM will also install a ResNet-50 model (FP32, ONNX) using this CM script:</p> <pre><code>cm show cache --tags=get,ml-model,resnet50\n</code></pre> <pre><code>* cache::eccee9fed2194558\n    Tags: ['get', 'image-classification', 'ml-model', 'ml-model-resnet50', 'resnet50', 'script-artifact-56203e4e998b4bc0', '_fp32', '_onnx', '_onnx-1.5-opset-11', '_onnx_']\n    Path: /home/fursin/CM/repos/local/cache/eccee9fed2194558\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf-part3/#performance-mode","title":"Performance mode","text":"<p>You can run MLPerf with ResNet50 in performance mode as follows:</p> <pre><code>cm run script --tags=run,mlperf,inference,generate-run-cmds \\\n         --adr.python.name=mlperf \\\n         --adr.python.version_min=3.8 \\\n         --submitter=\"Community\" \\\n         --implementation=python \\\n         --hw_name=default \\\n         --model=resnet50 \\\n         --backend=onnxruntime \\\n         --device=cpu \\\n         --scenario=Offline \\\n         --mode=performance \\\n         --test_query_count=5 \\\n         --quiet \\\n         --clean\n</code></pre> <p>In case of a successful run, you should see the following output: <pre><code>...\n\nINFO:main:starting TestScenario.Offline\nTestScenario.Offline qps=4.98, mean=0.1756, time=0.201, queries=1, tiles=50.0:0.1756,80.0:0.1756,90.0:0.1756,95.0:0.1756,99.0:0.1756,99.9:0.1756\n\n\n================================================\nMLPerf Results Summary\n================================================\nSUT name : PySUT\nScenario : Offline\nMode     : PerformanceOnly\nSamples per second: 28.3498\nResult is : VALID\n  Min duration satisfied : Yes\n  Min queries satisfied : Yes\n  Early stopping satisfied: Yes\n\n================================================\nAdditional Stats\n================================================\nMin latency (ns)                : 176368211\nMax latency (ns)                : 176368211\nMean latency (ns)               : 176368211\n50.00 percentile latency (ns)   : 176368211\n90.00 percentile latency (ns)   : 176368211\n95.00 percentile latency (ns)   : 176368211\n97.00 percentile latency (ns)   : 176368211\n99.00 percentile latency (ns)   : 176368211\n99.90 percentile latency (ns)   : 176368211\n\n================================================\nTest Parameters Used\n================================================\nsamples_per_query : 5\ntarget_qps : 1\ntarget_latency (ns): 0\nmax_async_queries : 1\nmin_duration (ms): 0\nmax_duration (ms): 0\nmin_query_count : 1\nmax_query_count : 5\nqsl_rng_seed : 14284205019438841327\nsample_index_rng_seed : 4163916728725999944\nschedule_rng_seed : 299063814864929621\naccuracy_log_rng_seed : 0\naccuracy_log_probability : 0\naccuracy_log_sampling_target : 0\nprint_timestamps : 0\nperformance_issue_unique : 0\nperformance_issue_same : 0\nperformance_issue_same_index : 0\nperformance_sample_count : 1024\n\nNo warnings encountered during test.\n\nNo errors encountered during test.\n\n...\n</code></pre></p>"},{"location":"tutorials/sc22-scc-mlperf-part3/#submission-mode","title":"Submission mode","text":"<p>You can now run MLPerf with ResNet50 in a submission mode:</p> <pre><code>cm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_dashboard \\\n         --adr.python.name=mlperf \\\n         --adr.python.version_min=3.8 \\\n         --submitter=\"Community\" \\\n         --implementation=python \\\n         --hw_name=default \\\n         --model=resnet50 \\\n         --backend=onnxruntime \\\n         --device=cpu \\\n         --scenario=Offline \\\n         --test_query_count=500 \\\n         --quiet \\\n         --clean\n</code></pre> <p>In case of a successfull run, you should see your crowd-testing results at this  live W&amp;B dashboard.</p>"},{"location":"tutorials/sc22-scc-mlperf-part3/#summary","title":"Summary","text":"<pre><code>python3 -m pip install cmind -U\ncm pull repo mlcommons@ck --checkout=master\n\ncm run script \"get sys-utils-cm\" --quiet\n\ncm run script \"install python-venv\" --version=3.10.8 --name=mlperf\n\ncm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_dashboard \\\n         --adr.python.name=mlperf \\\n         --adr.python.version_min=3.8 \\\n         --submitter=\"Community\" \\\n         --implementation=python \\\n         --hw_name=default \\\n         --model=resnet50 \\\n         --backend=onnxruntime \\\n         --device=cpu \\\n         --scenario=Offline \\\n         --test_query_count=500 \\\n         --quiet \\\n         --clean\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf-part3/#mlperf-inference-python-resnet50-fp32-imagenet-tvm-cpu-offline","title":"MLPerf inference - Python - ResNet50 FP32 - ImageNet - TVM - CPU - Offline","text":""},{"location":"tutorials/sc22-scc-mlperf-part3/#accuracy","title":"Accuracy","text":"<p>You can now run MLPerf inference with the Apache TVM backend that OctoML has recently added to the  MLPerf inference vision benchmark:</p> <pre><code>cm run script --tags=run,mlperf,inference,generate-run-cmds \\\n         --adr.python.name=mlperf \\\n         --adr.python.version_min=3.8 \\\n         --submitter=\"Community\" \\\n         --implementation=python \\\n         --hw_name=default \\\n         --model=resnet50 \\\n         --backend=tvm-onnx \\\n         --device=cpu \\\n         --scenario=Offline \\\n         --mode=accuracy \\\n         --test_query_count=5 \\\n         --quiet \\\n         --clean\n</code></pre> <p>This workflow will use other CM scripts to install CMake, LLVM 14+,  ONNX to load ONNX models to TVM  and will build Apache TVM.</p> <p>In case of a successful run, you should see the following output: <pre><code>...\n\naccuracy=80.000%, good=4, total=5\n\n...\n</code></pre></p> <p>If you want to use TVM via PIP install, you can use <code>--adr.tvm.tags=_pip-install</code>:</p> <pre><code>cm run script --tags=run,mlperf,inference,generate-run-cmds \\\n         --adr.python.name=mlperf \\\n         --adr.python.version_min=3.8 \\\n         --adr.tvm.tags=_pip-install \\\n         --submitter=\"Community\" \\\n         --implementation=python \\\n         --hw_name=default \\\n         --model=resnet50 \\\n         --backend=tvm-onnx \\\n         --device=cpu \\\n         --scenario=Offline \\\n         --mode=accuracy \\\n         --test_query_count=5 \\\n         --quiet \\\n         --clean\n</code></pre> <p>Note that sometimes this benchmark may hang with TVM. You need to stop it and restart it and then it should work fine. We expect the TVM community to fix this problem at some point.</p> <p>In case of a successful run, you should see the following output: <pre><code>...\n\naccuracy=80.000%, good=4, total=5\n\n...\n</code></pre></p>"},{"location":"tutorials/sc22-scc-mlperf-part3/#submission-mode_1","title":"Submission mode","text":"<pre><code>cm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_dashboard \\\n         --adr.python.name=mlperf \\\n         --adr.python.version_min=3.8 \\\n         --submitter=\"Community\" \\\n         --implementation=python \\\n         --hw_name=default \\\n         --model=resnet50 \\\n         --backend=tvm-onnx \\\n         --device=cpu \\\n         --scenario=Offline \\\n         --test_query_count=500 \\\n         --quiet \\\n         --clean\n</code></pre> <p>In case of a successfull run, you should see your crowd-testing results at this  live W&amp;B dashboard.</p>"},{"location":"tutorials/sc22-scc-mlperf-part3/#summary_1","title":"Summary","text":"<pre><code>python3 -m pip install cmind -U\ncm pull repo mlcommons@ck --checkout=master\n\ncm run script \"get sys-utils-cm\" --quiet\n\ncm run script \"install python-venv\" --version=3.10.8 --name=mlperf\n\ncm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_dashboard \\\n         --adr.python.name=mlperf \\\n         --adr.python.version_min=3.8 \\\n         --submitter=\"Community\" \\\n         --implementation=python \\\n         --hw_name=default \\\n         --model=resnet50 \\\n         --backend=tvm-onnx \\\n         --device=cpu \\\n         --scenario=Offline \\\n         --test_query_count=500 \\\n         --quiet \\\n         --clean\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf-part3/#the-next-steps","title":"The next steps","text":"<p>Please check other parts of this tutorial to learn how to  customize and optimize MLPerf inference benchmark using MLCommons CM (under preparation):</p> <ul> <li>1st part: customize MLPerf inference (Python ref implementation, Open images, ONNX, CPU)</li> <li>2nd part: customize MLPerf inference (C++ implementation, CUDA, PyTorch)</li> <li>To be continued</li> </ul> <p>You are welcome to join the open MLCommons taskforce on automation and reproducibility to contribute to this project and continue optimizing this benchmark and prepare an official submission  for MLPerf inference v3.0 (March 2023) with the help of the community.</p> <p>See the development roadmap here.</p>"},{"location":"tutorials/sc22-scc-mlperf-part3/#authors","title":"Authors","text":"<ul> <li>Grigori Fursin (cTuning foundation and cKnowledge.org)</li> <li>Arjun Suresh (cTuning foundation and cKnowledge.org)</li> </ul>"},{"location":"tutorials/sc22-scc-mlperf-part3/#acknowledgments","title":"Acknowledgments","text":"<p>We thank  Hai Ah Nam, Steve Leak, Vijay Janappa Reddi, Tom Jablin, Ramesh N Chukka, Peter Mattson, David Kanter, Pablo Gonzalez Mesa, Thomas Zhu, Thomas Schmid and Gaurav Verma for their suggestions and contributions.</p>"},{"location":"tutorials/sc22-scc-mlperf/","title":"Sc22 scc mlperf","text":"<p>[ Back to index ]</p>"},{"location":"tutorials/sc22-scc-mlperf/#tutorial-modularizing-and-automating-mlperf","title":"Tutorial: modularizing and automating MLPerf","text":"Click here to see the table of contents.  * [Introduction](#introduction) * [System preparation](#system-preparation)   * [Minimal system requirements](#minimal-system-requirements)   * [MLCommons CM automation meta-framework](#mlcommons-cm-automation-meta-framework)   * [CM installation](#cm-installation)   * [Pull CM repository with cross-platform MLOps and DevOps scripts](#pull-cm-repository-with-cross-platform-mlops-and-devops-scripts)   * [Optional: update CM and repository to the latest version](#optional-update-cm-and-repository-to-the-latest-version)   * [Install system dependencies for your platform](#install-system-dependencies-for-your-platform)   * [Use CM to detect or install Python 3.8+](#use-cm-to-detect-or-install-python-38)   * [Pull MLPerf inference sources](#pull-mlperf-inference-sources)   * [Compile MLPerf loadgen](#compile-mlperf-loadgen) * [CM automation for the MLPerf benchmark](#cm-automation-for-the-mlperf-benchmark)   * [MLPerf inference - Python - RetinaNet FP32 - Open Images - ONNX - CPU - Offline](#mlperf-inference---python---retinanet-fp32---open-images---onnx---cpu---offline)     * [Download Open Images dataset](#download-open-images-dataset)     * [Preprocess Open Images dataset](#preprocess-open-images-dataset)     * [Install ONNX runtime for CPU](#install-onnx-runtime-for-cpu)     * [Download RetinaNet model (FP32, ONNX format)](#download-retinanet-model-fp32-onnx-format)     * [Run reference MLPerf inference benchmark (offline, accuracy)](#run-reference-mlperf-inference-benchmark-offline-accuracy)     * [Run MLPerf inference benchmark (offline, performance)](#run-mlperf-inference-benchmark-offline-performance)     * [Customize MLPerf inference benchmark](#customize-mlperf-inference-benchmark)     * [Prepare MLPerf submission](#prepare-mlperf-submission)     * [Push results to a live dashboard](#push-results-to-a-live-dashboard)     * [Summary](#summary)       * [With explicit dependencies first](#with-explicit-dependencies-first)       * [With one CM command that will install all dependencies automatically](#with-one-cm-command-that-will-install-all-dependencies-automatically)     * [Use Python virtual environment with CM and MLPerf](#use-python-virtual-environment-with-cm-and-mlperf) * [The next steps](#the-next-steps) * [Authors](#authors) * [Acknowledgments](#acknowledgments)"},{"location":"tutorials/sc22-scc-mlperf/#introduction","title":"Introduction","text":"<p>This tutorial was prepared for the Student Cluster Competition'22  to explain how to prepare and run a modular version of the MLPerf inference benchmark using the cross-platform automation meta-framework (MLCommons CM). It is assembled from reusable and interoperable MLOps and DevOps scripts being developed by the open MLCommons taskforce on automation and reproducibility based on this roadmap.</p> <p>There are 4 main goals: - Trying the MLCommons CM meta-framework for modular benchmarking. - Learning how to prepare and run the MLPerf inference benchmark using CM. - Obtaining and submitting benchmarking results (accuracy and performance) for MLPerf object detection with RetinaNet in offline mode on your platform   (see related slides). - Learning how to optimize this benchmark, submit your results to the MLPerf inference v3.0 (March 2023)   and get them to the scoreboard similar to v2.1.</p> <p>It should take less than an hour to complete this tutorial. In the end, you should obtain a tarball (open.tar.gz) with the MLPerf-compatible results. You should submit it to the SCC'22 organizers  to get extra points.</p> <p>During SCC, you will attempt to run a reference (unoptimized) Python implementation of the MLPerf object detection benchmark with RetinaNet model, Open Images dataset, ONNX runtime and CPU target. </p> <p>After the SCC, you are welcome to join the open MLCommons taskforce on automation and reproducibility to learn how to optimize this benchmark further (trying various run-time parameters, trying different benchmark implementations (Nvidia, C++),  changing ML frameworks and run-times, optimizing RetinaNet model, and trying different CPUs and GPUs) and submit Pareto-optimal results to MLPerf.</p> <p>Note that both MLPerf and CM automation are evolving projects.  If you encounter issues or have questions, please submit them here  and feel free to join our weekly conf-calls.</p>"},{"location":"tutorials/sc22-scc-mlperf/#system-preparation","title":"System preparation","text":""},{"location":"tutorials/sc22-scc-mlperf/#minimal-system-requirements","title":"Minimal system requirements","text":"<ul> <li>CPU: 1 node (x86-64 or Arm64)</li> <li>OS: we have tested this automation on Ubuntu 20.04, Ubuntu 22.04, Debian 10, Red Hat 9 and MacOS 13</li> <li>Disk space: </li> <li>with a minimal preprocessed dataset for test runs: ~5GB</li> <li>with a full preprocessed dataset for the official MLPerf submission: ~200GB</li> <li>Python: 3.8+</li> <li>All other dependencies (artifacts and tools) will be installed by the CM meta-framework</li> </ul>"},{"location":"tutorials/sc22-scc-mlperf/#mlcommons-cm-automation-meta-framework","title":"MLCommons CM automation meta-framework","text":"<p>The MLCommons is developing an open-source and technology-neutral  Collective Mind meta-framework (CM) to modularize ML Systems and automate their benchmarking, optimization  and design space exploration across continuously changing software, hardware and data.</p> <p>CM is the second generation of the MLCommons CK workflow automation framework  that was originally developed to make it easier to reproduce research papers at ML and Systems conferences. The goal is to help researchers unify and automate all the steps to prepare and run MLPerf and other benchmarks across diverse ML models, datasets, frameworks, compilers and hardware (see HPCA'22 presentation about our motivation).</p>"},{"location":"tutorials/sc22-scc-mlperf/#cm-installation","title":"CM installation","text":"<p>Follow this guide to install the MLCommons CM framework on your system.</p> <p>After the installation, you should be able to access the CM command line as follows:</p> <pre><code>$ cm\n\ncm {action} {automation} {artifact(s)} {--flags} @input.yaml @input.json\n</code></pre> <p>Note: we have prepared this tutorial using CM v1.1.1. You can enforce this version as follows: <pre><code>python3 -m pip install cmind==1.1.1\n\ncm --version\n</code></pre></p> <pre><code>1.1.1\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf/#pull-cm-repository-with-cross-platform-mlops-and-devops-scripts","title":"Pull CM repository with cross-platform MLOps and DevOps scripts","text":"<p>Pull stable MLCommons CM repository with cross-platform CM scripts for modular ML Systems:</p> <pre><code>cm pull repo mlcommons@ck\n</code></pre> <p>CM pulls all such repositories into the <code>$HOME/CM</code> directory to search for CM automations and artifacts. You can find the location of a pulled repository as follows:</p> <pre><code>cm find repo mlcommons@ck\n</code></pre> <p>You can now use the unified CM CLI/API of reusable and cross-platform CM scripts) to detect or install all the latest artifacts (tools, models, datasets, libraries, etc)  required for a given software project (MLPerf inference benchmark with RetinaNet, Open Images and ONNX in our case).</p> <p>Conceptually, these scripts take some environment variables and files as an input, perform a cross-platform action (detect artifact, download files, install tools), prepare new environment variables and cache output if needed.</p> <p>Note that CM can automatically detect or install all dependencies for a given benchmark and run it on a given platform in just one command using a simple JSON or YAML description of dependencies on all required CM scripts.</p> <p>However, since the goal of this tutorial is to explain you how we modularize MLPerf and any other benchmark,  we will show you all individual CM commands to prepare and run the MLPerf inference benchmark.  You can reuse these commands in your own projects thus providing a common interface for research projects.</p> <p>In the end, we will also show you how to run MLPerf benchmark in one command from scratch.</p>"},{"location":"tutorials/sc22-scc-mlperf/#optional-update-cm-and-repository-to-the-latest-version","title":"Optional: update CM and repository to the latest version","text":"<p>Note that if you already have CM and mlcommons@ck reposity installed on your system, you can update them to the latest version at any time and clean the CM cache as follows:</p> <pre><code>python3 -m pip install cmind -U\ncm pull repo mlcommons@ck --checkout=master\ncm rm cache -f\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf/#install-system-dependencies-for-your-platform","title":"Install system dependencies for your platform","text":"<p>First, you need to install various system dependencies required by the MLPerf inference benchmark.</p> <p>For this purpose, we have created a cross-platform CM script that will automatically install  such dependencies based on your OS (Ubuntu, Debian, Red Hat, MacOS ...). </p> <p>In this case, CM script serves simply as a wrapper with a unified and cross-platform interface for native scripts that you can find and extend here if some dependencies are missing on your machine - this is a collaborative way to make  CM scripts portable and interoperable.</p> <p>You can run this CM scripts as follows (note that you may be asked for a SUDO password on your platform):</p> <pre><code>cm run script \"get sys-utils-cm\" --quiet\n</code></pre> <p>If you think that you have all system dependencies installed, you can run this script without <code>--quiet</code> flag and type \"skip\" in the script prompt.</p>"},{"location":"tutorials/sc22-scc-mlperf/#use-cm-to-detect-or-install-python-38","title":"Use CM to detect or install Python 3.8+","text":"<p>Since we use Python reference implementation of the MLPerf inference benchmark (unoptimized), we need to detect or install Python 3.8+ (MLPerf requirement). </p> <p>You need to detect it using the following CM script:</p> <pre><code>cm run script \"get python\" --version_min=3.8\n</code></pre> <p>Note, that all artifacts (including above scripts) in MLCommons CM are organized as a database of interconnected components. They can be found either by their user friendly tags (such as <code>get,python</code>) or aliases (<code>get-python3</code>) and unique identifiers (<code>5b4e0237da074764</code>). You can find this information in a CM meta description of this script.</p> <p>If required Python is already installed on your system, CM will detect it and will cache related environment variables such as PATH, PYTHONPATH, etc. to be reused by other CM scripts. You can find an associated CM cache entry for your python as follows:</p> <pre><code>cm show cache --tags=get,python\n</code></pre> <p>You can see the environment variables produced by this CM script in the following JSON file: <pre><code>cat `cm find cache --tags=get,python`/cm-cached-state.json\n</code></pre></p> <p>If required Python is not detected, CM will automatically attempt to download and build it from sources  using another cross-platform CM script \"install-python-src\". In the end, CM will also cache new binaries and related environment variables such as PATH, PYTHONPATH, etc:</p> <pre><code>cm show cache\n</code></pre> <p>You can find installed binaries and reuse them in your own project with or without CM as follows: <pre><code>cm find cache --tags=install,python\n</code></pre></p> <p>Note that if you run the same script again, CM will automatically find and reuse the cached output: <pre><code>cm run script \"get python\" --version_min=3.8 --out=json\n</code></pre></p>"},{"location":"tutorials/sc22-scc-mlperf/#pull-mlperf-inference-sources","title":"Pull MLPerf inference sources","text":"<p>You should now download and cache the MLPerf inference sources using the following command:</p> <pre><code>cm run script \"get mlperf inference src\"\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf/#compile-mlperf-loadgen","title":"Compile MLPerf loadgen","text":"<p>You need to compile loadgen from the above inference sources while forcing compiler dependency to GCC:</p> <pre><code>cm run script \"get mlperf loadgen\" --adr.compiler.tags=gcc\n</code></pre> <p>The <code>--adr</code> flag stands for \"Add to all Dependencies Recursively\" and will find all sub-dependencies on other CM scripts  in the CM loadgen script with the \"compiler\" name and will append \"gcc\" tag  to enforce detection and usage of GCC to build loadgen.</p>"},{"location":"tutorials/sc22-scc-mlperf/#cm-automation-for-the-mlperf-benchmark","title":"CM automation for the MLPerf benchmark","text":""},{"location":"tutorials/sc22-scc-mlperf/#mlperf-inference-python-retinanet-fp32-open-images-onnx-cpu-offline","title":"MLPerf inference - Python - RetinaNet FP32 - Open Images - ONNX - CPU - Offline","text":""},{"location":"tutorials/sc22-scc-mlperf/#download-open-images-dataset","title":"Download Open Images dataset","text":"<p>You can now download the minimal Open Images validation datasets v6  with the first 500 images using the following CM script:</p> <p><pre><code>cm run script \"get dataset object-detection open-images original _validation _500\"\n</code></pre> Note that <code>_</code> prefix means some variation of a script that can update environment variables and adds extra dependencies to customize the execution of a given script. See <code>\"variation\"</code> key in the meta description of this script here.</p> <p>This script will automatically install various Python sub-dependencies and openssl to download and process this dataset. The minimal set will download 500 images and will need ~200MB of disk space.</p> <p>After installing this dataset via CM, you can reuse it in your own projects or other CM scripts (including MLPerf benchmarks). You can check the CM cache as follows (the unique ID of the CM cache entry will be different on your machine): <pre><code>cm show cache --tags=get,dataset,open-images,original\n</code></pre></p> <pre><code>* cache::67d2c092e64744e5\n    Tags: ['dataset', 'get', 'object-detection', 'open-images', 'openimages', 'original', 'script-artifact-0a9d49b644cf4142', '_500', '_validation']\n    Path: /home/fursin/CM/repos/local/cache/67d2c092e64744e5\n</code></pre> <p>You can find the images and annotations in the CM cache as follows:</p> <pre><code>ls `cm find cache --tags=get,dataset,open-images,original`/install/validation/data\nls `cm find cache --tags=get,dataset,open-images,original`/install/annotations\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf/#preprocess-open-images-dataset","title":"Preprocess Open Images dataset","text":"<p>You now need to preprocess this dataset to convert it into a format consumable by the MLPerf inference benchmark using the following CM script  (converting each jpeg image from the above dataset to binary numpy format and NCHW):</p> <pre><code>cm run script \"get preprocessed dataset object-detection open-images _validation _500 _NCHW\"\n</code></pre> <p>You can find them in the CM cache as follows:</p> <pre><code>cm show cache --tags=get,preprocessed,dataset,open-images\n</code></pre> <pre><code>* cache::6b13fc343a52499c\n    Tags: ['dataset', 'get', 'object-detection', 'open-images', 'openimages', 'preprocessed', 'script-artifact-9842f1be8cba4c7b', '_500', '_NCHW', '_validation']\n    Path: /home/fursin/CM/repos/local/cache/6b13fc343a52499c\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf/#install-onnx-runtime-for-cpu","title":"Install ONNX runtime for CPU","text":"<p>Now detect or install ONNX Python runtime (targeting CPU) your system using a generic CM script to install python package:</p> <pre><code>cm run script \"get generic-python-lib _onnxruntime\" --version_min=1.10.0\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf/#download-retinanet-model-fp32-onnx-format","title":"Download RetinaNet model (FP32, ONNX format)","text":"<p>Download and cache this reference model in the ONNX format (float32) using the following CM script:</p> <pre><code>cm run script \"get ml-model object-detection retinanet resnext50 _onnx\"\n</code></pre> <p>It takes around ~150MB of disk space. You can find it in the CM cache as follows:</p> <pre><code>cm show cache --tags=get,ml-model,resnext50,_onnx\n</code></pre> <pre><code>* cache::7e1ca80c06154f22\n    Tags: ['fp32', 'get', 'ml-model', 'object-detection', 'resnext50', 'retinanet', 'script-artifact-427bc5665e4541c2', '_onnx']\n    Path: /home/fursin/CM/repos/local/cache/7e1ca80c06154f22\n</code></pre> <pre><code>ls `cm find cache --tags=get,ml-model,resnext50,_onnx`/*.onnx -l\n</code></pre> <p>Output:</p> <pre><code>148688824  resnext50_32x4d_fpn.onnx\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf/#run-reference-mlperf-inference-benchmark-offline-accuracy","title":"Run reference MLPerf inference benchmark (offline, accuracy)","text":"<p>You are now ready to run the reference (unoptimized) Python implemnentaton  of the MLPerf vision benchmark with ONNX backend.</p> <p>Normally, you would need to go through this README.md and prepare all the dependencies and environment variables manually.</p> <p>The CM \"app-mlperf-inference\" script allows you to run this benchmark as follows:</p> <pre><code>cm run script \"app mlperf inference generic _python _retinanet _onnxruntime _cpu\" \\\n     --scenario=Offline \\\n     --mode=accuracy \\\n     --test_query_count=10 \\\n     --rerun\n</code></pre> <p>This CM script will automatically find or install all dependencies described in its CM meta description, aggregate all environment variables, preprocess all files and assemble the MLPerf benchmark CMD.</p> <p>It will take a few minutes to run it and you should see the following accuracy:</p> <pre><code>loading annotations into memory...\nDone (t=0.02s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.02s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nDONE (t=0.09s).\nAccumulating evaluation results...\nDONE (t=0.11s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.548\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.787\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.714\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.304\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.631\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.433\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.648\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.663\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.731\n\nmAP=54.814%\n</code></pre> <p>Congratulations, you can now play with this benchmark using the unified CM commands!</p> <p>Note that even if did not install all above dependencies manually, the same command will automatically install all the necessary dependencies (you just need to specify that you use GCC and 500 images). </p> <p>You can check it by cleaning the CM cache and executing this command again  (it will take around ~10 minutes depending on the speed of your system and the Internet connection):</p> <pre><code>cm rm cache -f\n\ncm run script \"app mlperf inference generic _python _retinanet _onnxruntime _cpu\" \\\n     --adr.python.version_min=3.8 \\\n     --adr.compiler.tags=gcc \\\n     --adr.openimages-preprocessed.tags=_500 \\\n     --scenario=Offline \\\n     --mode=accuracy \\\n     --test_query_count=10 \\\n     --quiet \\\n     --rerun\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf/#run-mlperf-inference-benchmark-offline-performance","title":"Run MLPerf inference benchmark (offline, performance)","text":"<p>Let's run the MLPerf object detection while measuring performance:</p> <pre><code>cm run script \"app mlperf inference generic _python _retinanet _onnxruntime _cpu\" \\\n     --scenario=Offline \\\n     --mode=performance \\\n     --rerun\n</code></pre> <p>It will run for ~10 minutes and you should see the output similar to the following one in the end (the QPS is the performance result of this benchmark that depends on the speed of your system):</p> <pre><code>TestScenario.Offline qps=1.29, mean=59.7877, time=513.360, queries=660, tiles=50.0:61.7757,80.0:63.8270,90.0:66.5430,95.0:67.6991,99.0:69.2812,99.9:70.5251\n\n\n================================================\nMLPerf Results Summary\n================================================\n...\n\nNo warnings encountered during test.\n\nNo errors encountered during test.\n\n  - running time of script \"app,vision,language,mlcommons,mlperf,inference,reference,generic,ref\": 529.25 sec.\n</code></pre> <p>Note that QPS is very low because we use an unoptimized reference implementation of this benchmark on CPU. In the 2nd part of this tutorial, we will explain how to optimize this benchmark and/or run other implementations  such as the universal C++ implementation of this benchmark  developed by OctoML and the MLCommons taskforce on automation and reproducibility as well as optimized implementation of MLPerf object detection with quantized models  from Nvidia.</p> <p>You can also find the reference Python implementation of this benchmark in the CM cache as follows:</p> <pre><code>cd `cm show cache --tags=get,mlperf,src,_default`/inference/vision/classification_and_detection/python\n</code></pre> <p>You can then modify it and rerun the above command to see the effects of your changes on the accuracy and performance of the MLPerf benchmark. </p>"},{"location":"tutorials/sc22-scc-mlperf/#customize-mlperf-inference-benchmark","title":"Customize MLPerf inference benchmark","text":"<p>The execution of the original MLPerf inference benchmark is customized by various flags and environment variables.</p> <p>The \"Collective Mind\" concept is to gradually expose all optimization \"knobs\" via unified CM script interface to enable automated and reroducible design space exploration and optimization of the whole application/software/hardware stack (one of the goals of the MLCommons taskforce on automation and reproducibility).</p> <p>That is why we have provided a user-friendly mapping of the flags from the CK MLPerf script CLI to the native MLPerf variables and flags using this meta description.</p> <p>For example, you can specify a number of threads used by this benchmark as follows:</p> <pre><code>cm run script \"app mlperf inference generic _python _retinanet _onnxruntime _cpu\" \\\n     --scenario=Offline \\\n     --mode=performance \\\n     --rerun \\\n     --num_threads=4\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf/#prepare-mlperf-submission","title":"Prepare MLPerf submission","text":"<p>You are now ready to generate the submission similar to the ones appearing on the official MLPerf inference dashboard.</p> <p>We have developed another script that runs the MLPerf inference benchmark in both accuracy and performance mode, runs the submission checker, unifies output for a dashboard and creates a valid MLPerf submission pack in <code>open.tar.gz</code>  with all required MLPerf logs and stats.</p> <p>You can run this script as follows (just substitute OctoML with the name of your organization):</p> <pre><code>cm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short \\\n      --submitter=\"Community\" \\\n      --hw_name=default \\\n      --lang=python \\\n      --model=retinanet \\\n      --backend=onnxruntime \\\n      --device=cpu \\\n      --scenario=Offline \\\n      --test_query_count=10 \\\n      --clean\n</code></pre> <p>It will take around 15-30 minutes to run and you should see the following output in the end:</p> <pre><code>[2022-11-09 16:33:45,968 log_parser.py:50 INFO] Sucessfully loaded MLPerf log from open/OctoML/results/onnxruntime-cpu/retinanet/offline/accuracy/mlperf_log_detail.txt.\n[2022-11-09 16:33:45,969 log_parser.py:50 INFO] Sucessfully loaded MLPerf log from open/OctoML/results/onnxruntime-cpu/retinanet/offline/performance/run_1/mlperf_log_detail.txt.\n[2022-11-09 16:33:45,971 log_parser.py:50 INFO] Sucessfully loaded MLPerf log from open/OctoML/results/onnxruntime-cpu/retinanet/offline/performance/run_1/mlperf_log_detail.txt.\n[2022-11-09 16:33:45,971 submission_checker1.py:1516 INFO] Target latency: None, Latency: 504767463228, Scenario: Offline\n[2022-11-09 16:33:45,971 submission_checker1.py:2455 INFO] ---\n[2022-11-09 16:33:45,971 submission_checker1.py:2459 INFO] Results open/OctoML/results/onnxruntime-cpu/retinanet/offline 1.30475\n[2022-11-09 16:33:45,971 submission_checker1.py:2461 INFO] ---\n[2022-11-09 16:33:45,971 submission_checker1.py:2467 INFO] ---\n[2022-11-09 16:33:45,971 submission_checker1.py:2468 INFO] Results=1, NoResults=0\n[2022-11-09 16:33:45,971 submission_checker1.py:2474 INFO] SUMMARY: submission looks OK\n                                                                           0\nOrganization                                                          OctoML\nAvailability                                                       available\nDivision                                                                open\nSystemType                                                              edge\nSystemName                 mlperf-tests-e2-x86-16-64-ubuntu-22 (auto dete...\nPlatform                                                     onnxruntime-cpu\nModel                                                              retinanet\nMlperfModel                                                        retinanet\nScenario                                                             Offline\nResult                                                               1.30475\nAccuracy                                                              54.814\nnumber_of_nodes                                                            1\nhost_processor_model_name                     Intel(R) Xeon(R) CPU @ 2.20GHz\nhost_processors_per_node                                                   1\nhost_processor_core_count                                                 16\naccelerator_model_name                                                   NaN\naccelerators_per_node                                                      0\nLocation                   open/OctoML/results/onnxruntime-cpu/retinanet/...\nframework                                                onnxruntime v1.13.1\noperating_system              Ubuntu 22.04 (linux-5.15.0-1021-gcp-glibc2.35)\nnotes                                                                    NaN\ncompilance                                                                 1\nerrors                                                                     0\nversion                                                                 v2.1\ninferred                                                                   0\nhas_power                                                              False\nUnits                                                              Samples/s\n</code></pre> <p>Note that <code>--clean</code> flag cleans all previous runs of MLPerf benchmark to make sure that  the MLPerf submission script picks up the latest results.</p> <p>You will also see the following 3 files in your current directory: <pre><code>ls -l\n\nopen.tar.gz\nsummary.csv\nsummary.json\n</code></pre></p> <p>You should submit these files to the organizing committee to get extra points in the Student Cluster Competition.</p> <p>Note that by default, CM-MLPerf will store the raw results  in <code>$HOME/mlperf_submission</code> (with truncated accuracy logs) and in <code>$HOME/mlperf_submission_logs</code>  (with complete and very large accuracy logs).</p> <p>You can change this directory using the flag <code>--submission_dir={directory to store raw MLPerf results}</code> in the above script.</p>"},{"location":"tutorials/sc22-scc-mlperf/#push-results-to-a-live-dashboard","title":"Push results to a live dashboard","text":"<p>You can attempt to push your results to public W&amp;B dashboard for SCC'22. You just need to rerun the above command with <code>_dashboard</code> variation:</p> <pre><code>cm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short,_dashboard \\\n      --submitter=\"Community\" \\\n      --hw_name=default \\\n      --lang=python \\\n      --model=retinanet \\\n      --backend=onnxruntime \\\n      --device=cpu \\\n      --scenario=Offline \\\n      --test_query_count=10\n</code></pre> <p>You should normally see your results at this live W&amp;B dashboard  or at the newer version.</p>"},{"location":"tutorials/sc22-scc-mlperf/#summary","title":"Summary","text":"<p>Here is a compact list of CM commands to prepare and run the MLPerf object detection benchmark  with RetinaNet, Open Images, ONNX runtime (CPU) on Ubuntu 22.04:</p>"},{"location":"tutorials/sc22-scc-mlperf/#with-explicit-dependencies-first","title":"With explicit dependencies first","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo apt install python3 python3-pip python3-venv git wget\n\npython3 -m pip install cmind\nsource $HOME/.profile\n\ncm pull repo mlcommons@ck\n\ncm run script \"get sys-utils-cm\" --quiet\n\ncm run script \"get python\" --version_min=3.8\n\ncm run script \"get mlperf inference src\"\n\ncm run script \"get mlperf loadgen\" --adr.compiler.tags=gcc\n\ncm run script \"get dataset object-detection open-images original _validation _500\"\n\ncm run script \"get preprocessed dataset object-detection open-images _validation _500 _NCHW\"\n\ncm run script \"get generic-python-lib _onnxruntime\" --version_min=1.10.0\n\ncm run script \"get ml-model object-detection retinanet resnext50 _onnx\"\n\ncm run script \"app mlperf inference generic _python _retinanet _onnxruntime _cpu\" \\\n     --scenario=Offline --mode=accuracy --test_query_count=10 --rerun\n\ncm run script \"app mlperf inference generic _python _retinanet _onnxruntime _cpu\" \\\n     --scenario=Offline --mode=performance --rerun\n\ncm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short \\\n      --submitter=\"Community\" \\\n      --hw_name=default \\\n      --lang=python \\\n      --model=retinanet \\\n      --backend=onnxruntime \\\n      --device=cpu \\\n      --scenario=Offline \\\n      --test_query_count=10 \\\n      --clean\n\ncm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short,_dashboard \\\n      --submitter=\"Community\" \\\n      --hw_name=default \\\n      --lang=python \\\n      --model=retinanet \\\n      --backend=onnxruntime \\\n      --device=cpu \\\n      --scenario=Offline \\\n      --test_query_count=10 \\\n      --clean\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf/#with-one-cm-command-that-will-install-all-dependencies-automatically","title":"With one CM command that will install all dependencies automatically","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo apt install python3 python3-pip python3-venv git wget\n\npython3 -m pip install cmind\nsource $HOME/.profile\n\ncm pull repo mlcommons@ck\n\ncm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short,_dashboard \\\n      --adr.python.version_min=3.8 \\\n      --adr.compiler.tags=gcc \\\n      --adr.openimages-preprocessed.tags=_500 \\\n      --submitter=\"Community\" \\\n      --hw_name=default \\\n      --lang=python \\\n      --model=retinanet \\\n      --backend=onnxruntime \\\n      --device=cpu \\\n      --scenario=Offline \\\n      --test_query_count=10 \\\n      --clean\n</code></pre>"},{"location":"tutorials/sc22-scc-mlperf/#use-python-virtual-environment-with-cm-and-mlperf","title":"Use Python virtual environment with CM and MLPerf","text":"<p>If you prefer to avoid installing all above python packages to your native Python, you can install multiple virtual environments using the same CM interface.</p> <p>Here are the CM instructions to run the MLPerf benchmark in the Python virtual environment called \"mlperf\":</p> <pre><code>cm pull repo mlcommons@ck\n\ncm run script \"get sys-utils-cm\" --quiet\n\ncm run script \"install python-venv\" --version=3.10.8 --name=mlperf\n\ncm run script --tags=run,mlperf,inference,generate-run-cmds,_submission,_short,_dashboard \\\n      --adr.python.name=mlperf \\\n      --adr.python.version_min=3.8 \\\n      --adr.compiler.tags=gcc \\\n      --adr.openimages-preprocessed.tags=_500 \\\n      --submitter=\"Community\" \\\n      --hw_name=default \\\n      --model=retinanet \\\n      --backend=onnxruntime \\\n      --device=cpu \\\n      --scenario=Offline \\\n      --test_query_count=10 \\\n      --clean\n</code></pre> <p>Note that you need to add a flag <code>--adr.python.name={name of a virtual environment (mlperf)</code>.</p>"},{"location":"tutorials/sc22-scc-mlperf/#the-next-steps","title":"The next steps","text":"<p>Please check other parts of this tutorial to learn how to  use other implementation of the MLPerf inference benchmark (C++, Nvidia, etc)  with other ML engines (PyTorch, TF, TVM), other MLPerf scenarios  (single stream, multiple stream, server), quantized/pruned models and GPUs:</p> <ul> <li>2nd part: customize MLPerf inference (C++ implementation, CUDA, PyTorch)</li> <li>3rd part: customize MLPerf inference (ResNet50 Int8, ImageNet, TVM)</li> <li>To be continued</li> </ul> <p>You are welcome to join the open MLCommons taskforce on automation and reproducibility to contribute to this project and continue optimizing this benchmark and prepare an official submission  for MLPerf inference v3.0 (March 2023) with the help of the community.</p> <p>See the development roadmap here.</p>"},{"location":"tutorials/sc22-scc-mlperf/#authors","title":"Authors","text":"<ul> <li>Grigori Fursin (cTuning foundation and cKnowledge.org)</li> <li>Arjun Suresh (cTuning foundation and cKnowledge.org)</li> </ul>"},{"location":"tutorials/sc22-scc-mlperf/#acknowledgments","title":"Acknowledgments","text":"<p>We thank  Hai Ah Nam, Steve Leak, Vijay Janappa Reddi, Tom Jablin, Ramesh N Chukka, Peter Mattson, David Kanter, Pablo Gonzalez Mesa, Thomas Zhu, Thomas Schmid and Gaurav Verma for their suggestions and contributions.</p>"},{"location":"tutorials/sc22-scc-mlperf2/","title":"Sc22 scc mlperf2","text":"<p>Moved here.</p>"},{"location":"tutorials/sc22-scc-mlperf3/","title":"Sc22 scc mlperf3","text":"<p>Moved here.</p>"},{"location":"tutorials/scc23-mlperf-inference-bert/","title":"Macro Syntax Error","text":"<p>File: <code>tutorials/scc23-mlperf-inference-bert.md</code></p> <p>Line 1249 in Markdown file: expected token 'end of print statement', got '{' <pre><code>           --adr.mlperf-inference-implementation.max_batchsize=\"{{BATCH_SIZE{[8,16,32,64,128,256,192,384]}}}\" \\\n</code></pre></p>"},{"location":"tutorials/scc24-mlperf-inference/","title":"Scc24 mlperf inference","text":"<p>[ Back to index ]</p>"},{"location":"tutorials/scc24-mlperf-inference/#tutorial-to-run-and-optimize-mlperf-inference-benchmark-at-scc24","title":"Tutorial to run and optimize MLPerf inference benchmark at SCC'24","text":"<p>TBD</p>"},{"location":"tutorials/scc24-mlperf-inference/#authors","title":"Authors","text":"<ul> <li>Arjun Suresh</li> <li>Miro Hodak</li> <li>Grigori Fursin</li> </ul>"},{"location":"tutorials/scripts/","title":"Scripts","text":"<p>[ Back to index ]</p>"},{"location":"tutorials/scripts/#tutorial-adding-new-cm-scripts-and-automation-workflowspipelines","title":"Tutorial: adding new CM scripts and automation workflows/pipelines","text":"<p>One of the goals behind the Collective Mind scripting language is to make it as easy as possible  for new users to add their own CM scripts and automation workflows based on the existing ones  similar to the Wikipedia concept.</p> <p>We suggest users to read about CM scripts  and study existing CM scripts from MLCommons, find the most close one for their required functionality, and either extend it via meta descriptions, native scripts and customize.py Python gluing or add the next one.</p>"},{"location":"tutorials/scripts/#adding-new-cm-script-with-a-template","title":"Adding new CM script with a template","text":"<p>If you found a similar script with some alias, such as <code>get-ml-model-retinanet</code>,  you can copy it to a new one such as <code>get-ml-model-new</code> using the CM CLI as follows:</p> <pre><code>cm cp script get-ml-model-retinanet get-ml-model-new\n</code></pre> <p>You can immediately specify new tags via CLI as follows:</p> <pre><code>cm cp script get-ml-model-retinanet get-ml-model-new --new_tags=new-model,...\n</code></pre> <p>You can also edit tags and other information in the _cm.json or _cm.yaml of the newly create script: <pre><code>cm find script get-ml-model-new\n</code></pre></p> <p>Note that the new script will be created in the same CM repository as the original one. If you want to create a new script in another repository (for example, in your private CM repository called <code>my-private-repo</code>), you can do it as follows: <pre><code>cm cp script get-ml-model-retinanet my-private-repo:get-ml-model-new\n</code></pre></p> <p>You can immediately run a new script (with the same automation as the original one) using the new alias: <pre><code>cm run script get-ml-model-new\n</code></pre> or a mix of original and new tags (recommended): <pre><code>cm run script \"get ml-model new-model\"\n</code></pre></p>"},{"location":"tutorials/scripts/#adding-new-dummy-cm-script","title":"Adding new dummy CM script","text":"<p>You can also create a dummy CM script with some tags <code>new-tag1,new-tag2</code>:</p> <pre><code>cm add script new-script --new_tags=new-tag1,new-tag2\n</code></pre> <p>By default, CM will create it in the <code>local</code> CM repository: <pre><code>cm find script new-script\n</code></pre></p> <p>And you can run it as follows: <pre><code>cm run script \"new-tag1 new-tag2\"\n</code></pre></p> <p>You can add flag --yaml if you prefer to use YAML meta description for a new script instead of JSON: <pre><code>cm add script new-script2 --new_tags=new-tag1,new-tag2 --yaml\n</code></pre></p>"},{"location":"tutorials/scripts/#customizing-new-cm-script","title":"Customizing new CM script","text":"<p>As soon as you created a new script, you can easily customize it via meta description files <code>_cm.json</code> |&amp; <code>_cm.yaml</code>, <code>run.sh</code> |&amp; <code>run.bat</code> and customize.py.</p> <p>For example, you can extend _cm.json to add dependencies on other scripts,  reuse environment variables and files in your new script prepared by other CM scripts  from public or private projects, and add or update the customize.py script  with preprocess and postprocess functions to implement more complex logic to customize the execution of your script  based on previous dependencies, flags, environment variables, platform and CPU features, etc.</p> <p>Please, follow this document  to learn more about CM script customization.</p> <p>Feel free to study these two CM scripts: * CM script to reproduce results from IPOL'22 journal paper with PyTorch and 2 images * CM script to run image classification with RESNET50 ONNX model</p>"},{"location":"tutorials/scripts/#collaborating-with-the-community-to-unify-api-and-meta-of-reusable-scripts","title":"Collaborating with the community to unify API and meta of reusable scripts","text":"<p>We have developed CM language to help the community reuse and improve common automations instead of reinventing the wheel. Feel free to join our Discord server and the MLCommons task force on automation and reproducibility to participate in the unification of shared scripts, APIs and meta information for diverse DevOps and MLOps scripts and tools, and development of automation pipelines and workflows to co-design, benchmark, optimize and deploy efficient computing systems for AI, ML and other emerging workloads.</p>"},{"location":"tutorials/test-spec-ptdaemon/","title":"Test spec ptdaemon","text":"<p>[ Back to index ]</p> <p>This tutorial describes how to test a power analyzer by manually giving the current and voltage ranges and thus avoiding a separate ranging mode run. If wrong values are given (tolerance is typically around 5 times), samples will be counted as uncertain and the number of uncertain samples are output in the <code>run_1/ptd_out.txt</code> file. </p>"},{"location":"tutorials/test-spec-ptdaemon/#requirements","title":"Requirements","text":"<p>Please see this documentation for the requirements and system setup.</p>"},{"location":"tutorials/test-spec-ptdaemon/#start-power-server-yokogawa-should-be-connected-to-this-and-ptdaemon-runs-here","title":"Start Power Server (Yokogawa should be connected to this and PTDaemon runs here)","text":"<pre><code>cm run script --tags=mlperf,power,server --device_type=49 --device_port=/dev/usbtmc0\n</code></pre>"},{"location":"tutorials/test-spec-ptdaemon/#start-power-client","title":"Start Power Client","text":"<pre><code>cm run script --tags=mlperf,power,client --power_server=&lt;POWER_SERVER_IP&gt; --max_amps=0.1 --max_volts=250\n</code></pre> <p>If <code>--max_amps=0</code> or <code>--max_volts=0</code> is given, the limits are ignored and both ranging and testing modes are done. If both the values are positive, ranging mode is skipped and testing mode happens with the provided limits. </p> <p>For each sample of the testing run, uncertainty is measured and we get the total number of certain and uncertain samples during the run in the <code>run_1/ptd_out.txt</code> file. If any one sample is uncertain (uncertainty threshold is 1%), the test result can be considered invalid. </p>"}]}